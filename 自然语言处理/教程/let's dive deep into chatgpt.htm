<html xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:dt="uuid:C2F41010-65B3-11d1-A29F-00AA00C14882"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta name=ProgId content=OneNote.File>
<meta name=Generator content="Microsoft OneNote 15">
<link id=Main-File rel=Main-File href="let's%20dive%20deep%20into%20chatgpt.htm">
<link rel=File-List
href="let's%20dive%20deep%20into%20chatgpt.files/filelist.xml">
</head>

<body lang=zh-CN style='font-family:微软雅黑;font-size:36.0pt'>

<div style='direction:ltr;border-width:100%'>

<div style='direction:ltr;margin-top:0in;margin-left:0in;width:30.3937in'>

<div style='direction:ltr;margin-top:0in;margin-left:0in;width:3.593in'>

<p style='margin:0in;font-size:20.0pt'><span style='font-family:"Calibri Light"'
lang=zh-CN>let's</span><span style='font-family:Calibri' lang=en-US> dive deep
into chatgpt</span></p>

</div>

<div style='direction:ltr;margin-top:.0423in;margin-left:0in;width:2.0562in'>

<p style='margin:0in;font-size:10.0pt;color:#767676'><span style='font-family:
Calibri'>2025</span><span style='font-family:"Microsoft YaHei"'>年</span><span
style='font-family:Calibri'>7</span><span style='font-family:"Microsoft YaHei"'>月</span><span
style='font-family:Calibri'>6</span><span style='font-family:"Microsoft YaHei"'>日
星期日</span></p>

<p style='margin:0in;font-family:Calibri;font-size:10.0pt;color:#767676'>16:59</p>

</div>

<div style='direction:ltr;margin-top:.2847in;margin-left:0in;width:30.3937in'><nobr><img
src="let's%20dive%20deep%20into%20chatgpt.files/image001.png" width=2880
height=1440
alt="&#13;&#10;1.对于base模型来说，如果输入wikipieda上某个词条的前几句话，base model很可能完全复述wikipieda上的内容，因为可能在这个wiki上训练了10个epoch，就像读了10遍，base模型直接把这篇文章背诵下来了。&#13;&#10;&#13;&#10;base模型是一个token补全器，而我们需要的是助手，所以需要进行SFT。&#13;&#10;2.关于大模型的幻觉：&#13;&#10;模型将学到的知识存储在模型的参数中，可以把这405B（以lamma 3为例）个参数视为对互联网的压缩，这种压缩当然是有损的，大模型生成的内容仅仅是对互联网的一些回忆，所以，互联网数据中经常出现的内容，可能会被更容易记住，不能完全相信生成的内容，以为这只是对互联网文档的模糊回忆，这些内容是概率性的，统计性的。模型只是按照概率进行最佳推测。&#13;&#10;即大模型捏造信息。&#13;&#10;为什么会有幻觉，幻觉是从哪来的？&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;traln&#10;test&#10;Hallucinations&#10;Human:&quot;WhoisTomCruise?&quot;&#10;Assistant:&quot;TomCruiseisafamousAmericanactorandproducer...&#10;Human:&quot;WhoisJohnBarrasso?&quot;&#10;Assistant:&quot;JohnBarrassoisAmericanphysicianandpolitician..&#10;Human:&quot;WhoisGenghisKhan?&#10;Assistant:&quot;GenghisKhanwasthefounderoftheMongolEmpire.》&#10;Human:&quot;\hoisOrsonKovacs?'&#10;Assistant:？？7&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image002.png" width=1497
height=1440
alt="1.对于base模型来说，如果输入wikipieda上某个词条的前几句话，base model很可能完全复述wikipieda上的内容，因为可能在这个wiki上训练了10个epoch，就像读了10遍，base模型直接把这篇文章背诵下来了。&#13;&#10;模型将学到的知识存储在模型的参数中，可以把这405B（以lamma 3为例）个参数视为对互联网的压缩，这种压缩当然是有损的，大模型生成的内容仅仅是对互联网的一些回忆，所以，互联网数据中经常出现的内容，可能会被更容易记住，不能完全相信生成的内容，以为这只是对互联网文档的模糊回忆，这些内容是概率性的，统计性的。模型只是按照概率进行最佳推测。&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;traln&#10;test&#10;Hallucinations&#10;Human:&quot;WhoisTomCruise?&quot;&#10;Assistant:&quot;TomCruiseisafamousAmericanactorandproducer...&#10;Human:&quot;WhoisJohnBarrasso?&quot;&#10;Assistant:&quot;JohnBarrassoisAmericanphysicianandpolitician..&#10;Human:&quot;WhoisGenghisKhan?&#10;Assistant:&quot;GenghisKhanwasthefounderoftheMongolEmpire.》&#10;Human:&quot;\hoisOrsonKovacs?'&#10;Assistant:？？7&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image003.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;traln&#10;test&#10;Hallucinations&#10;Human:&quot;WhoisTomCruise?&quot;&#10;Assistant:&quot;TomCruiseisafamousAmericanactorandproducer...&#10;Human:&quot;WhoisJohnBarrasso?&quot;&#10;Assistant:&quot;JohnBarrassoisAmericanphysicianandpolitician..&#10;Human:&quot;WhoisGenghisKhan?&#10;Assistant:&quot;GenghisKhanwasthefounderoftheMongolEmpire.》&#10;Human:&quot;\hoisOrsonKovacs?'&#10;Assistant:？？7&#13;&#10;模型在统计上模仿训练集，比如训练集中有如上图所示的3条数据，并自信地回答了正确答案，当推理时被问一个新的不存在的人物时，模型会模仿这种回答，并给出统计上的最佳推测，结果就是编造信息。也就是，模型并不会表达出自己不知道，而是尽力模仿训练集，从而进行编造。&#13;&#10;&#13;&#10;如何缓解？&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;tralnlng&#10;example&#10;Mitigation#1&#10;=&gt;Usemodelinterrogationtodiscovermodel'sknowledge,and&#10;programmaticallyaugmentitstrainingdatasetwithknowledge-based&#10;refusalsincaseswherethemodeldoesn'tknowEg..&#10;Human:&quot;WhoisOrsonKovacs?'&#10;Assistant:&quot;I'msorry，|don'tbelieve《know》&#10;Mitigation#2&#10;=&gt;Allowthemodeltosearch!&#10;Human:&quot;WhoisOrsonKovacs?'&#10;Assistant:&#10;&lt;SEARCHSTART&gt;VVhoisOrsonKovacs?&lt;SEARCHEND&gt;&#10;OrsonKovacsappearstobe。&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image004.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;traln&#10;test&#10;Hallucinations&#10;Human:&quot;WhoisTomCruise?&quot;&#10;Assistant:&quot;TomCruiseisafamousAmericanactorandproducer...&#10;Human:&quot;WhoisJohnBarrasso?&quot;&#10;Assistant:&quot;JohnBarrassoisAmericanphysicianandpolitician..&#10;Human:&quot;WhoisGenghisKhan?&#10;Assistant:&quot;GenghisKhanwasthefounderoftheMongolEmpire.》&#10;Human:&quot;\hoisOrsonKovacs?'&#10;Assistant:？？7&#13;&#10;模型在统计上模仿训练集，比如训练集中有如上图所示的3条数据，并自信地回答了正确答案，当推理时被问一个新的不存在的人物时，模型会模仿这种回答，并给出统计上的最佳推测，结果就是编造信息。也就是，模型并不会表达出自己不知道，而是尽力模仿训练集，从而进行编造。&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;tralnlng&#10;example&#10;Mitigation#1&#10;=&gt;Usemodelinterrogationtodiscovermodel'sknowledge,and&#10;programmaticallyaugmentitstrainingdatasetwithknowledge-based&#10;refusalsincaseswherethemodeldoesn'tknowEg..&#10;Human:&quot;WhoisOrsonKovacs?'&#10;Assistant:&quot;I'msorry，|don'tbelieve《know》&#10;Mitigation#2&#10;=&gt;Allowthemodeltosearch!&#10;Human:&quot;WhoisOrsonKovacs?'&#10;Assistant:&#10;&lt;SEARCHSTART&gt;VVhoisOrsonKovacs?&lt;SEARCHEND&gt;&#10;OrsonKovacsappearstobe。&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image005.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;tralnlng&#10;example&#10;Mitigation#1&#10;=&gt;Usemodelinterrogationtodiscovermodel'sknowledge,and&#10;programmaticallyaugmentitstrainingdatasetwithknowledge-based&#10;refusalsincaseswherethemodeldoesn'tknowEg..&#10;Human:&quot;WhoisOrsonKovacs?'&#10;Assistant:&quot;I'msorry，|don'tbelieve《know》&#10;Mitigation#2&#10;=&gt;Allowthemodeltosearch!&#10;Human:&quot;WhoisOrsonKovacs?'&#10;Assistant:&#10;&lt;SEARCHSTART&gt;VVhoisOrsonKovacs?&lt;SEARCHEND&gt;&#10;OrsonKovacsappearstobe。&#13;&#10;&#13;&#10;1）在训练集中包含新的示例数据，当模型不知道示例的答案时，这些数据的答案是不知道，从而让模型能够表达出自己不知道这件事，而不是胡编乱造。现在的问题是，如何知道模型不知道这个问题的答案？Meta LLAMA3的做法是进行knowledge probe，从训练集中采样一段文字，让另一个LLM生成关于这段文字的几个QA pair，用这几个QA向LLM提问，用llm as judge进行打分，重复多次，如果LLM不知道这个答案，那么就构造一条新的数据，Q不变，A是I don’t know 等类似的refusal回答。&#13;&#10;&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;Extractadatasnippet什0mthepre-trainingdata.&#10;1.&#10;2．Generate0factualquestionabouttheseSnippets(context)bypromptingLlama3&#10;3．SampleresponsesfromLlama3tothequestion.&#10;4．ScorethecorrectnessOfthegenerationsusingtheoriginalcontextasareferenceandLlama3“ajudge.&#10;5．ScoretheinformativenessOfthegenerationsusingLlama3asajudge.&#10;6．Generatearefusalforresponseswhichareconsistentlyinformativeandincorrectacrossthegenerations，&#10;usmgLlama3．&#10;Weusedatageneratedfromtheknowledgeprobetoencouragethemodeltoonlyanswerquestionswhichit&#10;hasknowledgeabout，andrefuseanswermgthosequestionsthatitisunsureabout.Further,pre-trammgdata&#10;isnotalwaysfactuallyconsistentorcorrect.Wethereforea卜0collectalimitedsetlabeledfactualitydata&#10;thatdealswithsensitivetopicswherefactuallycontradictoryorincorrectstatementsareprevalent.&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image006.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;tralnlng&#10;example&#10;Mitigation#1&#10;=&gt;Usemodelinterrogationtodiscovermodel'sknowledge,and&#10;programmaticallyaugmentitstrainingdatasetwithknowledge-based&#10;refusalsincaseswherethemodeldoesn'tknowEg..&#10;Human:&quot;WhoisOrsonKovacs?'&#10;Assistant:&quot;I'msorry，|don'tbelieve《know》&#10;Mitigation#2&#10;=&gt;Allowthemodeltosearch!&#10;Human:&quot;WhoisOrsonKovacs?'&#10;Assistant:&#10;&lt;SEARCHSTART&gt;VVhoisOrsonKovacs?&lt;SEARCHEND&gt;&#10;OrsonKovacsappearstobe。&#13;&#10;1）在训练集中包含新的示例数据，当模型不知道示例的答案时，这些数据的答案是不知道，从而让模型能够表达出自己不知道这件事，而不是胡编乱造。现在的问题是，如何知道模型不知道这个问题的答案？Meta LLAMA3的做法是进行knowledge probe，从训练集中采样一段文字，让另一个LLM生成关于这段文字的几个QA pair，用这几个QA向LLM提问，用llm as judge进行打分，重复多次，如果LLM不知道这个答案，那么就构造一条新的数据，Q不变，A是I don’t know 等类似的refusal回答。&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;Extractadatasnippet什0mthepre-trainingdata.&#10;1.&#10;2．Generate0factualquestionabouttheseSnippets(context)bypromptingLlama3&#10;3．SampleresponsesfromLlama3tothequestion.&#10;4．ScorethecorrectnessOfthegenerationsusingtheoriginalcontextasareferenceandLlama3“ajudge.&#10;5．ScoretheinformativenessOfthegenerationsusingLlama3asajudge.&#10;6．Generatearefusalforresponseswhichareconsistentlyinformativeandincorrectacrossthegenerations，&#10;usmgLlama3．&#10;Weusedatageneratedfromtheknowledgeprobetoencouragethemodeltoonlyanswerquestionswhichit&#10;hasknowledgeabout，andrefuseanswermgthosequestionsthatitisunsureabout.Further,pre-trammgdata&#10;isnotalwaysfactuallyconsistentorcorrect.Wethereforea卜0collectalimitedsetlabeledfactualitydata&#10;thatdealswithsensitivetopicswherefactuallycontradictoryorincorrectstatementsareprevalent.&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image007.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Extractadatasnippet什0mthepre-trainingdata.&#10;1.&#10;2．Generate0factualquestionabouttheseSnippets(context)bypromptingLlama3&#10;3．SampleresponsesfromLlama3tothequestion.&#10;4．ScorethecorrectnessOfthegenerationsusingtheoriginalcontextasareferenceandLlama3“ajudge.&#10;5．ScoretheinformativenessOfthegenerationsusingLlama3asajudge.&#10;6．Generatearefusalforresponseswhichareconsistentlyinformativeandincorrectacrossthegenerations，&#10;usmgLlama3．&#10;Weusedatageneratedfromtheknowledgeprobetoencouragethemodeltoonlyanswerquestionswhichit&#10;hasknowledgeabout，andrefuseanswermgthosequestionsthatitisunsureabout.Further,pre-trammgdata&#10;isnotalwaysfactuallyconsistentorcorrect.Wethereforea卜0collectalimitedsetlabeledfactualitydata&#10;thatdealswithsensitivetopicswherefactuallycontradictoryorincorrectstatementsareprevalent.&#13;&#10;&#13;&#10;2）让LLM可以使用tools，可以创造一个机制，使LLM能够生成特殊的token，&lt;search_start&gt; &lt;search_end&gt;，当模型发出这个token时，会停止后续生成，并进行网络搜索，然后将检索到的文本返回给模型，拼接到context window中，然后继续生成。&#13;&#10;现在的问题是，如何教会模型发出使用这种tool的token呢，还是通过在训练集中构建几千几万个这种数据来训练模型。&#13;&#10;&#13;&#10;总结：当我们问LLM某个问题时，如果LLM对它的权重 、参数、激活值等有足够的信心，认为这个问题的答案可以从记忆中检索，它会直接生成答案。如果它不确定，就会使用网络搜索。对于user来说，要对自己问的问题有一个大致的判断， 这个问题是否是很清晰地容易回答的，是否能让LLM仅通过本身几百B的参数（对互联网的模糊记忆）就能正确回答的，如果不是，要进行网络搜索，来确保答案的正确性。&#13;&#10;&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image008.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Extractadatasnippet什0mthepre-trainingdata.&#10;1.&#10;2．Generate0factualquestionabouttheseSnippets(context)bypromptingLlama3&#10;3．SampleresponsesfromLlama3tothequestion.&#10;4．ScorethecorrectnessOfthegenerationsusingtheoriginalcontextasareferenceandLlama3“ajudge.&#10;5．ScoretheinformativenessOfthegenerationsusingLlama3asajudge.&#10;6．Generatearefusalforresponseswhichareconsistentlyinformativeandincorrectacrossthegenerations，&#10;usmgLlama3．&#10;Weusedatageneratedfromtheknowledgeprobetoencouragethemodeltoonlyanswerquestionswhichit&#10;hasknowledgeabout，andrefuseanswermgthosequestionsthatitisunsureabout.Further,pre-trammgdata&#10;isnotalwaysfactuallyconsistentorcorrect.Wethereforea卜0collectalimitedsetlabeledfactualitydata&#10;thatdealswithsensitivetopicswherefactuallycontradictoryorincorrectstatementsareprevalent.&#13;&#10;总结：当我们问LLM某个问题时，如果LLM对它的权重 、参数、激活值等有足够的信心，认为这个问题的答案可以从记忆中检索，它会直接生成答案。如果它不确定，就会使用网络搜索。对于user来说，要对自己问的问题有一个大致的判断， 这个问题是否是很清晰地容易回答的，是否能让LLM仅通过本身几百B的参数（对互联网的模糊记忆）就能正确回答的，如果不是，要进行网络搜索，来确保答案的正确性。&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image009.png" width=2880
height=1440
alt="&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;缜&quot;Vasuerecollectionvs.“。表3memory&quot;&#10;Knowledgeintheparameters==Vaguerecollection(e.g.Ofsomethingyouread1monthago)&#10;Knowledgeinthetokensofthecontextwindow==Workingmemory&#13;&#10;&#13;&#10;神经网络的参数是一种模糊的回忆，而上下文窗口是模型的工作记忆，和人类类似。上下文的数据可以被模型直接访问，它直接输入到神经网络中，而不是模糊的记忆。这对我们使用LLM也有帮助，例如，问LLM， can you summarize chapter1 of Pride and Prejudice? LLM可以很好地回答，因为LLM看过傲慢与偏见，LLM有足够的记忆。但是比起让LLM回忆，更有效的方式是直接提供给他们信息，将第一章的内容也输入到上下文窗口中，这样LLM可以直接访问他们，而不需要回忆。LLM回答的质量也会显著提高。（这和人类一样，在总结之间重新阅读了文章，会做得更好。）&#13;&#10;&#13;&#10;&#13;&#10;&#13;&#10;3.后训练，用人类标注的多轮对话进行SFT&#13;&#10;&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image010.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;缜&quot;Vasuerecollectionvs.“。表3memory&quot;&#10;Knowledgeintheparameters==Vaguerecollection(e.g.Ofsomethingyouread1monthago)&#10;Knowledgeinthetokensofthecontextwindow==Workingmemory&#13;&#10;神经网络的参数是一种模糊的回忆，而上下文窗口是模型的工作记忆，和人类类似。上下文的数据可以被模型直接访问，它直接输入到神经网络中，而不是模糊的记忆。这对我们使用LLM也有帮助，例如，问LLM， can you summarize chapter1 of Pride and Prejudice? LLM可以很好地回答，因为LLM看过傲慢与偏见，LLM有足够的记忆。但是比起让LLM回忆，更有效的方式是直接提供给他们信息，将第一章的内容也输入到上下文窗口中，这样LLM可以直接访问他们，而不需要回忆。LLM回答的质量也会显著提高。（这和人类一样，在总结之间重新阅读了文章，会做得更好。）&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image011.png" width=2880
height=1440
alt="&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;Tiktokenizer&#10;自“an《&#10;Assistant&#10;What22？&#10;2+2=4&#10;What《w，？&#10;2．2。sameas2+烈&#10;Addmessage&#10;到降u0p1&gt;What2·20叱叫&gt;&#10;到巧一，蜘Isassistant•:l一肄p怿2=耘0叱雩@怿&#10;《lin—start降uselam—sep1&gt;Whatifitwas？引in—end&#10;到in—startl&gt;assistant&lt;li旧．se。‰sa就2·2k0的．endl&gt;&#10;gp卜40&#10;TokenCount&#10;elim—Startlsuserelim—sepl&gt;Whatis2+2？引im一end《卩忄擎&#10;tart《&gt;assistant&lt;lim—sep《&gt;2+2=4&lt;1im—endl&gt;&lt;lim_startl&gt;&#10;user«]im_sep《*Whatifit翮S*?&lt;lim_endl&gt;&lt;lin_startl&gt;a&#10;SSiStant&lt;li忄一se叫&gt;2*2=4，sameas2+2!&lt;lim—endl&gt;&#10;200264，28，200266》4827，382，22‰17，1‰17，3‰&#10;209265，2e9264，173781，209266，1六19，&#10;19，2e0265，2e0264，1428，209266，4827，&#10;425，3‰290265，20026‰173781，2e0266，&#10;4，220，四，11，2684，472，220，17，10，&#10;1六&#10;538，&#10;17，&#10;17》&#10;314，220．&#10;48673，&#10;‰17，31&#10;0，200265&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image012.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Tiktokenizer&#10;自“an《&#10;Assistant&#10;What22？&#10;2+2=4&#10;What《w，？&#10;2．2。sameas2+烈&#10;Addmessage&#10;到降u0p1&gt;What2·20叱叫&gt;&#10;到巧一，蜘Isassistant•:l一肄p怿2=耘0叱雩@怿&#10;《lin—start降uselam—sep1&gt;Whatifitwas？引in—end&#10;到in—startl&gt;assistant&lt;li旧．se。‰sa就2·2k0的．endl&gt;&#10;gp卜40&#10;TokenCount&#10;elim—Startlsuserelim—sepl&gt;Whatis2+2？引im一end《卩忄擎&#10;tart《&gt;assistant&lt;lim—sep《&gt;2+2=4&lt;1im—endl&gt;&lt;lim_startl&gt;&#10;user«]im_sep《*Whatifit翮S*?&lt;lim_endl&gt;&lt;lin_startl&gt;a&#10;SSiStant&lt;li忄一se叫&gt;2*2=4，sameas2+2!&lt;lim—endl&gt;&#10;200264，28，200266》4827，382，22‰17，1‰17，3‰&#10;209265，2e9264，173781，209266，1六19，&#10;19，2e0265，2e0264，1428，209266，4827，&#10;425，3‰290265，20026‰173781，2e0266，&#10;4，220，四，11，2684，472，220，17，10，&#10;1六&#10;538，&#10;17，&#10;17》&#10;314，220．&#10;48673，&#10;‰17，31&#10;0，200265&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image013.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Tiktokenizer&#10;自“an《&#10;Assistant&#10;What22？&#10;2+2=4&#10;What《w，？&#10;2．2。sameas2+烈&#10;Addmessage&#10;到降u0p1&gt;What2·20叱叫&gt;&#10;到巧一，蜘Isassistant•:l一肄p怿2=耘0叱雩@怿&#10;《lin—start降uselam—sep1&gt;Whatifitwas？引in—end&#10;到in—startl&gt;assistant&lt;li旧．se。‰sa就2·2k0的．endl&gt;&#10;gp卜40&#10;TokenCount&#10;elim—Startlsuserelim—sepl&gt;Whatis2+2？引im一end《卩忄擎&#10;tart《&gt;assistant&lt;lim—sep《&gt;2+2=4&lt;1im—endl&gt;&lt;lim_startl&gt;&#10;user«]im_sep《*Whatifit翮S*?&lt;lim_endl&gt;&lt;lin_startl&gt;a&#10;SSiStant&lt;li忄一se叫&gt;2*2=4，sameas2+2!&lt;lim—endl&gt;&#10;200264，28，200266》4827，382，22‰17，1‰17，3‰&#10;209265，2e9264，173781，209266，1六19，&#10;19，2e0265，2e0264，1428，209266，4827，&#10;425，3‰290265，20026‰173781，2e0266，&#10;4，220，四，11，2684，472，220，17，10，&#10;1六&#10;538，&#10;17，&#10;17》&#10;314，220．&#10;48673，&#10;‰17，31&#10;0，200265&#13;&#10;&#13;&#10; openai的InstructGPT，雇了几十个外包人员labelers，生成多轮对话的数据集，生成的原则大概是1)helpful，2）truthful, 3)harmless，&#13;&#10; 这些对话数据集当然无法cover所有的prompt，但是模型经过这样的helpful，truthful，harmless数据集训练之后，模型就会开始表现出这种类似的乐于助人、真实、无害的助手个性persona。&#13;&#10;&#13;&#10;近些年，随着技术发展，更多的是使用LLM合成数据，人类进行轻微地干预，来生成数据集。Ultrachat数据集。&#13;&#10;&#13;&#10;当你问chatgpt时，chatgpt给你回复，这些回复并不是来自什么神奇魔法AI，it's coming from something that is statistically &#13;&#10;imitating human labels which comes from labeling instructions written by companies like openai. &#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image014.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Tiktokenizer&#10;自“an《&#10;Assistant&#10;What22？&#10;2+2=4&#10;What《w，？&#10;2．2。sameas2+烈&#10;Addmessage&#10;到降u0p1&gt;What2·20叱叫&gt;&#10;到巧一，蜘Isassistant•:l一肄p怿2=耘0叱雩@怿&#10;《lin—start降uselam—sep1&gt;Whatifitwas？引in—end&#10;到in—startl&gt;assistant&lt;li旧．se。‰sa就2·2k0的．endl&gt;&#10;gp卜40&#10;TokenCount&#10;elim—Startlsuserelim—sepl&gt;Whatis2+2？引im一end《卩忄擎&#10;tart《&gt;assistant&lt;lim—sep《&gt;2+2=4&lt;1im—endl&gt;&lt;lim_startl&gt;&#10;user«]im_sep《*Whatifit翮S*?&lt;lim_endl&gt;&lt;lin_startl&gt;a&#10;SSiStant&lt;li忄一se叫&gt;2*2=4，sameas2+2!&lt;lim—endl&gt;&#10;200264，28，200266》4827，382，22‰17，1‰17，3‰&#10;209265，2e9264，173781，209266，1六19，&#10;19，2e0265，2e0264，1428，209266，4827，&#10;425，3‰290265，20026‰173781，2e0266，&#10;4，220，四，11，2684，472，220，17，10，&#10;1六&#10;538，&#10;17，&#10;17》&#10;314，220．&#10;48673，&#10;‰17，31&#10;0，200265&#13;&#10; openai的InstructGPT，雇了几十个外包人员labelers，生成多轮对话的数据集，生成的原则大概是1)helpful，2）truthful, 3)harmless，&#13;&#10; 这些对话数据集当然无法cover所有的prompt，但是模型经过这样的helpful，truthful，harmless数据集训练之后，模型就会开始表现出这种类似的乐于助人、真实、无害的助手个性persona。&#13;&#10;近些年，随着技术发展，更多的是使用LLM合成数据，人类进行轻微地干预，来生成数据集。Ultrachat数据集。&#13;&#10;当你问chatgpt时，chatgpt给你回复，这些回复并不是来自什么神奇魔法AI，it's coming from something that is statistically &#13;&#10;imitating human labels which comes from labeling instructions written by companies like openai. &#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image015.png" width=2880
height=1440
alt="imitating human labels which comes from labeling instructions written by companies like openai. &#13;&#10;问chatgpt就像在问human labeler，human labeler会说什么，it's kind of like asking what would human labeler say in this conversation.&#13;&#10;当然这些human labelers不是一些随便的人，而是一些领域的专家。例如，当你询问代码问题时，参与代码对话数据集的human labelers是eduated expert。&#13;&#10;&#13;&#10;4.关于LLM的自我认知&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;Knowledseself&#10;TheLLMhasnoknowledgeofself'loutofthebox&quot;&#10;Ifyoudonothing,itwillprobablythinkitisChatGPT,developedbyOpenAl.&#10;丫Oucanprograma&quot;senseofself&quot;in一2ways:&#10;-hardcodedconversationsaroundthesetopicsintheConversationsdata&#10;systemmessage'thatremindsthemodelatthebeginningOfevery&#10;conversationaboutitsidentity&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image016.png" width=1497
height=1440
alt="imitating human labels which comes from labeling instructions written by companies like openai. &#13;&#10;问chatgpt就像在问human labeler，human labeler会说什么，it's kind of like asking what would human labeler say in this conversation.&#13;&#10;当然这些human labelers不是一些随便的人，而是一些领域的专家。例如，当你询问代码问题时，参与代码对话数据集的human labelers是eduated expert。&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;Knowledseself&#10;TheLLMhasnoknowledgeofself'loutofthebox&quot;&#10;Ifyoudonothing,itwillprobablythinkitisChatGPT,developedbyOpenAl.&#10;丫Oucanprograma&quot;senseofself&quot;in一2ways:&#10;-hardcodedconversationsaroundthesetopicsintheConversationsdata&#10;systemmessage'thatremindsthemodelatthebeginningOfevery&#10;conversationaboutitsidentity&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image017.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Knowledseself&#10;TheLLMhasnoknowledgeofself'loutofthebox&quot;&#10;Ifyoudonothing,itwillprobablythinkitisChatGPT,developedbyOpenAl.&#10;丫Oucanprograma&quot;senseofself&quot;in一2ways:&#10;-hardcodedconversationsaroundthesetopicsintheConversationsdata&#10;systemmessage'thatremindsthemodelatthebeginningOfevery&#10;conversationaboutitsidentity&#13;&#10;&#13;&#10;当问chatgpt，who are you ,who built you? chatgpt回答，I was built by openai based on gpt model。&#13;&#10;这不代表LLM有自我意识，它之所以这样说，是因为它的训练数据里有大量关于此的回答。作为开发者，有2种方式可以覆盖这个标签。&#13;&#10;1）硬编码，在SFT数据集中指定这种问题的答案。&#13;&#10;2）system message。在上下文窗口中的不可见token， 提醒模型它的身份（用来防止客服机器人变成虚拟女友）。&#13;&#10;&#13;&#10;&#13;&#10;5.LLM need tokens to think&#13;&#10;当模型进行训练和推理时，是在一维token序列进行工作，是从左到右的顺序进行演变，并给出下一个token的概率。每一个token的计算量是有限的（当然输入序列越长，计算量越大，但可以忽略不计），因此，我们希望将计算分布在多个token上。知道这一点对我们训练LLM和使用LLM都有帮助。&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image018.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Knowledseself&#10;TheLLMhasnoknowledgeofself'loutofthebox&quot;&#10;Ifyoudonothing,itwillprobablythinkitisChatGPT,developedbyOpenAl.&#10;丫Oucanprograma&quot;senseofself&quot;in一2ways:&#10;-hardcodedconversationsaroundthesetopicsintheConversationsdata&#10;systemmessage'thatremindsthemodelatthebeginningOfevery&#10;conversationaboutitsidentity&#13;&#10;当问chatgpt，who are you ,who built you? chatgpt回答，I was built by openai based on gpt model。&#13;&#10;这不代表LLM有自我意识，它之所以这样说，是因为它的训练数据里有大量关于此的回答。作为开发者，有2种方式可以覆盖这个标签。&#13;&#10;2）system message。在上下文窗口中的不可见token， 提醒模型它的身份（用来防止客服机器人变成虚拟女友）。&#13;&#10;当模型进行训练和推理时，是在一维token序列进行工作，是从左到右的顺序进行演变，并给出下一个token的概率。每一个token的计算量是有限的（当然输入序列越长，计算量越大，但可以忽略不计），因此，我们希望将计算分布在多个token上。知道这一点对我们训练LLM和使用LLM都有帮助。&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image019.png" width=2880
height=1440
alt="当模型进行训练和推理时，是在一维token序列进行工作，是从左到右的顺序进行演变，并给出下一个token的概率。每一个token的计算量是有限的（当然输入序列越长，计算量越大，但可以忽略不计），因此，我们希望将计算分布在多个token上。知道这一点对我们训练LLM和使用LLM都有帮助。&#13;&#10;1）训练时，下图右边的答案要好于左边，所以SFT数据集应该是右边这种数据，而不是左边的。因为左边是先直接回答，也就是在训练模型在单个token中得到答案，这是行不通的，因为每个token的计算量是有限的。而右边是让模型慢慢得出答案，也就是训练模型在多个token上分散计算，每个token上的计算都相对简单，多个token上的计算进行累加，得到最终的答案。&#13;&#10;&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;。Isneedtokenstot戒&#10;Human:&quot;Emilybuys3applesand2oranges.Each&#10;orangecosts$2.Thetotalcostofallthefruitiss13．&#10;WhatisthecostOfapples?&quot;&#10;Assistant:&quot;Theansweris$3.Thisis&#10;because2orangesat$2are$4total.&#10;Sothe3applescost$9,andtherefore&#10;eachappleis9/3=$3&quot;&#10;Assistant:&quot;Thetotalcostofthe&#10;orangesis94．13-4=9，thecostof&#10;the3applesis$9.9/3=3，soeach&#10;applecosts$3.Theansweris$3&quot;&#13;&#10;，&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image020.png" width=1497
height=1440
alt="当模型进行训练和推理时，是在一维token序列进行工作，是从左到右的顺序进行演变，并给出下一个token的概率。每一个token的计算量是有限的（当然输入序列越长，计算量越大，但可以忽略不计），因此，我们希望将计算分布在多个token上。知道这一点对我们训练LLM和使用LLM都有帮助。&#13;&#10;1）训练时，下图右边的答案要好于左边，所以SFT数据集应该是右边这种数据，而不是左边的。因为左边是先直接回答，也就是在训练模型在单个token中得到答案，这是行不通的，因为每个token的计算量是有限的。而右边是让模型慢慢得出答案，也就是训练模型在多个token上分散计算，每个token上的计算都相对简单，多个token上的计算进行累加，得到最终的答案。&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;。Isneedtokenstot戒&#10;Human:&quot;Emilybuys3applesand2oranges.Each&#10;orangecosts$2.Thetotalcostofallthefruitiss13．&#10;WhatisthecostOfapples?&quot;&#10;Assistant:&quot;Theansweris$3.Thisis&#10;because2orangesat$2are$4total.&#10;Sothe3applescost$9,andtherefore&#10;eachappleis9/3=$3&quot;&#10;Assistant:&quot;Thetotalcostofthe&#10;orangesis94．13-4=9，thecostof&#10;the3applesis$9.9/3=3，soeach&#10;applecosts$3.Theansweris$3&quot;&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image021.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;。Isneedtokenstot戒&#10;Human:&quot;Emilybuys3applesand2oranges.Each&#10;orangecosts$2.Thetotalcostofallthefruitiss13．&#10;WhatisthecostOfapples?&quot;&#10;Assistant:&quot;Theansweris$3.Thisis&#10;because2orangesat$2are$4total.&#10;Sothe3applescost$9,andtherefore&#10;eachappleis9/3=$3&quot;&#10;Assistant:&quot;Thetotalcostofthe&#10;orangesis94．13-4=9，thecostof&#10;the3applesis$9.9/3=3，soeach&#10;applecosts$3.Theansweris$3&quot;&#13;&#10;&#13;&#10;2)在推理时，promt中加入let's think step by step，将计算分散在更多的token上。实际上通常不需要user来显式地考虑这个问题，这个问题是openai的数据标注人员考虑的，他们确保SFT数据集中的数据是这样的就可以了，这样训练出来的LLM回答时会自动生成中间结果 ，think step by step.&#13;&#10;&#13;&#10;PS：在现实中，这种需要数学计算或者心算的问题，通常use code来解决，而不是完全依赖LLM进行下图中的心算，这种心算不是100%可靠的，特别是当数字很大时，任何一个步骤都可能出错 。当使用代码时，就不需要依赖LLM的心算了，LLM只需要生成代码，使用python interpreter执行，得到结果，会可靠得多。&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image022.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;。Isneedtokenstot戒&#10;Human:&quot;Emilybuys3applesand2oranges.Each&#10;orangecosts$2.Thetotalcostofallthefruitiss13．&#10;WhatisthecostOfapples?&quot;&#10;Assistant:&quot;Theansweris$3.Thisis&#10;because2orangesat$2are$4total.&#10;Sothe3applescost$9,andtherefore&#10;eachappleis9/3=$3&quot;&#10;Assistant:&quot;Thetotalcostofthe&#10;orangesis94．13-4=9，thecostof&#10;the3applesis$9.9/3=3，soeach&#10;applecosts$3.Theansweris$3&quot;&#13;&#10;2)在推理时，promt中加入let's think step by step，将计算分散在更多的token上。实际上通常不需要user来显式地考虑这个问题，这个问题是openai的数据标注人员考虑的，他们确保SFT数据集中的数据是这样的就可以了，这样训练出来的LLM回答时会自动生成中间结果 ，think step by step.&#13;&#10;PS：在现实中，这种需要数学计算或者心算的问题，通常use code来解决，而不是完全依赖LLM进行下图中的心算，这种心算不是100%可靠的，特别是当数字很大时，任何一个步骤都可能出错 。当使用代码时，就不需要依赖LLM的心算了，LLM只需要生成代码，使用python interpreter执行，得到结果，会可靠得多。&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image023.png" width=2880
height=1440
alt="PS：在现实中，这种需要数学计算或者心算的问题，通常use code来解决，而不是完全依赖LLM进行下图中的心算，这种心算不是100%可靠的，特别是当数字很大时，任何一个步骤都可能出错 。当使用代码时，就不需要依赖LLM的心算了，LLM只需要生成代码，使用python interpreter执行，得到结果，会可靠得多。&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;Step1：Writethetotalcostequation&#10;2+177（4）=869&#10;Step2：Simplify&#10;2+708=869&#10;Step3：Solvefor冚&#10;2牖=869一708&#10;23T=161&#10;161&#10;23&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image024.png" width=1497
height=1440
alt="PS：在现实中，这种需要数学计算或者心算的问题，通常use code来解决，而不是完全依赖LLM进行下图中的心算，这种心算不是100%可靠的，特别是当数字很大时，任何一个步骤都可能出错 。当使用代码时，就不需要依赖LLM的心算了，LLM只需要生成代码，使用python interpreter执行，得到结果，会可靠得多。&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;Step1：Writethetotalcostequation&#10;2+177（4）=869&#10;Step2：Simplify&#10;2+708=869&#10;Step3：Solvefor冚&#10;2牖=869一708&#10;23T=161&#10;161&#10;23&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image025.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Step1：Writethetotalcostequation&#10;2+177（4）=869&#10;Step2：Simplify&#10;2+708=869&#10;Step3：Solvefor冚&#10;2牖=869一708&#10;23T=161&#10;161&#10;23&#13;&#10;&#13;&#10;总结：LLM需要tokens to think，将计算分散到多个token上，要求模型创建中间结果 ，并尽量使用tools，而不是让模型纯粹依赖它的记忆参数来处理这一切。&#13;&#10;&#13;&#10;LLM需要tokens to think的另一个例子是counting。如下所示，这就是让llm在单个token中完成计数这个操作。此时好的做法是让LLM用代码tool，LLM很擅长copy paste，只需要把这个字符串复制粘贴到代码中，执行，就可以得到长度了。&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;HOWmanydotsarebelow?&#10;Thereare161dotsinyourmessage.&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image026.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Step1：Writethetotalcostequation&#10;2+177（4）=869&#10;Step2：Simplify&#10;2+708=869&#10;Step3：Solvefor冚&#10;2牖=869一708&#10;23T=161&#10;161&#10;23&#13;&#10;总结：LLM需要tokens to think，将计算分散到多个token上，要求模型创建中间结果 ，并尽量使用tools，而不是让模型纯粹依赖它的记忆参数来处理这一切。&#13;&#10;LLM需要tokens to think的另一个例子是counting。如下所示，这就是让llm在单个token中完成计数这个操作。此时好的做法是让LLM用代码tool，LLM很擅长copy paste，只需要把这个字符串复制粘贴到代码中，执行，就可以得到长度了。&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;HOWmanydotsarebelow?&#10;Thereare161dotsinyourmessage.&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image027.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;HOWmanydotsarebelow?&#10;Thereare161dotsinyourmessage.&#13;&#10;&#13;&#10;另一个例子是问LLM，how many 'r' in 'strawberry'？这个问题的难点有2个，一个是LLM看到的是token而不是letter，第二个是这个问题是个计数问题，计数问题是llm不擅长的，因为llm需要tokens to think。&#13;&#10;&#13;&#10;LLM会在一些很简单的问题上出错，如认为9.11比9.9大。原因是在圣经中，章节9.11在9.9之后出现，所以模型认为9.11比9.9大。&#13;&#10;&#13;&#10;6.强化学习&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;exposition〈pretraining&#10;(backgroundknowledge)&#10;workedproblems．supervisedC!卩9喱pg&#10;(problem+demonstratedsolution,fo「imitation)&#10;practiceproblems、·reinforcementlearning&#10;(promptstopractice,trial&amp;erroruntilyoureachthecorrectanswer)&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image028.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;HOWmanydotsarebelow?&#10;Thereare161dotsinyourmessage.&#13;&#10;另一个例子是问LLM，how many 'r' in 'strawberry'？这个问题的难点有2个，一个是LLM看到的是token而不是letter，第二个是这个问题是个计数问题，计数问题是llm不擅长的，因为llm需要tokens to think。&#13;&#10;LLM会在一些很简单的问题上出错，如认为9.11比9.9大。原因是在圣经中，章节9.11在9.9之后出现，所以模型认为9.11比9.9大。&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;exposition〈pretraining&#10;(backgroundknowledge)&#10;workedproblems．supervisedC!卩9喱pg&#10;(problem+demonstratedsolution,fo「imitation)&#10;practiceproblems、·reinforcementlearning&#10;(promptstopractice,trial&amp;erroruntilyoureachthecorrectanswer)&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image029.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;exposition〈pretraining&#10;(backgroundknowledge)&#10;workedproblems．supervisedC!卩9喱pg&#10;(problem+demonstratedsolution,fo「imitation)&#10;practiceproblems、·reinforcementlearning&#10;(promptstopractice,trial&amp;erroruntilyoureachthecorrectanswer)&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image030.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;exposition〈pretraining&#10;(backgroundknowledge)&#10;workedproblems．supervisedC!卩9喱pg&#10;(problem+demonstratedsolution,fo「imitation)&#10;practiceproblems、·reinforcementlearning&#10;(promptstopractice,trial&amp;erroruntilyoureachthecorrectanswer)&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image031.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;exposition〈pretraining&#10;(backgroundknowledge)&#10;workedproblems．supervisedC!卩9喱pg&#10;(problem+demonstratedsolution,fo「imitation)&#10;practiceproblems、·reinforcementlearning&#10;(promptstopractice,trial&amp;erroruntilyoureachthecorrectanswer)&#13;&#10;&#13;&#10;LLM的训练可以类比上学学习书本上的知识。书本上的大部分知识是阐述性的知识，exposition，就像是背景知识，当人在阅读背景知识时，可以粗略地视为在该数据上的训练，等同于LLM的预训练。书本上的第二类信息是问题及专家的解答过程，这个解答相当于LLM助手的理想回答，相当于SFT训练。书本上的第三类信息是每个章节的练习题，这部分只有问题，没有解答过程，但是有标准正确答案，这个部分相当于强化学习，自己尝试不同的解决方法，并观察哪些方法能达到最终正确答案。&#13;&#10;&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image032.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;exposition〈pretraining&#10;(backgroundknowledge)&#10;workedproblems．supervisedC!卩9喱pg&#10;(problem+demonstratedsolution,fo「imitation)&#10;practiceproblems、·reinforcementlearning&#10;(promptstopractice,trial&amp;erroruntilyoureachthecorrectanswer)&#13;&#10;LLM的训练可以类比上学学习书本上的知识。书本上的大部分知识是阐述性的知识，exposition，就像是背景知识，当人在阅读背景知识时，可以粗略地视为在该数据上的训练，等同于LLM的预训练。书本上的第二类信息是问题及专家的解答过程，这个解答相当于LLM助手的理想回答，相当于SFT训练。书本上的第三类信息是每个章节的练习题，这部分只有问题，没有解答过程，但是有标准正确答案，这个部分相当于强化学习，自己尝试不同的解决方法，并观察哪些方法能达到最终正确答案。&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image033.png" width=2880
height=1440
alt="&#13;&#10;为什么需要强化学习？&#13;&#10;1）&#13;&#10;&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;Tiktokenizer&#10;Emilybuys3applesand20ranges．Eachorangecosts$2．The&#10;totaIcostOfallthefru$13·WhatisthecostOfeach&#10;apple?I&#10;upasystemOfequations．&#10;Set&#10;priceOfapples&#10;X&#10;3*×&#10;+2*2=13&#10;3*×&#10;+4=13&#10;3*×&#10;9&#10;×=3&#10;0rangescost2冰2=4．&#10;The&#10;SOtheapplescost13_4=9&#10;Thereare3apples•&#10;SOeachapplecosts9/3=3．&#10;13一4=％&#10;9／3&#10;（13一4）/3一3．&#10;Answer：$3&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image034.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Tiktokenizer&#10;Emilybuys3applesand20ranges．Eachorangecosts$2．The&#10;totaIcostOfallthefru$13·WhatisthecostOfeach&#10;apple?I&#10;upasystemOfequations．&#10;Set&#10;priceOfapples&#10;X&#10;3*×&#10;+2*2=13&#10;3*×&#10;+4=13&#10;3*×&#10;9&#10;×=3&#10;0rangescost2冰2=4．&#10;The&#10;SOtheapplescost13_4=9&#10;Thereare3apples•&#10;SOeachapplecosts9/3=3．&#10;13一4=％&#10;9／3&#10;（13一4）/3一3．&#10;Answer：$3&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image035.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Tiktokenizer&#10;Emilybuys3applesand20ranges．Eachorangecosts$2．The&#10;totaIcostOfallthefru$13·WhatisthecostOfeach&#10;apple?I&#10;upasystemOfequations．&#10;Set&#10;priceOfapples&#10;X&#10;3*×&#10;+2*2=13&#10;3*×&#10;+4=13&#10;3*×&#10;9&#10;×=3&#10;0rangescost2冰2=4．&#10;The&#10;SOtheapplescost13_4=9&#10;Thereare3apples•&#10;SOeachapplecosts9/3=3．&#10;13一4=％&#10;9／3&#10;（13一4）/3一3．&#10;Answer：$3&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image036.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Tiktokenizer&#10;Emilybuys3applesand20ranges．Eachorangecosts$2．The&#10;totaIcostOfallthefru$13·WhatisthecostOfeach&#10;apple?I&#10;upasystemOfequations．&#10;Set&#10;priceOfapples&#10;X&#10;3*×&#10;+2*2=13&#10;3*×&#10;+4=13&#10;3*×&#10;9&#10;×=3&#10;0rangescost2冰2=4．&#10;The&#10;SOtheapplescost13_4=9&#10;Thereare3apples•&#10;SOeachapplecosts9/3=3．&#10;13一4=％&#10;9／3&#10;（13一4）/3一3．&#10;Answer：$3&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image037.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Tiktokenizer&#10;Emilybuys3applesand20ranges．Eachorangecosts$2．The&#10;totaIcostOfallthefru$13·WhatisthecostOfeach&#10;apple?I&#10;upasystemOfequations．&#10;Set&#10;priceOfapples&#10;X&#10;3*×&#10;+2*2=13&#10;3*×&#10;+4=13&#10;3*×&#10;9&#10;×=3&#10;0rangescost2冰2=4．&#10;The&#10;SOtheapplescost13_4=9&#10;Thereare3apples•&#10;SOeachapplecosts9/3=3．&#10;13一4=％&#10;9／3&#10;（13一4）/3一3．&#10;Answer：$3&#13;&#10;&#13;&#10;当构建SFT数据集时，如上图所示，有4种response，4个response的答案都是正确的3，但是具体选择哪种response作为SFT的训练数据呢，是不确定的，因为我们和LLM的思考方式不同，我们的知识和LLM不同，LLM可能认为某种response是trivial的，过长的response仅仅是浪费token，也可能认为某种response是它不曾掌握的知识，这种response会让它感到困惑。我们并不知道最适合LLM的response 是什么，因此，我们需要允许LLM发现适用于它的response token序列，可以可靠地得到答案，此时就需要强化学习来不断试错 。如下图所示，prompt----&gt;solution-----&gt;correct answer。prompt和correct answer是确实的，solution是要LLM自己通过试错生成的，而不是人类标注员提供。（LLM就像在游乐场玩耍，它知道要达到什么目标，它发现了适合它的response  solution 序列，这个序列在统计上更平滑，充分利用了模型已有的知识）&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image038.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;Tiktokenizer&#10;Emilybuys3applesand20ranges．Eachorangecosts$2．The&#10;totaIcostOfallthefru$13·WhatisthecostOfeach&#10;apple?I&#10;upasystemOfequations．&#10;Set&#10;priceOfapples&#10;X&#10;3*×&#10;+2*2=13&#10;3*×&#10;+4=13&#10;3*×&#10;9&#10;×=3&#10;0rangescost2冰2=4．&#10;The&#10;SOtheapplescost13_4=9&#10;Thereare3apples•&#10;SOeachapplecosts9/3=3．&#10;13一4=％&#10;9／3&#10;（13一4）/3一3．&#10;Answer：$3&#13;&#10;当构建SFT数据集时，如上图所示，有4种response，4个response的答案都是正确的3，但是具体选择哪种response作为SFT的训练数据呢，是不确定的，因为我们和LLM的思考方式不同，我们的知识和LLM不同，LLM可能认为某种response是trivial的，过长的response仅仅是浪费token，也可能认为某种response是它不曾掌握的知识，这种response会让它感到困惑。我们并不知道最适合LLM的response 是什么，因此，我们需要允许LLM发现适用于它的response token序列，可以可靠地得到答案，此时就需要强化学习来不断试错 。如下图所示，prompt----&gt;solution-----&gt;correct answer。prompt和correct answer是确实的，solution是要LLM自己通过试错生成的，而不是人类标注员提供。（LLM就像在游乐场玩耍，它知道要达到什么目标，它发现了适合它的response  solution 序列，这个序列在统计上更平滑，充分利用了模型已有的知识）&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image039.png" width=2880
height=1440
alt="当构建SFT数据集时，如上图所示，有4种response，4个response的答案都是正确的3，但是具体选择哪种response作为SFT的训练数据呢，是不确定的，因为我们和LLM的思考方式不同，我们的知识和LLM不同，LLM可能认为某种response是trivial的，过长的response仅仅是浪费token，也可能认为某种response是它不曾掌握的知识，这种response会让它感到困惑。我们并不知道最适合LLM的response 是什么，因此，我们需要允许LLM发现适用于它的response token序列，可以可靠地得到答案，此时就需要强化学习来不断试错 。如下图所示，prompt----&gt;solution-----&gt;correct answer。prompt和correct answer是确实的，solution是要LLM自己通过试错生成的，而不是人类标注员提供。（LLM就像在游乐场玩耍，它知道要达到什么目标，它发现了适合它的response  solution 序列，这个序列在统计上更平滑，充分利用了模型已有的知识）&#13;&#10;&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;POST-TRAINING:REINFORCEMENTLEARNING&#10;Wearegivenproblemstatement&#10;(prompt)andthefinalanswer.&#10;Wewanttopracticesolutionsthat&#10;takeuSfromproblemstatementto&#10;theanswer，and&quot;internalize&quot;them&#10;intothemodel.&#10;Problemstatement&#10;Solution&#10;Answer&#10;prompt&#10;Emilybuys3applesand2oranges.Eachorange&#10;costs$2.ThetotalcostofallthefruitisS13&#10;WhatisthecostOfeachapple?&#10;solutions&#10;Answer:3&#10;Wegenerated15solutions.&#10;Only4ofthemgottherightanswer.&#10;Takethetopsolution(eachrightandshort).&#10;TrainOnit.&#10;Repeatmany，manytimes.&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image040.png" width=1497
height=1440
alt="当构建SFT数据集时，如上图所示，有4种response，4个response的答案都是正确的3，但是具体选择哪种response作为SFT的训练数据呢，是不确定的，因为我们和LLM的思考方式不同，我们的知识和LLM不同，LLM可能认为某种response是trivial的，过长的response仅仅是浪费token，也可能认为某种response是它不曾掌握的知识，这种response会让它感到困惑。我们并不知道最适合LLM的response 是什么，因此，我们需要允许LLM发现适用于它的response token序列，可以可靠地得到答案，此时就需要强化学习来不断试错 。如下图所示，prompt----&gt;solution-----&gt;correct answer。prompt和correct answer是确实的，solution是要LLM自己通过试错生成的，而不是人类标注员提供。（LLM就像在游乐场玩耍，它知道要达到什么目标，它发现了适合它的response  solution 序列，这个序列在统计上更平滑，充分利用了模型已有的知识）&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;POST-TRAINING:REINFORCEMENTLEARNING&#10;Wearegivenproblemstatement&#10;(prompt)andthefinalanswer.&#10;Wewanttopracticesolutionsthat&#10;takeuSfromproblemstatementto&#10;theanswer，and&quot;internalize&quot;them&#10;intothemodel.&#10;Problemstatement&#10;Solution&#10;Answer&#10;prompt&#10;Emilybuys3applesand2oranges.Eachorange&#10;costs$2.ThetotalcostofallthefruitisS13&#10;WhatisthecostOfeachapple?&#10;solutions&#10;Answer:3&#10;Wegenerated15solutions.&#10;Only4ofthemgottherightanswer.&#10;Takethetopsolution(eachrightandshort).&#10;TrainOnit.&#10;Repeatmany，manytimes.&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image041.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;POST-TRAINING:REINFORCEMENTLEARNING&#10;Wearegivenproblemstatement&#10;(prompt)andthefinalanswer.&#10;Wewanttopracticesolutionsthat&#10;takeuSfromproblemstatementto&#10;theanswer，and&quot;internalize&quot;them&#10;intothemodel.&#10;Problemstatement&#10;Solution&#10;Answer&#10;prompt&#10;Emilybuys3applesand2oranges.Eachorange&#10;costs$2.ThetotalcostofallthefruitisS13&#10;WhatisthecostOfeachapple?&#10;solutions&#10;Answer:3&#10;Wegenerated15solutions.&#10;Only4ofthemgottherightanswer.&#10;Takethetopsolution(eachrightandshort).&#10;TrainOnit.&#10;Repeatmany，manytimes.&#13;&#10;2）一味模仿无法超越，只有自己探索才能超越&#13;&#10;监督学习是模型在模仿人类专家，但是永远无法超越顶尖专家。&#13;&#10;Alphago超越了人类的围棋水平，例如在第37步中走了非常不同寻常的一步，人类几乎不会走的一步，这实际上是超越了人类。&#13;&#10;同理，将强化学习应用于训练LLM，理论上LLM可以在推理或思考上超越人类。what does this mean?也许是LLM发现了一种完全不同的类比，或思考方式，或新的语言类似编程语言。&#13;&#10;&#13;&#10;&#13;&#10;Deepseek r1&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image042.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;POST-TRAINING:REINFORCEMENTLEARNING&#10;Wearegivenproblemstatement&#10;(prompt)andthefinalanswer.&#10;Wewanttopracticesolutionsthat&#10;takeuSfromproblemstatementto&#10;theanswer，and&quot;internalize&quot;them&#10;intothemodel.&#10;Problemstatement&#10;Solution&#10;Answer&#10;prompt&#10;Emilybuys3applesand2oranges.Eachorange&#10;costs$2.ThetotalcostofallthefruitisS13&#10;WhatisthecostOfeachapple?&#10;solutions&#10;Answer:3&#10;Wegenerated15solutions.&#10;Only4ofthemgottherightanswer.&#10;Takethetopsolution(eachrightandshort).&#10;TrainOnit.&#10;Repeatmany，manytimes.&#13;&#10;Alphago超越了人类的围棋水平，例如在第37步中走了非常不同寻常的一步，人类几乎不会走的一步，这实际上是超越了人类。&#13;&#10;同理，将强化学习应用于训练LLM，理论上LLM可以在推理或思考上超越人类。what does this mean?也许是LLM发现了一种完全不同的类比，或思考方式，或新的语言类似编程语言。&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image043.png" width=2880
height=1440
alt="Deepseek r1&#13;&#10;随着训练步数增加，推理准确率增加，与此同时推理的tokens序列也增加，因为LLM涌现出了类似wait, that an aha moment这种情况。模型发展了自己的认知策略，自己的思考方式，reevaluate，多方面思考，思维链，这就是导致响应长度增加的原因，同时也是提高推理准确性的原因。&#13;&#10;&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;ReinforcementLearningdiscovers&quot;thinking&quot;and&quot;cognitivestrategies'&#10;Itisemergentduringtheoptimization,justintheprocessOfsolvingmathproblems.&#10;DeepSeek-R1：IncentivizingReasoningCapabilityinLLMsvia&#10;ReinforcementLearning&#10;r0000rcb0d00p000k．c00&#10;Question:Ifa&gt;thenthesumoftherealsolutionsof、‰二乁唁；；=isequal№&#10;Response:&lt;think&gt;&#10;Tosolvetheequation、‰二乁《=丫，№t'5startbysquaringboth&#10;RearrangetoisolatetheInnersquarerootterm:&#10;@一攴2）2=。+攴：=·a2一2“2+02）2=a+攴=。4一2“2一攴+2一a）&#10;=0&#10;Wait,wait.Wait.That'sanahamomentIcanflaghere.&#10;Leesreevaluatethisstep-by-steptoidentifyifthe（0“tsumcan。&#10;Westartedwiththeequation.&#10;a+丫=&#10;First,let'ssquarebothsides.&#10;Next,Icouldsquarebothsidesagain,treatingtheequation:&#10;Table3《Aninteresting&quot;ahamoment&quot;ofanintermediateversionOfDeepSeek-R1-Zero.le&#10;modellearnstorethinkusingananthropomorphictone.Thisisalsoanahamomentfor，&#10;allowingustowi凸5thepowerandbeautyOfreinforcementlearning.&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image044.png" width=1497
height=1440
alt="随着训练步数增加，推理准确率增加，与此同时推理的tokens序列也增加，因为LLM涌现出了类似wait, that an aha moment这种情况。模型发展了自己的认知策略，自己的思考方式，reevaluate，多方面思考，思维链，这就是导致响应长度增加的原因，同时也是提高推理准确性的原因。&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;ReinforcementLearningdiscovers&quot;thinking&quot;and&quot;cognitivestrategies'&#10;Itisemergentduringtheoptimization,justintheprocessOfsolvingmathproblems.&#10;DeepSeek-R1：IncentivizingReasoningCapabilityinLLMsvia&#10;ReinforcementLearning&#10;r0000rcb0d00p000k．c00&#10;Question:Ifa&gt;thenthesumoftherealsolutionsof、‰二乁唁；；=isequal№&#10;Response:&lt;think&gt;&#10;Tosolvetheequation、‰二乁《=丫，№t'5startbysquaringboth&#10;RearrangetoisolatetheInnersquarerootterm:&#10;@一攴2）2=。+攴：=·a2一2“2+02）2=a+攴=。4一2“2一攴+2一a）&#10;=0&#10;Wait,wait.Wait.That'sanahamomentIcanflaghere.&#10;Leesreevaluatethisstep-by-steptoidentifyifthe（0“tsumcan。&#10;Westartedwiththeequation.&#10;a+丫=&#10;First,let'ssquarebothsides.&#10;Next,Icouldsquarebothsidesagain,treatingtheequation:&#10;Table3《Aninteresting&quot;ahamoment&quot;ofanintermediateversionOfDeepSeek-R1-Zero.le&#10;modellearnstorethinkusingananthropomorphictone.Thisisalsoanahamomentfor，&#10;allowingustowi凸5thepowerandbeautyOfreinforcementlearning.&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image045.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;ReinforcementLearningdiscovers&quot;thinking&quot;and&quot;cognitivestrategies'&#10;Itisemergentduringtheoptimization,justintheprocessOfsolvingmathproblems.&#10;DeepSeek-R1：IncentivizingReasoningCapabilityinLLMsvia&#10;ReinforcementLearning&#10;r0000rcb0d00p000k．c00&#10;Question:Ifa&gt;thenthesumoftherealsolutionsof、‰二乁唁；；=isequal№&#10;Response:&lt;think&gt;&#10;Tosolvetheequation、‰二乁《=丫，№t'5startbysquaringboth&#10;RearrangetoisolatetheInnersquarerootterm:&#10;@一攴2）2=。+攴：=·a2一2“2+02）2=a+攴=。4一2“2一攴+2一a）&#10;=0&#10;Wait,wait.Wait.That'sanahamomentIcanflaghere.&#10;Leesreevaluatethisstep-by-steptoidentifyifthe（0“tsumcan。&#10;Westartedwiththeequation.&#10;a+丫=&#10;First,let'ssquarebothsides.&#10;Next,Icouldsquarebothsidesagain,treatingtheequation:&#10;Table3《Aninteresting&quot;ahamoment&quot;ofanintermediateversionOfDeepSeek-R1-Zero.le&#10;modellearnstorethinkusingananthropomorphictone.Thisisalsoanahamomentfor，&#10;allowingustowi凸5thepowerandbeautyOfreinforcementlearning.&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image046.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;ReinforcementLearningdiscovers&quot;thinking&quot;and&quot;cognitivestrategies'&#10;Itisemergentduringtheoptimization,justintheprocessOfsolvingmathproblems.&#10;DeepSeek-R1：IncentivizingReasoningCapabilityinLLMsvia&#10;ReinforcementLearning&#10;r0000rcb0d00p000k．c00&#10;Question:Ifa&gt;thenthesumoftherealsolutionsof、‰二乁唁；；=isequal№&#10;Response:&lt;think&gt;&#10;Tosolvetheequation、‰二乁《=丫，№t'5startbysquaringboth&#10;RearrangetoisolatetheInnersquarerootterm:&#10;@一攴2）2=。+攴：=·a2一2“2+02）2=a+攴=。4一2“2一攴+2一a）&#10;=0&#10;Wait,wait.Wait.That'sanahamomentIcanflaghere.&#10;Leesreevaluatethisstep-by-steptoidentifyifthe（0“tsumcan。&#10;Westartedwiththeequation.&#10;a+丫=&#10;First,let'ssquarebothsides.&#10;Next,Icouldsquarebothsidesagain,treatingtheequation:&#10;Table3《Aninteresting&quot;ahamoment&quot;ofanintermediateversionOfDeepSeek-R1-Zero.le&#10;modellearnstorethinkusingananthropomorphictone.Thisisalsoanahamomentfor，&#10;allowingustowi凸5thepowerandbeautyOfreinforcementlearning.&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image047.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;ReinforcementLearningdiscovers&quot;thinking&quot;and&quot;cognitivestrategies'&#10;Itisemergentduringtheoptimization,justintheprocessOfsolvingmathproblems.&#10;DeepSeek-R1：IncentivizingReasoningCapabilityinLLMsvia&#10;ReinforcementLearning&#10;r0000rcb0d00p000k．c00&#10;Question:Ifa&gt;thenthesumoftherealsolutionsof、‰二乁唁；；=isequal№&#10;Response:&lt;think&gt;&#10;Tosolvetheequation、‰二乁《=丫，№t'5startbysquaringboth&#10;RearrangetoisolatetheInnersquarerootterm:&#10;@一攴2）2=。+攴：=·a2一2“2+02）2=a+攴=。4一2“2一攴+2一a）&#10;=0&#10;Wait,wait.Wait.That'sanahamomentIcanflaghere.&#10;Leesreevaluatethisstep-by-steptoidentifyifthe（0“tsumcan。&#10;Westartedwiththeequation.&#10;a+丫=&#10;First,let'ssquarebothsides.&#10;Next,Icouldsquarebothsidesagain,treatingtheequation:&#10;Table3《Aninteresting&quot;ahamoment&quot;ofanintermediateversionOfDeepSeek-R1-Zero.le&#10;modellearnstorethinkusingananthropomorphictone.Thisisalsoanahamomentfor，&#10;allowingustowi凸5thepowerandbeautyOfreinforcementlearning.&#13;&#10;&#13;&#10;RLHF&#13;&#10;当问题没有答案时，即不可验证，如创意写作，给不同解决方法评分变得困难。&#13;&#10;用神经网络来模拟人类偏好，即训练一个reward model。&#13;&#10;RLHF的好处：可以在任意领域进行RL，即使是不可验证，而且empirically效果提升。&#13;&#10;坏处：reward model是有损的，而且reward model本质上是神经网络，RL非常擅长愚弄reward model，生成对抗性样本。&#13;&#10;因此在实际中，RLHF通常只运行几百steps，如果一直运行，就会开始愚弄reward model。而RL则可以无限运行。从这个意义上说，RLHF不是RL，而是一种微调。&#13;&#10;&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image048.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;ReinforcementLearningdiscovers&quot;thinking&quot;and&quot;cognitivestrategies'&#10;Itisemergentduringtheoptimization,justintheprocessOfsolvingmathproblems.&#10;DeepSeek-R1：IncentivizingReasoningCapabilityinLLMsvia&#10;ReinforcementLearning&#10;r0000rcb0d00p000k．c00&#10;Question:Ifa&gt;thenthesumoftherealsolutionsof、‰二乁唁；；=isequal№&#10;Response:&lt;think&gt;&#10;Tosolvetheequation、‰二乁《=丫，№t'5startbysquaringboth&#10;RearrangetoisolatetheInnersquarerootterm:&#10;@一攴2）2=。+攴：=·a2一2“2+02）2=a+攴=。4一2“2一攴+2一a）&#10;=0&#10;Wait,wait.Wait.That'sanahamomentIcanflaghere.&#10;Leesreevaluatethisstep-by-steptoidentifyifthe（0“tsumcan。&#10;Westartedwiththeequation.&#10;a+丫=&#10;First,let'ssquarebothsides.&#10;Next,Icouldsquarebothsidesagain,treatingtheequation:&#10;Table3《Aninteresting&quot;ahamoment&quot;ofanintermediateversionOfDeepSeek-R1-Zero.le&#10;modellearnstorethinkusingananthropomorphictone.Thisisalsoanahamomentfor，&#10;allowingustowi凸5thepowerandbeautyOfreinforcementlearning.&#13;&#10;RLHF的好处：可以在任意领域进行RL，即使是不可验证，而且empirically效果提升。&#13;&#10;坏处：reward model是有损的，而且reward model本质上是神经网络，RL非常擅长愚弄reward model，生成对抗性样本。&#13;&#10;因此在实际中，RLHF通常只运行几百steps，如果一直运行，就会开始愚弄reward model。而RL则可以无限运行。从这个意义上说，RLHF不是RL，而是一种微调。&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image049.png" width=2880
height=1440
alt="&#13;&#10;未命名图片.jpg 计算机生成了可选文字:&#10;RLHFupside&#10;WecanrunRL,inarbitrarydomains!(eventheunverifiableones)&#10;This(empirically)improvestheperformanceofthemodel,possiblydue&#10;tothe&quot;discriminator-generatorgap。&#10;Inmanycases，itismucheasiertodiscriminatethantogenerate.&#10;e.g.&quot;VVriteapoem'vs.&quot;VVhichofthese5poemsisbest?&quot;&#10;RLHFdownside&#10;WearedoingRLwithrespecttoalossysimulationofhumans.Itmight&#10;bemisleading!&#10;Evenmoresubtle:&#10;RLdiscoverswaystogame&quot;themodel.&#10;Itdiscovers&quot;adversarialexamples&quot;oftherewardmodel.&#10;E.g.after1，000updates,thetopjokeaboutpelicansisnotthebanger&#10;youwant,butsomethingtotallynon-sensicallike&quot;thethethethethe&#10;thethethe&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image050.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;RLHFupside&#10;WecanrunRL,inarbitrarydomains!(eventheunverifiableones)&#10;This(empirically)improvestheperformanceofthemodel,possiblydue&#10;tothe&quot;discriminator-generatorgap。&#10;Inmanycases，itismucheasiertodiscriminatethantogenerate.&#10;e.g.&quot;VVriteapoem'vs.&quot;VVhichofthese5poemsisbest?&quot;&#10;RLHFdownside&#10;WearedoingRLwithrespecttoalossysimulationofhumans.Itmight&#10;bemisleading!&#10;Evenmoresubtle:&#10;RLdiscoverswaystogame&quot;themodel.&#10;Itdiscovers&quot;adversarialexamples&quot;oftherewardmodel.&#10;E.g.after1，000updates,thetopjokeaboutpelicansisnotthebanger&#10;youwant,butsomethingtotallynon-sensicallike&quot;thethethethethe&#10;thethethe&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image051.png" width=2880
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;RLHFupside&#10;WecanrunRL,inarbitrarydomains!(eventheunverifiableones)&#10;This(empirically)improvestheperformanceofthemodel,possiblydue&#10;tothe&quot;discriminator-generatorgap。&#10;Inmanycases，itismucheasiertodiscriminatethantogenerate.&#10;e.g.&quot;VVriteapoem'vs.&quot;VVhichofthese5poemsisbest?&quot;&#10;RLHFdownside&#10;WearedoingRLwithrespecttoalossysimulationofhumans.Itmight&#10;bemisleading!&#10;Evenmoresubtle:&#10;RLdiscoverswaystogame&quot;themodel.&#10;Itdiscovers&quot;adversarialexamples&quot;oftherewardmodel.&#10;E.g.after1，000updates,thetopjokeaboutpelicansisnotthebanger&#10;youwant,butsomethingtotallynon-sensicallike&quot;thethethethethe&#10;thethethe&#13;&#10;&#13;&#10;PREVIEW OF THINGS TO COME&#13;&#10;&#13;&#10;- multimodal (not just text but audio, images, video, natural conversations)&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image052.png" width=1497
height=1440
alt="未命名图片.jpg 计算机生成了可选文字:&#10;RLHFupside&#10;WecanrunRL,inarbitrarydomains!(eventheunverifiableones)&#10;This(empirically)improvestheperformanceofthemodel,possiblydue&#10;tothe&quot;discriminator-generatorgap。&#10;Inmanycases，itismucheasiertodiscriminatethantogenerate.&#10;e.g.&quot;VVriteapoem'vs.&quot;VVhichofthese5poemsisbest?&quot;&#10;RLHFdownside&#10;WearedoingRLwithrespecttoalossysimulationofhumans.Itmight&#10;bemisleading!&#10;Evenmoresubtle:&#10;RLdiscoverswaystogame&quot;themodel.&#10;Itdiscovers&quot;adversarialexamples&quot;oftherewardmodel.&#10;E.g.after1，000updates,thetopjokeaboutpelicansisnotthebanger&#10;youwant,butsomethingtotallynon-sensicallike&quot;thethethethethe&#10;thethethe&#13;&#10;"><br>
<img src="let's%20dive%20deep%20into%20chatgpt.files/image053.png" width=2880
height=787
alt="- multimodal (not just text but audio, images, video, natural conversations)&#13;&#10;- tasks -&gt; agents (long, coherent, error-correcting contexts)现在大多数情况是将单个任务交给模型，仍然需要我们来organize a coherent execution of tasks，模型不能修正错误，尤其是长时间运行。Agents随着时间执行任务，人类负责监督，agents偶尔会报告进展，&#13;&#10;- pervasive, invisible&#13;&#10;- computer-using&#13;&#10;- test-time training?, etc.&#13;&#10;"><img
src="let's%20dive%20deep%20into%20chatgpt.files/image054.png" width=1497
height=787
alt="- tasks -&gt; agents (long, coherent, error-correcting contexts)现在大多数情况是将单个任务交给模型，仍然需要我们来organize a coherent execution of tasks，模型不能修正错误，尤其是长时间运行。Agents随着时间执行任务，人类负责监督，agents偶尔会报告进展，&#13;&#10;"><br>
</nobr></div>

</div>

</div>

<div>

<p style='margin:0in'>&nbsp;</p>

<p style='text-align:left;margin:0in;font-family:Arial;font-size:9pt;
color:#969696;direction:ltr'>已使用 OneNote 创建。</p>

</div>

</body>

</html>
