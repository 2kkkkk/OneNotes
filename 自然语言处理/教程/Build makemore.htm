<html xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:dt="uuid:C2F41010-65B3-11d1-A29F-00AA00C14882"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta name=ProgId content=OneNote.File>
<meta name=Generator content="Microsoft OneNote 15">
<link id=Main-File rel=Main-File href="Build%20makemore.htm">
<link rel=File-List href="Build%20makemore.files/filelist.xml">
</head>

<body lang=zh-CN style='font-family:微软雅黑;font-size:36.0pt'>

<div style='direction:ltr;border-width:100%'>

<div style='direction:ltr;margin-top:0in;margin-left:0in;width:44.9722in'>

<div style='direction:ltr;margin-top:0in;margin-left:0in;width:2.3652in'>

<p style='margin:0in;font-family:"Calibri Light";font-size:20.0pt' lang=en-US>Build
makemore</p>

</div>

<div style='direction:ltr;margin-top:.0423in;margin-left:0in;width:1.5979in'>

<p style='margin:0in;font-size:10.0pt;color:#767676'><span style='font-family:
Calibri'>2025</span><span style='font-family:"Microsoft YaHei"'>年</span><span
style='font-family:Calibri'>4</span><span style='font-family:"Microsoft YaHei"'>月</span><span
style='font-family:Calibri'>2</span><span style='font-family:"Microsoft YaHei"'>日</span></p>

<p style='margin:0in;font-family:Calibri;font-size:10.0pt;color:#767676'>18:49</p>

</div>

<div style='direction:ltr;margin-top:.3652in;margin-left:0in;width:12.102in'>

<p style='margin:0in;font-family:Calibri;font-size:48.0pt'><span lang=en-US>MLP:
</span><span style='font-weight:bold;color:#0F0F0F' lang=zh-CN>Activations
&amp; Gradients, BatchNorm</span></p>

</div>

<div style='direction:ltr;margin-top:.9291in;margin-left:.25in;width:42.7951in'>

<p style='margin:0in;font-family:微软雅黑;font-size:36.0pt'><span style='font-weight:
bold'>关于初始化神经网络的参数，网络训练之前应该考虑的问题：</span></p>

<p style='margin:0in;font-family:微软雅黑;font-size:36.0pt'>what loss do you expect
at initialization? like uniform distribution of 27 characters</p>

<p style='margin:0in;font-family:微软雅黑;font-size:36.0pt'>You don't want the
output of tanh function to be land on -1 or 1 region, because the gradient of
tanh in this region is zero , therefore can't pass gradient backwards.</p>

<p style='margin:0in;font-family:微软雅黑;font-size:36.0pt'>tanh ,sigmoid,
relu都有这个问题，即都存在导数等于0的平坦区域。</p>

<p style='margin:0in;font-family:微软雅黑;font-size:36.0pt'>如果一个神经元dead，说明无论input如何变化
，该神经元的激活值，即tanh(h)总是落在导数为0的区域，使得梯度无法传播。相当于never activates.</p>

<p style='margin:0in;font-family:微软雅黑;font-size:36.0pt'>神经元永久失活可能发生在Initialization时，也可能发生在训练过程中，比如在某个batch上更新参数后，下一个batch中所有样本在该神经元上的激活值都落在了导数为0的区域，反向传播也就不会对参数进行更新，也就相当于这个batch没有对参数进行更新,下下一个batch进来，可能还是不会更新...一直重复下去。</p>

<p style='margin:0in;font-family:微软雅黑;font-size:36.0pt'>关于Initialization的另一点，x和w都是标准正态分布,x@w的均值是0，但是方差不是1，要想使(x@w)的分布也是均值为0，方差为1（落在激活区域），那么在初始化w的时候就不应该用标准正态分布初始化，详见pytorch中的kaiming_normal及paper。</p>

<p style='margin:0in;font-family:微软雅黑;font-size:36.0pt'>&nbsp;</p>

</div>

<div style='direction:ltr;margin-top:.4138in;margin-left:.25in;width:42.0909in'>

<p style='margin:0in;font-family:微软雅黑;font-size:36.0pt'>Batchnorm的出发点，你想让隐藏层神经层的激活值分布在均值=0，方差为1，为什么不直接对激活值做standardization呢。因此Batchnorm层在激活函数之前
。</p>

<p style='margin:0in;font-family:微软雅黑;font-size:36.0pt'>我们只是想在initialization的时候，不想让神经元失活，而不是一直让神经元的预激活值在训练过程中保持标准正态分布，我们想让back
propogate告诉我们预激活值应该如何分布，所以batchnorm有两个可学习参数。</p>

<p style='margin:0in;font-family:微软雅黑;font-size:36.0pt'>Batchnorm的inefficiency。Batch中的每个样本不是独立的了，当前样本的forward
pass需要计算与batch中其他样本的均值，也就将batch中各个样本的前向传播耦合在了一起，而非独立进行。另一方面，当batch中其他样本换成别的样本，forward
pass 也会发生变化 ，也就是与batch如何划分有关，使得模型训练变得不desireable。</p>

</div>

<div style='direction:ltr;margin-top:1.2034in;margin-left:.4791in;width:44.493in'><nobr><img
src="Build%20makemore.files/image001.png" width=2880 height=1440
alt="# Let's train a deeper network&#13;# The classes we create here are the same API as nn.Module in PyTorch&#13;&#13;&#10;class Linear:&#13;  &#13;  def __init__(self, fan_in, fan_out, bias=True):&#13;    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5  W的初始化，假设x是normal gaussion分布，w也是normal gaussion，那么w*x的均值仍是0，但标准差不是1，会被放大，也就是说w*x不是norm gaussion。所以w 除以fan_in**0.5的目的是 防止wx的分布不是均值为0，方差为1的分布。关于为什么除以fan_in**0.5，而与fan_out的维度无关，我的想法是因为x是个n*fan_in矩阵，w是个fan_in*fan_out矩阵，w*x中的每个元素是两个向量的点积，向量的长度是fan_in，所以w*x的缩放取决于fan_in，而与fan_out无关。pytorch里是均匀分布。但是当网络很深时，it becomes harder and harder to precisely set the weights and bias in such way that activations are roughly uniform, 从而引出了batchnorm。&#13;&#10;&#13;    self.bias = torch.zeros(fan_out) if bias else None&#13;  &#13;  def __call__(self, x):&#13;    self.out = x @ self.weight&#13;    if self.bias is not None:&#13;      self.out += self.bias&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight] + ([] if self.bias is None else [self.bias])&#13;&#10;"><img
src="Build%20makemore.files/image002.png" width=2880 height=1440
alt="# Let's train a deeper network&#13;# The classes we create here are the same API as nn.Module in PyTorch&#13;&#13;&#10;class Linear:&#13;  &#13;  def __init__(self, fan_in, fan_out, bias=True):&#13;    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5  W的初始化，假设x是normal gaussion分布，w也是normal gaussion，那么w*x的均值仍是0，但标准差不是1，会被放大，也就是说w*x不是norm gaussion。所以w 除以fan_in**0.5的目的是 防止wx的分布不是均值为0，方差为1的分布。关于为什么除以fan_in**0.5，而与fan_out的维度无关，我的想法是因为x是个n*fan_in矩阵，w是个fan_in*fan_out矩阵，w*x中的每个元素是两个向量的点积，向量的长度是fan_in，所以w*x的缩放取决于fan_in，而与fan_out无关。pytorch里是均匀分布。但是当网络很深时，it becomes harder and harder to precisely set the weights and bias in such way that activations are roughly uniform, 从而引出了batchnorm。&#13;&#10;&#13;    self.bias = torch.zeros(fan_out) if bias else None&#13;  &#13;  def __call__(self, x):&#13;    self.out = x @ self.weight&#13;    if self.bias is not None:&#13;      self.out += self.bias&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight] + ([] if self.bias is None else [self.bias])&#13;&#10;"><img
src="Build%20makemore.files/image003.png" width=647 height=1440
alt="# Let's train a deeper network&#13;# The classes we create here are the same API as nn.Module in PyTorch&#13;&#13;&#10;class Linear:&#13;  &#13;  def __init__(self, fan_in, fan_out, bias=True):&#13;    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5  W的初始化，假设x是normal gaussion分布，w也是normal gaussion，那么w*x的均值仍是0，但标准差不是1，会被放大，也就是说w*x不是norm gaussion。所以w 除以fan_in**0.5的目的是 防止wx的分布不是均值为0，方差为1的分布。关于为什么除以fan_in**0.5，而与fan_out的维度无关，我的想法是因为x是个n*fan_in矩阵，w是个fan_in*fan_out矩阵，w*x中的每个元素是两个向量的点积，向量的长度是fan_in，所以w*x的缩放取决于fan_in，而与fan_out无关。pytorch里是均匀分布。但是当网络很深时，it becomes harder and harder to precisely set the weights and bias in such way that activations are roughly uniform, 从而引出了batchnorm。&#13;&#10;&#13;    self.bias = torch.zeros(fan_out) if bias else None&#13;  &#13;  def __call__(self, x):&#13;    self.out = x @ self.weight&#13;    if self.bias is not None:&#13;      self.out += self.bias&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight] + ([] if self.bias is None else [self.bias])&#13;&#10;"><br>
<img src="Build%20makemore.files/image004.png" width=2880 height=1440
alt="&#13;    self.bias = torch.zeros(fan_out) if bias else None&#13;  &#13;  def __call__(self, x):&#13;    self.out = x @ self.weight&#13;    if self.bias is not None:&#13;      self.out += self.bias&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight] + ([] if self.bias is None else [self.bias])&#13;&#10;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):  ★ pytorch默认是momentum是0.1，公式是(1-momentum)*v + momentum*v' 。&#13;&#10;momentum的设置需要考虑batchsize，如果batchsize   很大，那么每个batch的均值和方差基本没啥变化，momentum可以设置的大一些，如0.1，表示用最近10个batch的平均，如果batchsize很小，那么momentum应该设置的小一些。&#13;&#10;&#13;    self.eps = eps&#13;    self.momentum = momentum &#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      xmean = x.mean(0, keepdim=True) # batch mean&#13;      xvar = x.var(0, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><img
src="Build%20makemore.files/image005.png" width=2880 height=1440
alt="&#13;    self.bias = torch.zeros(fan_out) if bias else None&#13;  &#13;  def __call__(self, x):&#13;    self.out = x @ self.weight&#13;    if self.bias is not None:&#13;      self.out += self.bias&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight] + ([] if self.bias is None else [self.bias])&#13;&#10;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):  ★ pytorch默认是momentum是0.1，公式是(1-momentum)*v + momentum*v' 。&#13;&#10;momentum的设置需要考虑batchsize，如果batchsize   很大，那么每个batch的均值和方差基本没啥变化，momentum可以设置的大一些，如0.1，表示用最近10个batch的平均，如果batchsize很小，那么momentum应该设置的小一些。&#13;&#10;&#13;    self.eps = eps&#13;    self.momentum = momentum &#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      xmean = x.mean(0, keepdim=True) # batch mean&#13;      xvar = x.var(0, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><img
src="Build%20makemore.files/image006.png" width=647 height=1440
alt="&#13;    self.bias = torch.zeros(fan_out) if bias else None&#13;  &#13;  def __call__(self, x):&#13;    self.out = x @ self.weight&#13;    if self.bias is not None:&#13;      self.out += self.bias&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight] + ([] if self.bias is None else [self.bias])&#13;&#10;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):  ★ pytorch默认是momentum是0.1，公式是(1-momentum)*v + momentum*v' 。&#13;&#10;momentum的设置需要考虑batchsize，如果batchsize   很大，那么每个batch的均值和方差基本没啥变化，momentum可以设置的大一些，如0.1，表示用最近10个batch的平均，如果batchsize很小，那么momentum应该设置的小一些。&#13;&#10;&#13;    self.eps = eps&#13;    self.momentum = momentum &#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      xmean = x.mean(0, keepdim=True) # batch mean&#13;      xvar = x.var(0, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><br>
<img src="Build%20makemore.files/image007.png" width=2880 height=1440
alt="&#13;    self.eps = eps&#13;    self.momentum = momentum &#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      xmean = x.mean(0, keepdim=True) # batch mean&#13;      xvar = x.var(0, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><img
src="Build%20makemore.files/image008.png" width=2880 height=1440
alt="&#13;    self.eps = eps&#13;    self.momentum = momentum &#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      xmean = x.mean(0, keepdim=True) # batch mean&#13;      xvar = x.var(0, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="&#13;    self.eps = eps&#13;    self.momentum = momentum &#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      xmean = x.mean(0, keepdim=True) # batch mean&#13;      xvar = x.var(0, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><br>
<img src="Build%20makemore.files/image010.png" width=2880 height=1440
alt="&#13;    self.eps = eps&#13;    self.momentum = momentum &#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      xmean = x.mean(0, keepdim=True) # batch mean&#13;      xvar = x.var(0, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;class Tanh:&#13;  def __call__(self, x):&#13;    self.out = torch.tanh(x)&#13;    return self.out&#13;  def parameters(self):&#13;    return []&#13;&#13;&#10;n_embd = 10 # the dimensionality of the character embedding vectors&#13;n_hidden = 100 # the number of neurons in the hidden layer of the MLP&#13;g = torch.Generator().manual_seed(2147483647) # for reproducibility&#13;&#13;&#10;C = torch.randn((vocab_size, n_embd),            generator=g)&#13;layers = [&#13;  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), ★ 加了batchnorm层之后 ，之前的全连接层就不需要加偏置b了。即bias=False&#13;&#10;"><img
src="Build%20makemore.files/image011.png" width=2880 height=1440
alt="&#13;    self.eps = eps&#13;    self.momentum = momentum &#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      xmean = x.mean(0, keepdim=True) # batch mean&#13;      xvar = x.var(0, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;class Tanh:&#13;  def __call__(self, x):&#13;    self.out = torch.tanh(x)&#13;    return self.out&#13;  def parameters(self):&#13;    return []&#13;&#13;&#10;n_embd = 10 # the dimensionality of the character embedding vectors&#13;n_hidden = 100 # the number of neurons in the hidden layer of the MLP&#13;g = torch.Generator().manual_seed(2147483647) # for reproducibility&#13;&#13;&#10;C = torch.randn((vocab_size, n_embd),            generator=g)&#13;layers = [&#13;  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), ★ 加了batchnorm层之后 ，之前的全连接层就不需要加偏置b了。即bias=False&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="&#13;    self.eps = eps&#13;    self.momentum = momentum &#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      xmean = x.mean(0, keepdim=True) # batch mean&#13;      xvar = x.var(0, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;class Tanh:&#13;  def __call__(self, x):&#13;    self.out = torch.tanh(x)&#13;    return self.out&#13;  def parameters(self):&#13;    return []&#13;&#13;&#10;n_embd = 10 # the dimensionality of the character embedding vectors&#13;n_hidden = 100 # the number of neurons in the hidden layer of the MLP&#13;g = torch.Generator().manual_seed(2147483647) # for reproducibility&#13;&#13;&#10;C = torch.randn((vocab_size, n_embd),            generator=g)&#13;layers = [&#13;  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), ★ 加了batchnorm层之后 ，之前的全连接层就不需要加偏置b了。即bias=False&#13;&#10;"><br>
<img src="Build%20makemore.files/image012.png" width=2880 height=1440
alt="C = torch.randn((vocab_size, n_embd),            generator=g)&#13;layers = [&#13;  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), ★ 加了batchnorm层之后 ，之前的全连接层就不需要加偏置b了。即bias=False&#13;&#10;&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),&#13;]&#13;# layers = [&#13;#   Linear(n_embd * block_size, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, vocab_size),&#13;# ]&#13;&#10;"><img
src="Build%20makemore.files/image013.png" width=2880 height=1440
alt="C = torch.randn((vocab_size, n_embd),            generator=g)&#13;layers = [&#13;  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), ★ 加了batchnorm层之后 ，之前的全连接层就不需要加偏置b了。即bias=False&#13;&#10;&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),&#13;]&#13;# layers = [&#13;#   Linear(n_embd * block_size, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, vocab_size),&#13;# ]&#13;&#10;"><img
src="Build%20makemore.files/image014.png" width=647 height=1440
alt="C = torch.randn((vocab_size, n_embd),            generator=g)&#13;layers = [&#13;  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), ★ 加了batchnorm层之后 ，之前的全连接层就不需要加偏置b了。即bias=False&#13;&#10;&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),&#13;]&#13;# layers = [&#13;#   Linear(n_embd * block_size, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, vocab_size),&#13;# ]&#13;&#10;"><br>
<img src="Build%20makemore.files/image015.png" width=2880 height=1440
alt="&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),&#13;]&#13;# layers = [&#13;#   Linear(n_embd * block_size, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, vocab_size),&#13;# ]&#13;&#10;with torch.no_grad():&#13;  # last layer: make less confident&#13;  layers[-1].gamma *= 0.1&#13;  #layers[-1].weight *= 0.1&#13;  # all other layers: apply gain&#13;  for layer in layers[:-1]:&#13;    if isinstance(layer, Linear):&#13;      layer.weight *= 1.0 #5/3                     ★当不加batchnorm时，需要乘以5/3相当于把w放大了一些，原因是tanh将输出缩小到-1，1,所以需要放大来抵消tanh的缩小作用。加了batchnorm后，就不需要*5/3了，乘以1保持原状即可。5/3被称为tanh的gain，linear和conv,sigmoid的gain都是1, relu的gain是根号2。&#13;&#10;parameters = [C] + [p for layer in layers for p in layer.parameters()]&#13;print(sum(p.nelement() for p in parameters)) # number of parameters in total&#13;for p in parameters:&#13;  p.requires_grad = True&#13;&#10;&#13;&#10;# same optimization as last time&#13;max_steps = 200000&#13;batch_size = 32&#13;lossi = []&#13;ud = []&#13;&#13;&#10;"><img
src="Build%20makemore.files/image016.png" width=2880 height=1440
alt="&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),&#13;]&#13;# layers = [&#13;#   Linear(n_embd * block_size, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, vocab_size),&#13;# ]&#13;&#10;with torch.no_grad():&#13;  # last layer: make less confident&#13;  layers[-1].gamma *= 0.1&#13;  #layers[-1].weight *= 0.1&#13;  # all other layers: apply gain&#13;  for layer in layers[:-1]:&#13;    if isinstance(layer, Linear):&#13;      layer.weight *= 1.0 #5/3                     ★当不加batchnorm时，需要乘以5/3相当于把w放大了一些，原因是tanh将输出缩小到-1，1,所以需要放大来抵消tanh的缩小作用。加了batchnorm后，就不需要*5/3了，乘以1保持原状即可。5/3被称为tanh的gain，linear和conv,sigmoid的gain都是1, relu的gain是根号2。&#13;&#10;parameters = [C] + [p for layer in layers for p in layer.parameters()]&#13;print(sum(p.nelement() for p in parameters)) # number of parameters in total&#13;for p in parameters:&#13;  p.requires_grad = True&#13;&#10;# same optimization as last time&#13;max_steps = 200000&#13;batch_size = 32&#13;lossi = []&#13;ud = []&#13;&#13;&#10;"><img
src="Build%20makemore.files/image017.png" width=647 height=1440
alt="&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),&#13;]&#13;# layers = [&#13;#   Linear(n_embd * block_size, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, n_hidden), Tanh(),&#13;#   Linear(           n_hidden, vocab_size),&#13;# ]&#13;&#10;with torch.no_grad():&#13;  # last layer: make less confident&#13;  layers[-1].gamma *= 0.1&#13;  #layers[-1].weight *= 0.1&#13;  # all other layers: apply gain&#13;  for layer in layers[:-1]:&#13;    if isinstance(layer, Linear):&#13;      layer.weight *= 1.0 #5/3                     ★当不加batchnorm时，需要乘以5/3相当于把w放大了一些，原因是tanh将输出缩小到-1，1,所以需要放大来抵消tanh的缩小作用。加了batchnorm后，就不需要*5/3了，乘以1保持原状即可。5/3被称为tanh的gain，linear和conv,sigmoid的gain都是1, relu的gain是根号2。&#13;&#10;parameters = [C] + [p for layer in layers for p in layer.parameters()]&#13;print(sum(p.nelement() for p in parameters)) # number of parameters in total&#13;for p in parameters:&#13;  p.requires_grad = True&#13;&#10;# same optimization as last time&#13;max_steps = 200000&#13;batch_size = 32&#13;lossi = []&#13;ud = []&#13;&#13;&#10;"><br>
<img src="Build%20makemore.files/image018.png" width=2880 height=1440
alt="# same optimization as last time&#13;max_steps = 200000&#13;batch_size = 32&#13;lossi = []&#13;ud = []&#13;&#13;&#10;for i in range(max_steps):&#13;  &#13;  # minibatch construct&#13;  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)&#13;  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y        Xb的shape: (batch_size, context length)  &#13;  # forward pass&#13;  emb = C[Xb] # embed the characters into vectors   emb的shape: (batch_size, context length,embedding_length)  &#13;&#10;&#13;  x = emb.view(emb.shape[0], -1) # concatenate the vectors x的shape: (batch_size,context_length * embedding_length)，也就相当于把context_length个字母的embedding concat起来。也就是将context中所有token（这里是单个字母）的embedding concat起来，平等地对待，而没有按照每个token的embedding区分对待。wavenet缓解了这个问题，wavenet逐级分层地将context中所有token的Embedding fuse起来，而不是一下子concat。wavenet每次concat 2个相邻的token的embedding，经过 context_length/2次操作，实现了将所有token concat起来的目标。&#13;&#10;"><img
src="Build%20makemore.files/image019.png" width=2880 height=1440
alt="# same optimization as last time&#13;max_steps = 200000&#13;batch_size = 32&#13;lossi = []&#13;ud = []&#13;&#13;&#10;for i in range(max_steps):&#13;  &#13;  # minibatch construct&#13;  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)&#13;  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y        Xb的shape: (batch_size, context length)  &#13;  # forward pass&#13;  emb = C[Xb] # embed the characters into vectors   emb的shape: (batch_size, context length,embedding_length)  &#13;&#10;&#13;  x = emb.view(emb.shape[0], -1) # concatenate the vectors x的shape: (batch_size,context_length * embedding_length)，也就相当于把context_length个字母的embedding concat起来。也就是将context中所有token（这里是单个字母）的embedding concat起来，平等地对待，而没有按照每个token的embedding区分对待。wavenet缓解了这个问题，wavenet逐级分层地将context中所有token的Embedding fuse起来，而不是一下子concat。wavenet每次concat 2个相邻的token的embedding，经过 context_length/2次操作，实现了将所有token concat起来的目标。&#13;&#10;"><img
src="Build%20makemore.files/image020.png" width=647 height=1440
alt="# same optimization as last time&#13;max_steps = 200000&#13;batch_size = 32&#13;lossi = []&#13;ud = []&#13;&#13;&#10;for i in range(max_steps):&#13;  &#13;  # minibatch construct&#13;  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)&#13;  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y        Xb的shape: (batch_size, context length)  &#13;  # forward pass&#13;  emb = C[Xb] # embed the characters into vectors   emb的shape: (batch_size, context length,embedding_length)  &#13;&#10;&#13;  x = emb.view(emb.shape[0], -1) # concatenate the vectors x的shape: (batch_size,context_length * embedding_length)，也就相当于把context_length个字母的embedding concat起来。也就是将context中所有token（这里是单个字母）的embedding concat起来，平等地对待，而没有按照每个token的embedding区分对待。wavenet缓解了这个问题，wavenet逐级分层地将context中所有token的Embedding fuse起来，而不是一下子concat。wavenet每次concat 2个相邻的token的embedding，经过 context_length/2次操作，实现了将所有token concat起来的目标。&#13;&#10;"><br>
<img src="Build%20makemore.files/image021.png" width=2880 height=1440
alt="&#13;  x = emb.view(emb.shape[0], -1) # concatenate the vectors x的shape: (batch_size,context_length * embedding_length)，也就相当于把context_length个字母的embedding concat起来。也就是将context中所有token（这里是单个字母）的embedding concat起来，平等地对待，而没有按照每个token的embedding区分对待。wavenet缓解了这个问题，wavenet逐级分层地将context中所有token的Embedding fuse起来，而不是一下子concat。wavenet每次concat 2个相邻的token的embedding，经过 context_length/2次操作，实现了将所有token concat起来的目标。&#13;&#10;&#13;&#10;&#13;  for layer in layers:&#13;    x = layer(x)&#13;  loss = F.cross_entropy(x, Yb) # loss function&#13;  &#13;  # backward pass&#13;  for layer in layers:&#13;    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph    默认情况下，非叶节点的梯度值在反向传播过程中使用完后就会被清除，不会被保留。只有叶节点的梯度值能够被保留下来。 叶子节点是由用户创建的，如w和b，非叶子节点即中间节点如这里的out，在loss.backwad执行后，默认不会保存非叶子节点的grad。&#13;  for p in parameters:&#13;    p.grad = None&#13;  loss.backward()&#13;  &#13;  # update&#13;  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay&#13;  for p in parameters:&#13;    p.data += -lr * p.grad&#13;&#10;"><img
src="Build%20makemore.files/image022.png" width=2880 height=1440
alt="&#13;  x = emb.view(emb.shape[0], -1) # concatenate the vectors x的shape: (batch_size,context_length * embedding_length)，也就相当于把context_length个字母的embedding concat起来。也就是将context中所有token（这里是单个字母）的embedding concat起来，平等地对待，而没有按照每个token的embedding区分对待。wavenet缓解了这个问题，wavenet逐级分层地将context中所有token的Embedding fuse起来，而不是一下子concat。wavenet每次concat 2个相邻的token的embedding，经过 context_length/2次操作，实现了将所有token concat起来的目标。&#13;&#10;&#13;  for layer in layers:&#13;    x = layer(x)&#13;  loss = F.cross_entropy(x, Yb) # loss function&#13;  &#13;  # backward pass&#13;  for layer in layers:&#13;    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph    默认情况下，非叶节点的梯度值在反向传播过程中使用完后就会被清除，不会被保留。只有叶节点的梯度值能够被保留下来。 叶子节点是由用户创建的，如w和b，非叶子节点即中间节点如这里的out，在loss.backwad执行后，默认不会保存非叶子节点的grad。&#13;  for p in parameters:&#13;    p.grad = None&#13;  loss.backward()&#13;  &#13;  # update&#13;  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay&#13;  for p in parameters:&#13;    p.data += -lr * p.grad&#13;&#10;"><img
src="Build%20makemore.files/image023.png" width=647 height=1440
alt="&#13;  x = emb.view(emb.shape[0], -1) # concatenate the vectors x的shape: (batch_size,context_length * embedding_length)，也就相当于把context_length个字母的embedding concat起来。也就是将context中所有token（这里是单个字母）的embedding concat起来，平等地对待，而没有按照每个token的embedding区分对待。wavenet缓解了这个问题，wavenet逐级分层地将context中所有token的Embedding fuse起来，而不是一下子concat。wavenet每次concat 2个相邻的token的embedding，经过 context_length/2次操作，实现了将所有token concat起来的目标。&#13;&#10;&#13;  for layer in layers:&#13;    x = layer(x)&#13;  loss = F.cross_entropy(x, Yb) # loss function&#13;  &#13;  # backward pass&#13;  for layer in layers:&#13;    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph    默认情况下，非叶节点的梯度值在反向传播过程中使用完后就会被清除，不会被保留。只有叶节点的梯度值能够被保留下来。 叶子节点是由用户创建的，如w和b，非叶子节点即中间节点如这里的out，在loss.backwad执行后，默认不会保存非叶子节点的grad。&#13;  for p in parameters:&#13;    p.grad = None&#13;  loss.backward()&#13;  &#13;  # update&#13;  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay&#13;  for p in parameters:&#13;    p.data += -lr * p.grad&#13;&#10;"><br>
<img src="Build%20makemore.files/image024.png" width=2880 height=1440
alt="&#13;  for layer in layers:&#13;    x = layer(x)&#13;  loss = F.cross_entropy(x, Yb) # loss function&#13;  &#13;  # backward pass&#13;  for layer in layers:&#13;    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph    默认情况下，非叶节点的梯度值在反向传播过程中使用完后就会被清除，不会被保留。只有叶节点的梯度值能够被保留下来。 叶子节点是由用户创建的，如w和b，非叶子节点即中间节点如这里的out，在loss.backwad执行后，默认不会保存非叶子节点的grad。&#13;  for p in parameters:&#13;    p.grad = None&#13;  loss.backward()&#13;  &#13;  # update&#13;  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay&#13;  for p in parameters:&#13;    p.data += -lr * p.grad&#13;&#10;# track stats&#13;  if i % 10000 == 0: # print every once in a while&#13;    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')&#13;  lossi.append(loss.log10().item())&#13;  with torch.no_grad():&#13;    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])&#13;&#13;&#10;if i &gt;= 1000:&#13;    break # AFTER_DEBUG: would take out obviously to run full optimization&#13;&#10;&#13;&#10;可视化各层的激活值，forward方向&#13;&#10;# visualize histograms&#13;plt.figure(figsize=(20, 4)) # width and height of the plot&#13;legends = []&#13;for i, layer in enumerate(layers[:-1]): # note: exclude the output layer&#13;  if isinstance(layer, Tanh):&#13;    t = layer.out&#13;    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() &gt; 0.97).float().mean()*100))&#13;    hy, hx = torch.histogram(t, density=True)&#13;    plt.plot(hx[:-1].detach(), hy.detach())&#13;    legends.append(f'layer {i} ({layer.__class__.__name__}')&#13;plt.legend(legends);&#13;plt.title('activation distribution')&#13;&#10;"><img
src="Build%20makemore.files/image025.png" width=2880 height=1440
alt="&#13;  for layer in layers:&#13;    x = layer(x)&#13;  loss = F.cross_entropy(x, Yb) # loss function&#13;  &#13;  # backward pass&#13;  for layer in layers:&#13;    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph    默认情况下，非叶节点的梯度值在反向传播过程中使用完后就会被清除，不会被保留。只有叶节点的梯度值能够被保留下来。 叶子节点是由用户创建的，如w和b，非叶子节点即中间节点如这里的out，在loss.backwad执行后，默认不会保存非叶子节点的grad。&#13;  for p in parameters:&#13;    p.grad = None&#13;  loss.backward()&#13;  &#13;  # update&#13;  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay&#13;  for p in parameters:&#13;    p.data += -lr * p.grad&#13;&#10;# track stats&#13;  if i % 10000 == 0: # print every once in a while&#13;    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')&#13;  lossi.append(loss.log10().item())&#13;  with torch.no_grad():&#13;    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])&#13;&#13;&#10;if i &gt;= 1000:&#13;    break # AFTER_DEBUG: would take out obviously to run full optimization&#13;&#10;# visualize histograms&#13;plt.figure(figsize=(20, 4)) # width and height of the plot&#13;legends = []&#13;for i, layer in enumerate(layers[:-1]): # note: exclude the output layer&#13;  if isinstance(layer, Tanh):&#13;    t = layer.out&#13;    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() &gt; 0.97).float().mean()*100))&#13;    hy, hx = torch.histogram(t, density=True)&#13;    plt.plot(hx[:-1].detach(), hy.detach())&#13;    legends.append(f'layer {i} ({layer.__class__.__name__}')&#13;plt.legend(legends);&#13;plt.title('activation distribution')&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="&#13;  for layer in layers:&#13;    x = layer(x)&#13;  loss = F.cross_entropy(x, Yb) # loss function&#13;  &#13;  # backward pass&#13;  for layer in layers:&#13;    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph    默认情况下，非叶节点的梯度值在反向传播过程中使用完后就会被清除，不会被保留。只有叶节点的梯度值能够被保留下来。 叶子节点是由用户创建的，如w和b，非叶子节点即中间节点如这里的out，在loss.backwad执行后，默认不会保存非叶子节点的grad。&#13;  for p in parameters:&#13;    p.grad = None&#13;  loss.backward()&#13;  &#13;  # update&#13;  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay&#13;  for p in parameters:&#13;    p.data += -lr * p.grad&#13;&#10;# track stats&#13;  if i % 10000 == 0: # print every once in a while&#13;    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')&#13;  lossi.append(loss.log10().item())&#13;  with torch.no_grad():&#13;    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])&#13;&#13;&#10;if i &gt;= 1000:&#13;    break # AFTER_DEBUG: would take out obviously to run full optimization&#13;&#10;# visualize histograms&#13;plt.figure(figsize=(20, 4)) # width and height of the plot&#13;legends = []&#13;for i, layer in enumerate(layers[:-1]): # note: exclude the output layer&#13;  if isinstance(layer, Tanh):&#13;    t = layer.out&#13;    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() &gt; 0.97).float().mean()*100))&#13;    hy, hx = torch.histogram(t, density=True)&#13;    plt.plot(hx[:-1].detach(), hy.detach())&#13;    legends.append(f'layer {i} ({layer.__class__.__name__}')&#13;plt.legend(legends);&#13;plt.title('activation distribution')&#13;&#10;"><br>
<img src="Build%20makemore.files/image026.png" width=2880 height=1440
alt="# visualize histograms&#13;plt.figure(figsize=(20, 4)) # width and height of the plot&#13;legends = []&#13;for i, layer in enumerate(layers[:-1]): # note: exclude the output layer&#13;  if isinstance(layer, Tanh):&#13;    t = layer.out&#13;    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() &gt; 0.97).float().mean()*100))&#13;    hy, hx = torch.histogram(t, density=True)&#13;    plt.plot(hx[:-1].detach(), hy.detach())&#13;    legends.append(f'layer {i} ({layer.__class__.__name__}')&#13;plt.legend(legends);&#13;plt.title('activation distribution')&#13;&#10;我们希望各层的激活值，也就是tanh的输出不要落在接近-1或1的区域，因为该区域梯度为0，&#13;&#10;我们不希望如下的结果，各层的饱和率太高，各层的激活值大部分落在接近-1和1的区域&#13;&#10;layer 1 (      Tanh): mean -0.04, std 0.80, saturated: 30.34%&#13;&#10;layer 3 (      Tanh): mean -0.01, std 0.77, saturated: 20.75%&#13;&#10;layer 5 (      Tanh): mean -0.01, std 0.78, saturated: 22.75%&#13;&#10;layer 7 (      Tanh): mean -0.05, std 0.78, saturated: 21.50%&#13;&#10;layer 9 (      Tanh): mean -0.00, std 0.77, saturated: 20.38%&#13;&#10;"><img
src="Build%20makemore.files/image027.png" width=2880 height=1440
alt="# visualize histograms&#13;plt.figure(figsize=(20, 4)) # width and height of the plot&#13;legends = []&#13;for i, layer in enumerate(layers[:-1]): # note: exclude the output layer&#13;  if isinstance(layer, Tanh):&#13;    t = layer.out&#13;    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() &gt; 0.97).float().mean()*100))&#13;    hy, hx = torch.histogram(t, density=True)&#13;    plt.plot(hx[:-1].detach(), hy.detach())&#13;    legends.append(f'layer {i} ({layer.__class__.__name__}')&#13;plt.legend(legends);&#13;plt.title('activation distribution')&#13;&#10;我们希望各层的激活值，也就是tanh的输出不要落在接近-1或1的区域，因为该区域梯度为0，&#13;&#10;"><img
src="Build%20makemore.files/image028.png" width=647 height=1440
alt="# visualize histograms&#13;plt.figure(figsize=(20, 4)) # width and height of the plot&#13;legends = []&#13;for i, layer in enumerate(layers[:-1]): # note: exclude the output layer&#13;  if isinstance(layer, Tanh):&#13;    t = layer.out&#13;    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() &gt; 0.97).float().mean()*100))&#13;    hy, hx = torch.histogram(t, density=True)&#13;    plt.plot(hx[:-1].detach(), hy.detach())&#13;    legends.append(f'layer {i} ({layer.__class__.__name__}')&#13;plt.legend(legends);&#13;plt.title('activation distribution')&#13;&#10;"><br>
<img src="Build%20makemore.files/image029.png" width=2880 height=1440
alt="layer 9 (      Tanh): mean -0.00, std 0.77, saturated: 20.38%&#13;&#10;未命名图片 &#13;&#10;我们希望如下图的结果，&#13;&#10;layer 1 (      Tanh): mean -0.04, std 0.64, saturated: 5.19%&#13;&#10;layer 3 (      Tanh): mean -0.01, std 0.54, saturated: 0.41%&#13;&#10;layer 5 (      Tanh): mean +0.01, std 0.53, saturated: 0.47%&#13;&#10;layer 7 (      Tanh): mean -0.02, std 0.53, saturated: 0.28%&#13;&#10;"><img
src="Build%20makemore.files/image030.png" width=2880 height=1440 alt="未命名图片 &#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440><br>
<img src="Build%20makemore.files/image031.png" width=2880 height=1440
alt="layer 7 (      Tanh): mean -0.02, std 0.53, saturated: 0.28%&#13;&#10;layer 9 (      Tanh): mean +0.01, std 0.54, saturated: 0.25%&#13;&#10;未命名图片.png 计算机生成了可选文字:&#10;0．8&#10;一L00&#10;一0寻5&#10;一0巧0&#10;一05&#10;activationdistribution&#10;0．00&#10;0．25&#10;0．50&#10;layer1(Tanh&#10;layer3(Tanh&#10;layer5(Tanh&#10;_；layer7(Tanh&#10;layer9(Tanh&#10;L00&#13;&#10;&#13;&#10;可视化各输出的梯度值，backward方向&#13;&#10;# visualize histograms&#13;plt.figure(figsize=(20, 4)) # width and height of the plot&#13;legends = []&#13;for i, layer in enumerate(layers[:-1]): # note: exclude the output layer&#13;  if isinstance(layer, Tanh):&#13;    t = layer.out.grad&#13;    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))&#13;    hy, hx = torch.histogram(t, density=True)&#13;    plt.plot(hx[:-1].detach(), hy.detach())&#13;    legends.append(f'layer {i} ({layer.__class__.__name__}')&#13;plt.legend(legends);&#13;plt.title('gradient distribution')&#13;&#10;"><img
src="Build%20makemore.files/image032.png" width=2880 height=1440
alt="未命名图片.png 计算机生成了可选文字:&#10;0．8&#10;一L00&#10;一0寻5&#10;一0巧0&#10;一05&#10;activationdistribution&#10;0．00&#10;0．25&#10;0．50&#10;layer1(Tanh&#10;layer3(Tanh&#10;layer5(Tanh&#10;_；layer7(Tanh&#10;layer9(Tanh&#10;L00&#13;&#10;# visualize histograms&#13;plt.figure(figsize=(20, 4)) # width and height of the plot&#13;legends = []&#13;for i, layer in enumerate(layers[:-1]): # note: exclude the output layer&#13;  if isinstance(layer, Tanh):&#13;    t = layer.out.grad&#13;    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))&#13;    hy, hx = torch.histogram(t, density=True)&#13;    plt.plot(hx[:-1].detach(), hy.detach())&#13;    legends.append(f'layer {i} ({layer.__class__.__name__}')&#13;plt.legend(legends);&#13;plt.title('gradient distribution')&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="# visualize histograms&#13;plt.figure(figsize=(20, 4)) # width and height of the plot&#13;legends = []&#13;for i, layer in enumerate(layers[:-1]): # note: exclude the output layer&#13;  if isinstance(layer, Tanh):&#13;    t = layer.out.grad&#13;    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))&#13;    hy, hx = torch.histogram(t, density=True)&#13;    plt.plot(hx[:-1].detach(), hy.detach())&#13;    legends.append(f'layer {i} ({layer.__class__.__name__}')&#13;plt.legend(legends);&#13;plt.title('gradient distribution')&#13;&#10;"><br>
<img src="Build%20makemore.files/image033.png" width=2880 height=1440
alt="# visualize histograms&#13;plt.figure(figsize=(20, 4)) # width and height of the plot&#13;legends = []&#13;for i, layer in enumerate(layers[:-1]): # note: exclude the output layer&#13;  if isinstance(layer, Tanh):&#13;    t = layer.out.grad&#13;    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))&#13;    hy, hx = torch.histogram(t, density=True)&#13;    plt.plot(hx[:-1].detach(), hy.detach())&#13;    legends.append(f'layer {i} ({layer.__class__.__name__}')&#13;plt.legend(legends);&#13;plt.title('gradient distribution')&#13;&#10;&#13;&#10;&#13;&#10;我们希望各层out的梯度值，是均匀的，而不是有些层梯度过大，有些层梯度过小，&#13;&#10;我们不希望如下 的结果：前面几层梯度大，越往后面层梯度过小&#13;&#10;layer 1 (      Tanh): mean -0.000086, std 1.620851e-02&#13;&#10;layer 3 (      Tanh): mean +0.000071, std 1.012546e-02&#13;&#10;layer 5 (      Tanh): mean +0.000057, std 5.541695e-03&#13;&#10;"><img
src="Build%20makemore.files/image034.png" width=2880 height=1440
alt="# visualize histograms&#13;plt.figure(figsize=(20, 4)) # width and height of the plot&#13;legends = []&#13;for i, layer in enumerate(layers[:-1]): # note: exclude the output layer&#13;  if isinstance(layer, Tanh):&#13;    t = layer.out.grad&#13;    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))&#13;    hy, hx = torch.histogram(t, density=True)&#13;    plt.plot(hx[:-1].detach(), hy.detach())&#13;    legends.append(f'layer {i} ({layer.__class__.__name__}')&#13;plt.legend(legends);&#13;plt.title('gradient distribution')&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="# visualize histograms&#13;plt.figure(figsize=(20, 4)) # width and height of the plot&#13;legends = []&#13;for i, layer in enumerate(layers[:-1]): # note: exclude the output layer&#13;  if isinstance(layer, Tanh):&#13;    t = layer.out.grad&#13;    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))&#13;    hy, hx = torch.histogram(t, density=True)&#13;    plt.plot(hx[:-1].detach(), hy.detach())&#13;    legends.append(f'layer {i} ({layer.__class__.__name__}')&#13;plt.legend(legends);&#13;plt.title('gradient distribution')&#13;&#10;"><br>
<img src="Build%20makemore.files/image035.png" width=2880 height=1440
alt="layer 5 (      Tanh): mean +0.000057, std 5.541695e-03&#13;&#10;layer 7 (      Tanh): mean +0.000013, std 3.469306e-03&#13;&#10;layer 9 (      Tanh): mean +0.000030, std 2.318119e-03&#13;&#10;未命名图片.png 计算机生成了可选文字:&#10;200&#10;150&#10;100&#10;50&#10;0&#10;一075&#10;一050&#10;一025&#10;gradientdistribution&#10;0．000&#10;0．025&#10;0．050&#10;layer1(Tanh&#10;layer3(Tanh&#10;layer5(Tanh&#10;layer7(Tanh&#10;layer9(Tanh&#10;0．075&#13;&#10;我们希望如下的结果：&#13;&#10;layer 1 (      Tanh): mean +0.000033, std 2.641852e-03&#13;&#10;layer 3 (      Tanh): mean +0.000043, std 2.440831e-03&#13;&#10;layer 5 (      Tanh): mean -0.000004, std 2.338152e-03&#13;&#10;"><img
src="Build%20makemore.files/image036.png" width=2880 height=1440
alt="未命名图片.png 计算机生成了可选文字:&#10;200&#10;150&#10;100&#10;50&#10;0&#10;一075&#10;一050&#10;一025&#10;gradientdistribution&#10;0．000&#10;0．025&#10;0．050&#10;layer1(Tanh&#10;layer3(Tanh&#10;layer5(Tanh&#10;layer7(Tanh&#10;layer9(Tanh&#10;0．075&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440><br>
<img src="Build%20makemore.files/image037.png" width=2880 height=1440
alt="layer 5 (      Tanh): mean -0.000004, std 2.338152e-03&#13;&#10;layer 7 (      Tanh): mean +0.000006, std 2.283551e-03&#13;&#10;layer 9 (      Tanh): mean +0.000040, std 2.059027e-03&#13;&#10;未命名图片.png 计算机生成了可选文字:&#10;250&#10;200&#10;150&#10;100&#10;50&#10;0&#10;一020&#10;一015&#10;一010&#10;gradientdistribution&#10;一005&#10;0．000&#10;0．005&#10;0．010&#10;layer1(Tanh&#10;layer3(Tanh&#10;layer5(Tanh&#10;layer7(Tanh&#10;layer9(Tanh&#10;0．015&#13;&#10;&#13;&#10;&#13;&#10;参数w的scale及w的梯度的scale&#13;&#10;# visualize histograms&#13;&#10;"><img
src="Build%20makemore.files/image038.png" width=2880 height=1440
alt="未命名图片.png 计算机生成了可选文字:&#10;250&#10;200&#10;150&#10;100&#10;50&#10;0&#10;一020&#10;一015&#10;一010&#10;gradientdistribution&#10;一005&#10;0．000&#10;0．005&#10;0．010&#10;layer1(Tanh&#10;layer3(Tanh&#10;layer5(Tanh&#10;layer7(Tanh&#10;layer9(Tanh&#10;0．015&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440><br>
<img src="Build%20makemore.files/image039.png" width=2880 height=1440
alt="# visualize histograms&#13;&#10;plt.figure(figsize=(20, 4)) # width and height of the plot&#13;&#10;legends = []&#13;&#10;for i,p in enumerate(parameters):&#13;&#10;&nbsp; t = p.grad&#13;&#10;&nbsp; if p.ndim == 2:&#13;&#10;&nbsp; &nbsp; print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))&#13;&#10;&nbsp; &nbsp; hy, hx = torch.histogram(t, density=True)&#13;&#10;&nbsp; &nbsp; plt.plot(hx[:-1].detach(), hy.detach())&#13;&#10;&nbsp; &nbsp; legends.append(f'{i} {tuple(p.shape)}')&#13;&#10;plt.legend(legends)&#13;&#10;plt.title('weights gradient distribution');&#13;&#10;"><img
src="Build%20makemore.files/image040.png" width=2880 height=1440
alt="&nbsp; &nbsp; print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440><br>
<img src="Build%20makemore.files/image041.png" width=2880 height=1440
alt="plt.title('weights gradient distribution');&#13;&#10;&#13;&#10;layer 1 (      Tanh): mean +0.000033, std 2.641852e-03&#13;&#10;layer 3 (      Tanh): mean +0.000043, std 2.440831e-03&#13;&#10;layer 5 (      Tanh): mean -0.000004, std 2.338152e-03&#13;&#10;layer 7 (      Tanh): mean +0.000006, std 2.283551e-03&#13;&#10;layer 9 (      Tanh): mean +0.000040, std 2.059027e-03&#13;&#10;我们希望w.grad.std()/w.std()的ratio在1e-3左右。也就是不希望w.grad相比于w变化太快。&#13;&#10;&#13;&#10;总结：加了batchnorm后，模型训练 变得robust，不需要考虑如何初始化参数，以及gain，w=w*fan_in**0.05这些。&#13;&#10;&#13;&#10;&#13;&#10;&#13;&#10;Building makemore Part 5: Building a WaveNet&#13;&#10;"><img
src="Build%20makemore.files/image042.png" width=2880 height=1440
alt="我们希望w.grad.std()/w.std()的ratio在1e-3左右。也就是不希望w.grad相比于w变化太快。&#13;&#10;总结：加了batchnorm后，模型训练 变得robust，不需要考虑如何初始化参数，以及gain，w=w*fan_in**0.05这些。&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440><br>
<img src="Build%20makemore.files/image043.png" width=2880 height=1440
alt="Building makemore Part 5: Building a WaveNet&#13;&#10;未命名图片.png 计算机生成了可选文字:&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;O&#10;0&#10;0&#10;0&#10;O&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;C)&#10;C)&#10;0&#10;0&#10;0&#10;CD&#10;0&#10;0&#10;Dilatlon=8&#10;HiddenLayer&#10;Dilation=4&#10;HiddenLayer&#10;Dilatlon=2&#10;HiddenLayer&#10;Dilatlon=1&#10;Figure3：VisualizationOfastackofdcausalconvolutionallayers.&#13;&#10;★上图中红框表示 linear层，8个linear层的参数是共享的。&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;"><img
src="Build%20makemore.files/image044.png" width=2880 height=1440
alt="未命名图片.png 计算机生成了可选文字:&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;O&#10;0&#10;0&#10;0&#10;O&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;0&#10;C)&#10;C)&#10;0&#10;0&#10;0&#10;CD&#10;0&#10;0&#10;Dilatlon=8&#10;HiddenLayer&#10;Dilation=4&#10;HiddenLayer&#10;Dilatlon=2&#10;HiddenLayer&#10;Dilatlon=1&#10;Figure3：VisualizationOfastackofdcausalconvolutionallayers.&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440><br>
<img src="Build%20makemore.files/image045.png" width=2880 height=1440
alt="★上图中红框表示 linear层，8个linear层的参数是共享的。&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;Linear层的定义不需要改变，linear层的核心是self.out = x @ self.weight，本质是pytorch矩阵乘法，而矩阵乘法不需要两个矩阵都是2维的，只需要保证第一个矩阵的最后一维的维数和第二个矩阵第一个维度的维数相等即可。利用这个性质，当需要将输入x两两一组进行分组，然后将分组后的数据输入到Linear层时，可以只创建一个linear层，即参数共享+并行，而不是创建context_length/2个linear层，（当然也可以实现创建context_length/2个linear层）。&#13;&#10;class Linear:&#13;  &#13;  def __init__(self, fan_in, fan_out, bias=True):&#13;    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init&#13;    self.bias = torch.zeros(fan_out) if bias else None&#13;  &#13;  def __call__(self, x):&#13;    self.out = x @ self.weight&#13;    if self.bias is not None:&#13;      self.out += self.bias&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight] + ([] if self.bias is None else [self.bias])&#13;&#13;&#10;"><img
src="Build%20makemore.files/image046.png" width=2880 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;Linear层的定义不需要改变，linear层的核心是self.out = x @ self.weight，本质是pytorch矩阵乘法，而矩阵乘法不需要两个矩阵都是2维的，只需要保证第一个矩阵的最后一维的维数和第二个矩阵第一个维度的维数相等即可。利用这个性质，当需要将输入x两两一组进行分组，然后将分组后的数据输入到Linear层时，可以只创建一个linear层，即参数共享+并行，而不是创建context_length/2个linear层，（当然也可以实现创建context_length/2个linear层）。&#13;&#10;class Linear:&#13;  &#13;  def __init__(self, fan_in, fan_out, bias=True):&#13;    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init&#13;    self.bias = torch.zeros(fan_out) if bias else None&#13;  &#13;  def __call__(self, x):&#13;    self.out = x @ self.weight&#13;    if self.bias is not None:&#13;      self.out += self.bias&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight] + ([] if self.bias is None else [self.bias])&#13;&#13;&#10;"><img
src="Build%20makemore.files/image047.png" width=647 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;Linear层的定义不需要改变，linear层的核心是self.out = x @ self.weight，本质是pytorch矩阵乘法，而矩阵乘法不需要两个矩阵都是2维的，只需要保证第一个矩阵的最后一维的维数和第二个矩阵第一个维度的维数相等即可。利用这个性质，当需要将输入x两两一组进行分组，然后将分组后的数据输入到Linear层时，可以只创建一个linear层，即参数共享+并行，而不是创建context_length/2个linear层，（当然也可以实现创建context_length/2个linear层）。&#13;&#10;class Linear:&#13;  &#13;  def __init__(self, fan_in, fan_out, bias=True):&#13;    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init&#13;    self.bias = torch.zeros(fan_out) if bias else None&#13;  &#13;  def __call__(self, x):&#13;    self.out = x @ self.weight&#13;    if self.bias is not None:&#13;      self.out += self.bias&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight] + ([] if self.bias is None else [self.bias])&#13;&#13;&#10;"><br>
<img src="Build%20makemore.files/image048.png" width=2880 height=1440
alt="class Linear:&#13;  &#13;  def __init__(self, fan_in, fan_out, bias=True):&#13;    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init&#13;    self.bias = torch.zeros(fan_out) if bias else None&#13;  &#13;  def __call__(self, x):&#13;    self.out = x @ self.weight&#13;    if self.bias is not None:&#13;      self.out += self.bias&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight] + ([] if self.bias is None else [self.bias])&#13;&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):&#13;    self.eps = eps&#13;    self.momentum = momentum&#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      if x.ndim == 2:&#13;        dim = 0&#13;      elif x.ndim == 3:    ★注意这里batchnorm的实现和我理解总结的pytorch的batchnorm用法一致，nlp中feature个数是n_embedding的个数，统计均值时，计算的时当前batch中所有句子的所有token的某一维embedding的均值。&#13;        dim = (0,1)&#13;      xmean = x.mean(dim, keepdim=True) # batch mean &#13;      xvar = x.var(dim, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><img
src="Build%20makemore.files/image049.png" width=2880 height=1440
alt="class Linear:&#13;  &#13;  def __init__(self, fan_in, fan_out, bias=True):&#13;    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init&#13;    self.bias = torch.zeros(fan_out) if bias else None&#13;  &#13;  def __call__(self, x):&#13;    self.out = x @ self.weight&#13;    if self.bias is not None:&#13;      self.out += self.bias&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight] + ([] if self.bias is None else [self.bias])&#13;&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):&#13;    self.eps = eps&#13;    self.momentum = momentum&#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      if x.ndim == 2:&#13;        dim = 0&#13;      elif x.ndim == 3:    ★注意这里batchnorm的实现和我理解总结的pytorch的batchnorm用法一致，nlp中feature个数是n_embedding的个数，统计均值时，计算的时当前batch中所有句子的所有token的某一维embedding的均值。&#13;        dim = (0,1)&#13;      xmean = x.mean(dim, keepdim=True) # batch mean &#13;      xvar = x.var(dim, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="class Linear:&#13;  &#13;  def __init__(self, fan_in, fan_out, bias=True):&#13;    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init&#13;    self.bias = torch.zeros(fan_out) if bias else None&#13;  &#13;  def __call__(self, x):&#13;    self.out = x @ self.weight&#13;    if self.bias is not None:&#13;      self.out += self.bias&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight] + ([] if self.bias is None else [self.bias])&#13;&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):&#13;    self.eps = eps&#13;    self.momentum = momentum&#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      if x.ndim == 2:&#13;        dim = 0&#13;      elif x.ndim == 3:    ★注意这里batchnorm的实现和我理解总结的pytorch的batchnorm用法一致，nlp中feature个数是n_embedding的个数，统计均值时，计算的时当前batch中所有句子的所有token的某一维embedding的均值。&#13;        dim = (0,1)&#13;      xmean = x.mean(dim, keepdim=True) # batch mean &#13;      xvar = x.var(dim, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><br>
<img src="Build%20makemore.files/image050.png" width=2880 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):&#13;    self.eps = eps&#13;    self.momentum = momentum&#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      if x.ndim == 2:&#13;        dim = 0&#13;      elif x.ndim == 3:    ★注意这里batchnorm的实现和我理解总结的pytorch的batchnorm用法一致，nlp中feature个数是n_embedding的个数，统计均值时，计算的时当前batch中所有句子的所有token的某一维embedding的均值。&#13;        dim = (0,1)&#13;      xmean = x.mean(dim, keepdim=True) # batch mean &#13;      xvar = x.var(dim, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><img
src="Build%20makemore.files/image044.png" width=2880 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):&#13;    self.eps = eps&#13;    self.momentum = momentum&#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      if x.ndim == 2:&#13;        dim = 0&#13;      elif x.ndim == 3:    ★注意这里batchnorm的实现和我理解总结的pytorch的batchnorm用法一致，nlp中feature个数是n_embedding的个数，统计均值时，计算的时当前batch中所有句子的所有token的某一维embedding的均值。&#13;        dim = (0,1)&#13;      xmean = x.mean(dim, keepdim=True) # batch mean &#13;      xvar = x.var(dim, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):&#13;    self.eps = eps&#13;    self.momentum = momentum&#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      if x.ndim == 2:&#13;        dim = 0&#13;      elif x.ndim == 3:    ★注意这里batchnorm的实现和我理解总结的pytorch的batchnorm用法一致，nlp中feature个数是n_embedding的个数，统计均值时，计算的时当前batch中所有句子的所有token的某一维embedding的均值。&#13;        dim = (0,1)&#13;      xmean = x.mean(dim, keepdim=True) # batch mean &#13;      xvar = x.var(dim, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><br>
<img src="Build%20makemore.files/image051.png" width=2880 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):&#13;    self.eps = eps&#13;    self.momentum = momentum&#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      if x.ndim == 2:&#13;        dim = 0&#13;      elif x.ndim == 3:    ★注意这里batchnorm的实现和我理解总结的pytorch的batchnorm用法一致，nlp中feature个数是n_embedding的个数，统计均值时，计算的时当前batch中所有句子的所有token的某一维embedding的均值。&#13;        dim = (0,1)&#13;      xmean = x.mean(dim, keepdim=True) # batch mean &#13;      xvar = x.var(dim, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><img
src="Build%20makemore.files/image052.png" width=2880 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):&#13;    self.eps = eps&#13;    self.momentum = momentum&#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      if x.ndim == 2:&#13;        dim = 0&#13;      elif x.ndim == 3:    ★注意这里batchnorm的实现和我理解总结的pytorch的batchnorm用法一致，nlp中feature个数是n_embedding的个数，统计均值时，计算的时当前batch中所有句子的所有token的某一维embedding的均值。&#13;        dim = (0,1)&#13;      xmean = x.mean(dim, keepdim=True) # batch mean &#13;      xvar = x.var(dim, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><img
src="Build%20makemore.files/image053.png" width=647 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):&#13;    self.eps = eps&#13;    self.momentum = momentum&#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      if x.ndim == 2:&#13;        dim = 0&#13;      elif x.ndim == 3:    ★注意这里batchnorm的实现和我理解总结的pytorch的batchnorm用法一致，nlp中feature个数是n_embedding的个数，统计均值时，计算的时当前batch中所有句子的所有token的某一维embedding的均值。&#13;        dim = (0,1)&#13;      xmean = x.mean(dim, keepdim=True) # batch mean &#13;      xvar = x.var(dim, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;"><br>
<img src="Build%20makemore.files/image054.png" width=2880 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):&#13;    self.eps = eps&#13;    self.momentum = momentum&#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      if x.ndim == 2:&#13;        dim = 0&#13;      elif x.ndim == 3:    ★注意这里batchnorm的实现和我理解总结的pytorch的batchnorm用法一致，nlp中feature个数是n_embedding的个数，统计均值时，计算的时当前batch中所有句子的所有token的某一维embedding的均值。&#13;        dim = (0,1)&#13;      xmean = x.mean(dim, keepdim=True) # batch mean &#13;      xvar = x.var(dim, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;class Tanh:&#13;  def __call__(self, x):&#13;    self.out = torch.tanh(x)&#13;    return self.out&#13;  def parameters(self):&#13;    return []&#13;&#13;&#10;"><img
src="Build%20makemore.files/image055.png" width=2880 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):&#13;    self.eps = eps&#13;    self.momentum = momentum&#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      if x.ndim == 2:&#13;        dim = 0&#13;      elif x.ndim == 3:    ★注意这里batchnorm的实现和我理解总结的pytorch的batchnorm用法一致，nlp中feature个数是n_embedding的个数，统计均值时，计算的时当前batch中所有句子的所有token的某一维embedding的均值。&#13;        dim = (0,1)&#13;      xmean = x.mean(dim, keepdim=True) # batch mean &#13;      xvar = x.var(dim, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;class Tanh:&#13;  def __call__(self, x):&#13;    self.out = torch.tanh(x)&#13;    return self.out&#13;  def parameters(self):&#13;    return []&#13;&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class BatchNorm1d:&#13;  &#13;  def __init__(self, dim, eps=1e-5, momentum=0.1):&#13;    self.eps = eps&#13;    self.momentum = momentum&#13;    self.training = True&#13;    # parameters (trained with backprop)&#13;    self.gamma = torch.ones(dim)&#13;    self.beta = torch.zeros(dim)&#13;    # buffers (trained with a running 'momentum update')&#13;    self.running_mean = torch.zeros(dim)&#13;    self.running_var = torch.ones(dim)&#13;  &#13;  def __call__(self, x):&#13;    # calculate the forward pass&#13;    if self.training:&#13;      if x.ndim == 2:&#13;        dim = 0&#13;      elif x.ndim == 3:    ★注意这里batchnorm的实现和我理解总结的pytorch的batchnorm用法一致，nlp中feature个数是n_embedding的个数，统计均值时，计算的时当前batch中所有句子的所有token的某一维embedding的均值。&#13;        dim = (0,1)&#13;      xmean = x.mean(dim, keepdim=True) # batch mean &#13;      xvar = x.var(dim, keepdim=True) # batch variance&#13;    else:&#13;      xmean = self.running_mean&#13;      xvar = self.running_var&#13;    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance&#13;    self.out = self.gamma * xhat + self.beta&#13;    # update the buffers&#13;    if self.training:&#13;      with torch.no_grad():&#13;        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean&#13;        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.gamma, self.beta]&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;class Tanh:&#13;  def __call__(self, x):&#13;    self.out = torch.tanh(x)&#13;    return self.out&#13;  def parameters(self):&#13;    return []&#13;&#13;&#10;"><br>
<img src="Build%20makemore.files/image056.png" width=2880 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class Tanh:&#13;  def __call__(self, x):&#13;    self.out = torch.tanh(x)&#13;    return self.out&#13;  def parameters(self):&#13;    return []&#13;&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;&#10;定义Embedding层，原来Embedding层就是个权重表。&#13;&#10;class Embedding:&#13;  &#13;  def __init__(self, num_embeddings, embedding_dim):&#13;    self.weight = torch.randn((num_embeddings, embedding_dim))&#13;    &#13;  def __call__(self, IX):&#13;    self.out = self.weight[IX]&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight]&#13;&#13;&#10;"><img
src="Build%20makemore.files/image057.png" width=2880 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class Tanh:&#13;  def __call__(self, x):&#13;    self.out = torch.tanh(x)&#13;    return self.out&#13;  def parameters(self):&#13;    return []&#13;&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;&#10;class Embedding:&#13;  &#13;  def __init__(self, num_embeddings, embedding_dim):&#13;    self.weight = torch.randn((num_embeddings, embedding_dim))&#13;    &#13;  def __call__(self, IX):&#13;    self.out = self.weight[IX]&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight]&#13;&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class Tanh:&#13;  def __call__(self, x):&#13;    self.out = torch.tanh(x)&#13;    return self.out&#13;  def parameters(self):&#13;    return []&#13;&#13;&#10;class Embedding:&#13;  &#13;  def __init__(self, num_embeddings, embedding_dim):&#13;    self.weight = torch.randn((num_embeddings, embedding_dim))&#13;    &#13;  def __call__(self, IX):&#13;    self.out = self.weight[IX]&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight]&#13;&#13;&#10;"><br>
<img src="Build%20makemore.files/image058.png" width=2880 height=1440
alt="class Embedding:&#13;  &#13;  def __init__(self, num_embeddings, embedding_dim):&#13;    self.weight = torch.randn((num_embeddings, embedding_dim))&#13;    &#13;  def __call__(self, IX):&#13;    self.out = self.weight[IX]&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight]&#13;&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;&#10;FlattenConsecutive的作用：输入x的shape是 B,T,C，B是batchsize，T是context_length，C是n_embedding，在build makemore mlp部分中，是将context_length个字母的embedding直接concat起来，即输出的shape是 B，T*C。而wavenet现在需要将context_length个字母进行相邻字母两两组合，输出的shape是 B，T//2，C*2&#13;&#10;class FlattenConsecutive:&#13;  &#13;  def __init__(self, n):&#13;    self.n = n&#13;    &#13;  def __call__(self, x):&#13;    B, T, C = x.shape&#13;    x = x.view(B, T//self.n, C*self.n)&#13;    if x.shape[1] == 1:&#13;      x = x.squeeze(1)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return []&#13;&#10;"><img
src="Build%20makemore.files/image059.png" width=2880 height=1440
alt="class Embedding:&#13;  &#13;  def __init__(self, num_embeddings, embedding_dim):&#13;    self.weight = torch.randn((num_embeddings, embedding_dim))&#13;    &#13;  def __call__(self, IX):&#13;    self.out = self.weight[IX]&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight]&#13;&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;&#10;FlattenConsecutive的作用：输入x的shape是 B,T,C，B是batchsize，T是context_length，C是n_embedding，在build makemore mlp部分中，是将context_length个字母的embedding直接concat起来，即输出的shape是 B，T*C。而wavenet现在需要将context_length个字母进行相邻字母两两组合，输出的shape是 B，T//2，C*2&#13;&#10;class FlattenConsecutive:&#13;  &#13;  def __init__(self, n):&#13;    self.n = n&#13;    &#13;  def __call__(self, x):&#13;    B, T, C = x.shape&#13;    x = x.view(B, T//self.n, C*self.n)&#13;    if x.shape[1] == 1:&#13;      x = x.squeeze(1)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return []&#13;&#10;"><img
src="Build%20makemore.files/image060.png" width=647 height=1440
alt="class Embedding:&#13;  &#13;  def __init__(self, num_embeddings, embedding_dim):&#13;    self.weight = torch.randn((num_embeddings, embedding_dim))&#13;    &#13;  def __call__(self, IX):&#13;    self.out = self.weight[IX]&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return [self.weight]&#13;&#13;&#10;FlattenConsecutive的作用：输入x的shape是 B,T,C，B是batchsize，T是context_length，C是n_embedding，在build makemore mlp部分中，是将context_length个字母的embedding直接concat起来，即输出的shape是 B，T*C。而wavenet现在需要将context_length个字母进行相邻字母两两组合，输出的shape是 B，T//2，C*2&#13;&#10;class FlattenConsecutive:&#13;  &#13;  def __init__(self, n):&#13;    self.n = n&#13;    &#13;  def __call__(self, x):&#13;    B, T, C = x.shape&#13;    x = x.view(B, T//self.n, C*self.n)&#13;    if x.shape[1] == 1:&#13;      x = x.squeeze(1)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return []&#13;&#10;"><br>
<img src="Build%20makemore.files/image061.png" width=2880 height=1440
alt="class FlattenConsecutive:&#13;  &#13;  def __init__(self, n):&#13;    self.n = n&#13;    &#13;  def __call__(self, x):&#13;    B, T, C = x.shape&#13;    x = x.view(B, T//self.n, C*self.n)&#13;    if x.shape[1] == 1:&#13;      x = x.squeeze(1)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return []&#13;&#10;"><img
src="Build%20makemore.files/image044.png" width=2880 height=1440
alt="class FlattenConsecutive:&#13;  &#13;  def __init__(self, n):&#13;    self.n = n&#13;    &#13;  def __call__(self, x):&#13;    B, T, C = x.shape&#13;    x = x.view(B, T//self.n, C*self.n)&#13;    if x.shape[1] == 1:&#13;      x = x.squeeze(1)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return []&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="class FlattenConsecutive:&#13;  &#13;  def __init__(self, n):&#13;    self.n = n&#13;    &#13;  def __call__(self, x):&#13;    B, T, C = x.shape&#13;    x = x.view(B, T//self.n, C*self.n)&#13;    if x.shape[1] == 1:&#13;      x = x.squeeze(1)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return []&#13;&#10;"><br>
<img src="Build%20makemore.files/image062.png" width=2880 height=1440
alt="class FlattenConsecutive:&#13;  &#13;  def __init__(self, n):&#13;    self.n = n&#13;    &#13;  def __call__(self, x):&#13;    B, T, C = x.shape&#13;    x = x.view(B, T//self.n, C*self.n)&#13;    if x.shape[1] == 1:&#13;      x = x.squeeze(1)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return []&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;class Sequential:&#13;  &#13;  def __init__(self, layers):&#13;    self.layers = layers&#13;  &#13;  def __call__(self, x):&#13;    for layer in self.layers:&#13;      x = layer(x)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    # get parameters of all layers and stretch them out into one list&#13;    return [p for layer in self.layers for p in layer.parameters()]&#13;&#10;"><img
src="Build%20makemore.files/image063.png" width=2880 height=1440
alt="class FlattenConsecutive:&#13;  &#13;  def __init__(self, n):&#13;    self.n = n&#13;    &#13;  def __call__(self, x):&#13;    B, T, C = x.shape&#13;    x = x.view(B, T//self.n, C*self.n)&#13;    if x.shape[1] == 1:&#13;      x = x.squeeze(1)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return []&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;class Sequential:&#13;  &#13;  def __init__(self, layers):&#13;    self.layers = layers&#13;  &#13;  def __call__(self, x):&#13;    for layer in self.layers:&#13;      x = layer(x)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    # get parameters of all layers and stretch them out into one list&#13;    return [p for layer in self.layers for p in layer.parameters()]&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="class FlattenConsecutive:&#13;  &#13;  def __init__(self, n):&#13;    self.n = n&#13;    &#13;  def __call__(self, x):&#13;    B, T, C = x.shape&#13;    x = x.view(B, T//self.n, C*self.n)&#13;    if x.shape[1] == 1:&#13;      x = x.squeeze(1)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    return []&#13;&#10;# -----------------------------------------------------------------------------------------------&#13;class Sequential:&#13;  &#13;  def __init__(self, layers):&#13;    self.layers = layers&#13;  &#13;  def __call__(self, x):&#13;    for layer in self.layers:&#13;      x = layer(x)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    # get parameters of all layers and stretch them out into one list&#13;    return [p for layer in self.layers for p in layer.parameters()]&#13;&#10;"><br>
<img src="Build%20makemore.files/image064.png" width=2880 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class Sequential:&#13;  &#13;  def __init__(self, layers):&#13;    self.layers = layers&#13;  &#13;  def __call__(self, x):&#13;    for layer in self.layers:&#13;      x = layer(x)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    # get parameters of all layers and stretch them out into one list&#13;    return [p for layer in self.layers for p in layer.parameters()]&#13;&#10;&#13;&#10;&#13;&#10;开始构建模型&#13;&#10;torch.manual_seed(42); # seed rng for reproducibility&#13;&#10;# original network&#13;# n_embd = 10 # the dimensionality of the character embedding vectors&#13;# n_hidden = 300 # the number of neurons in the hidden layer of the MLP&#13;# model = Sequential([&#13;#   Embedding(vocab_size, n_embd),&#13;#   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;#   Linear(n_hidden, vocab_size),&#13;# ])&#13;&#13;&#10;"><img
src="Build%20makemore.files/image044.png" width=2880 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class Sequential:&#13;  &#13;  def __init__(self, layers):&#13;    self.layers = layers&#13;  &#13;  def __call__(self, x):&#13;    for layer in self.layers:&#13;      x = layer(x)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    # get parameters of all layers and stretch them out into one list&#13;    return [p for layer in self.layers for p in layer.parameters()]&#13;&#10;# original network&#13;# n_embd = 10 # the dimensionality of the character embedding vectors&#13;# n_hidden = 300 # the number of neurons in the hidden layer of the MLP&#13;# model = Sequential([&#13;#   Embedding(vocab_size, n_embd),&#13;#   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;#   Linear(n_hidden, vocab_size),&#13;# ])&#13;&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="# -----------------------------------------------------------------------------------------------&#13;class Sequential:&#13;  &#13;  def __init__(self, layers):&#13;    self.layers = layers&#13;  &#13;  def __call__(self, x):&#13;    for layer in self.layers:&#13;      x = layer(x)&#13;    self.out = x&#13;    return self.out&#13;  &#13;  def parameters(self):&#13;    # get parameters of all layers and stretch them out into one list&#13;    return [p for layer in self.layers for p in layer.parameters()]&#13;&#10;# original network&#13;# n_embd = 10 # the dimensionality of the character embedding vectors&#13;# n_hidden = 300 # the number of neurons in the hidden layer of the MLP&#13;# model = Sequential([&#13;#   Embedding(vocab_size, n_embd),&#13;#   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;#   Linear(n_hidden, vocab_size),&#13;# ])&#13;&#13;&#10;"><br>
<img src="Build%20makemore.files/image065.png" width=2880 height=1440
alt="# original network&#13;# n_embd = 10 # the dimensionality of the character embedding vectors&#13;# n_hidden = 300 # the number of neurons in the hidden layer of the MLP&#13;# model = Sequential([&#13;#   Embedding(vocab_size, n_embd),&#13;#   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;#   Linear(n_hidden, vocab_size),&#13;# ])&#13;&#13;&#10;# hierarchical network&#13;&#10;block_size = 8 # context length: how many characters do we take to predict the next one?&#13;&#10;n_embd = 24 # the dimensionality of the character embedding vectors&#13;n_hidden = 128 # the number of neurons in the hidden layer of the MLP&#13;model = Sequential([&#13;  Embedding(vocab_size, n_embd),&#13;  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(n_hidden, vocab_size),&#13;])&#13;&#10;"><img
src="Build%20makemore.files/image066.png" width=2880 height=1440
alt="# original network&#13;# n_embd = 10 # the dimensionality of the character embedding vectors&#13;# n_hidden = 300 # the number of neurons in the hidden layer of the MLP&#13;# model = Sequential([&#13;#   Embedding(vocab_size, n_embd),&#13;#   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;#   Linear(n_hidden, vocab_size),&#13;# ])&#13;&#13;&#10;block_size = 8 # context length: how many characters do we take to predict the next one?&#13;&#10;n_embd = 24 # the dimensionality of the character embedding vectors&#13;n_hidden = 128 # the number of neurons in the hidden layer of the MLP&#13;model = Sequential([&#13;  Embedding(vocab_size, n_embd),&#13;  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(n_hidden, vocab_size),&#13;])&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="# original network&#13;# n_embd = 10 # the dimensionality of the character embedding vectors&#13;# n_hidden = 300 # the number of neurons in the hidden layer of the MLP&#13;# model = Sequential([&#13;#   Embedding(vocab_size, n_embd),&#13;#   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;#   Linear(n_hidden, vocab_size),&#13;# ])&#13;&#13;&#10;n_embd = 24 # the dimensionality of the character embedding vectors&#13;n_hidden = 128 # the number of neurons in the hidden layer of the MLP&#13;model = Sequential([&#13;  Embedding(vocab_size, n_embd),&#13;  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(n_hidden, vocab_size),&#13;])&#13;&#10;"><br>
<img src="Build%20makemore.files/image067.png" width=2880 height=1440
alt="n_embd = 24 # the dimensionality of the character embedding vectors&#13;n_hidden = 128 # the number of neurons in the hidden layer of the MLP&#13;model = Sequential([&#13;  Embedding(vocab_size, n_embd),&#13;  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(n_hidden, vocab_size),&#13;])&#13;&#10;输入x的shape是32，8，27，其中32是batch_size，8是block_size，27是vocab_size。经过model的变化历程为：首先经过Embedding层，shape变为32，8，24，24是n_embedding，经过 FlattenConsecutive(2)变为32，4，24*2，经过 Linear(n_embd * 2, n_hidden, bias=False)变为32，4，n_hidden，然后Batchnorm和tanh层不改变shape，再次经过FlattenConsecutive(2)变为32，2，n_hidden*2，经过 Linear(n_hidden* 2, n_hidden, bias=False)变为32，2，n_hidden，然后Batchnorm和tanh层不改变shape，再次经过FlattenConsecutive(2)变为32，1，n_hidden*2，FlattenConsecutive会把32，1，n_hidden*2  squeeze 成32,n_hidden*2，经过 Linear(n_hidden* 2, n_hidden, bias=False)变为 32，n_hidden，然后Batchnorm和tanh层不改变shape，最后经过 Linear(n_hidden, vocab_size)变为 32,vocab_size&#13;&#10;&#13;&#10;# parameter init&#13;with torch.no_grad():&#13;  model.layers[-1].weight *= 0.1 # last layer make less confident&#13;&#10;"><img
src="Build%20makemore.files/image068.png" width=2880 height=1440
alt="n_embd = 24 # the dimensionality of the character embedding vectors&#13;n_hidden = 128 # the number of neurons in the hidden layer of the MLP&#13;model = Sequential([&#13;  Embedding(vocab_size, n_embd),&#13;  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(n_hidden, vocab_size),&#13;])&#13;&#10;输入x的shape是32，8，27，其中32是batch_size，8是block_size，27是vocab_size。经过model的变化历程为：首先经过Embedding层，shape变为32，8，24，24是n_embedding，经过 FlattenConsecutive(2)变为32，4，24*2，经过 Linear(n_embd * 2, n_hidden, bias=False)变为32，4，n_hidden，然后Batchnorm和tanh层不改变shape，再次经过FlattenConsecutive(2)变为32，2，n_hidden*2，经过 Linear(n_hidden* 2, n_hidden, bias=False)变为32，2，n_hidden，然后Batchnorm和tanh层不改变shape，再次经过FlattenConsecutive(2)变为32，1，n_hidden*2，FlattenConsecutive会把32，1，n_hidden*2  squeeze 成32,n_hidden*2，经过 Linear(n_hidden* 2, n_hidden, bias=False)变为 32，n_hidden，然后Batchnorm和tanh层不改变shape，最后经过 Linear(n_hidden, vocab_size)变为 32,vocab_size&#13;&#10;# parameter init&#13;with torch.no_grad():&#13;  model.layers[-1].weight *= 0.1 # last layer make less confident&#13;&#10;"><img
src="Build%20makemore.files/image069.png" width=647 height=1440
alt="n_embd = 24 # the dimensionality of the character embedding vectors&#13;n_hidden = 128 # the number of neurons in the hidden layer of the MLP&#13;model = Sequential([&#13;  Embedding(vocab_size, n_embd),&#13;  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),&#13;  Linear(n_hidden, vocab_size),&#13;])&#13;&#10;输入x的shape是32，8，27，其中32是batch_size，8是block_size，27是vocab_size。经过model的变化历程为：首先经过Embedding层，shape变为32，8，24，24是n_embedding，经过 FlattenConsecutive(2)变为32，4，24*2，经过 Linear(n_embd * 2, n_hidden, bias=False)变为32，4，n_hidden，然后Batchnorm和tanh层不改变shape，再次经过FlattenConsecutive(2)变为32，2，n_hidden*2，经过 Linear(n_hidden* 2, n_hidden, bias=False)变为32，2，n_hidden，然后Batchnorm和tanh层不改变shape，再次经过FlattenConsecutive(2)变为32，1，n_hidden*2，FlattenConsecutive会把32，1，n_hidden*2  squeeze 成32,n_hidden*2，经过 Linear(n_hidden* 2, n_hidden, bias=False)变为 32，n_hidden，然后Batchnorm和tanh层不改变shape，最后经过 Linear(n_hidden, vocab_size)变为 32,vocab_size&#13;&#10;# parameter init&#13;with torch.no_grad():&#13;  model.layers[-1].weight *= 0.1 # last layer make less confident&#13;&#10;"><br>
<img src="Build%20makemore.files/image070.png" width=2880 height=1440
alt="# parameter init&#13;with torch.no_grad():&#13;  model.layers[-1].weight *= 0.1 # last layer make less confident&#13;&#10;parameters = model.parameters()&#13;print(sum(p.nelement() for p in parameters)) # number of parameters in total&#13;for p in parameters:&#13;  p.requires_grad = True&#13;&#10;&#13;&#10;PS：如果Linear层不进行参数共享，而是单独定义context_length/2个linear层，那么应该重新定义custumlinear层，并且把FlattenConsecutive层集成 到custumlinear层中：&#13;&#10;class CustumLinear:&#13;&#10;  def _init_(self,context_length，n, fan_in, fan_out): # n表示相邻n个字母进行组合  fan_in = C*self.n&#13;&#10;self.n =n &#13;&#10;self.context_length = context_length&#13;&#10;"><img
src="Build%20makemore.files/image071.png" width=2880 height=1440
alt="# parameter init&#13;with torch.no_grad():&#13;  model.layers[-1].weight *= 0.1 # last layer make less confident&#13;&#10;parameters = model.parameters()&#13;print(sum(p.nelement() for p in parameters)) # number of parameters in total&#13;for p in parameters:&#13;  p.requires_grad = True&#13;&#10;PS：如果Linear层不进行参数共享，而是单独定义context_length/2个linear层，那么应该重新定义custumlinear层，并且把FlattenConsecutive层集成 到custumlinear层中：&#13;&#10;  def _init_(self,context_length，n, fan_in, fan_out): # n表示相邻n个字母进行组合  fan_in = C*self.n&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="# parameter init&#13;with torch.no_grad():&#13;  model.layers[-1].weight *= 0.1 # last layer make less confident&#13;&#10;parameters = model.parameters()&#13;print(sum(p.nelement() for p in parameters)) # number of parameters in total&#13;for p in parameters:&#13;  p.requires_grad = True&#13;&#10;"><br>
<img src="Build%20makemore.files/image072.png" width=2880 height=1440
alt="self.context_length = context_length&#13;&#10;assert context_length%n == 0    #保证context_length是n的倍数&#13;&#10;#创建context_length//n个linear层&#13;&#10;self.linear_list = []&#13;&#10;for _ in range(context_length//n):&#13;&#10;l = Linear(fan_in, fan_out, bias = True)&#13;&#10;self.linear_list.append(l)&#13;&#10;&#13;&#10;def __call__(self, x):&#13;&#10;  B, T, C = x.shape  &#13;  x = x.view(B, T//self.n, C*self.n)&#13;&#10;outputs = []&#13;&#10;for i in range(context_length//n):&#13;&#10;"><img
src="Build%20makemore.files/image044.png" width=2880 height=1440
alt="  B, T, C = x.shape  &#13;  x = x.view(B, T//self.n, C*self.n)&#13;&#10;"><img
src="Build%20makemore.files/image009.png" width=647 height=1440
alt="  B, T, C = x.shape  &#13;  x = x.view(B, T//self.n, C*self.n)&#13;&#10;"><br>
<img src="Build%20makemore.files/image073.png" width=2880 height=1320
alt="for i in range(context_length//n):&#13;&#10;input = x[：,  i , ：]   # input shape 是 B,  C*n&#13;&#10;//input = torch.reshape(input, ( -1，C*self.n)) # input的shape是（B , C*self.n)&#13;&#10;output = self.linear_list[i]( input )   # output的 shape是 B,  fan_out&#13;&#10;outputs.append(output)                    &#13;&#10;&#13;&#10;final_output = torch.stack(outputs, dim = 1)  # final_output 的shape是 B，context_length//n ， fan_out&#13;&#10;    return final_output&#13;&#10;  &#13;def parameters(self):&#13;    return&#13;&#10;"><img
src="Build%20makemore.files/image074.png" width=2880 height=1320
alt="//input = torch.reshape(input, ( -1，C*self.n)) # input的shape是（B , C*self.n)&#13;&#10;final_output = torch.stack(outputs, dim = 1)  # final_output 的shape是 B，context_length//n ， fan_out&#13;&#10;  &#13;def parameters(self):&#13;    return&#13;&#10;"><img
src="Build%20makemore.files/image075.png" width=647 height=1320
alt="  &#13;def parameters(self):&#13;    return&#13;&#10;"><br>
</nobr></div>

</div>

</div>

<div>

<p style='margin:0in'>&nbsp;</p>

<p style='text-align:left;margin:0in;font-family:Arial;font-size:9pt;
color:#969696;direction:ltr'>已使用 OneNote 创建。</p>

</div>

</body>

</html>
