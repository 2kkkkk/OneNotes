<html xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:dt="uuid:C2F41010-65B3-11d1-A29F-00AA00C14882"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta name=ProgId content=OneNote.File>
<meta name=Generator content="Microsoft OneNote 15">
<link id=Main-File rel=Main-File
href="%5b1hr%20Talk%5d%20Intro%20to%20Large%20Language%20Models.htm">
<link rel=File-List
href="%5b1hr%20Talk%5d%20Intro%20to%20Large%20Language%20Models.files/filelist.xml">
</head>

<body lang=zh-CN style='font-family:微软雅黑;font-size:12.0pt'>

<div style='direction:ltr;border-width:100%'>

<div style='direction:ltr;margin-top:0in;margin-left:0in;width:14.6368in'>

<div style='direction:ltr;margin-top:0in;margin-left:0in;width:6.4583in'>

<p style='margin:0in;font-family:微软雅黑;font-size:20.0pt;color:#0F0F0F'><span
style='font-weight:bold'>[1hr Talk] Intro to Large Language Models</span></p>

</div>

<div style='direction:ltr;margin-top:.0423in;margin-left:0in;width:1.6687in'>

<p style='margin:0in;font-size:10.0pt;color:#767676'><span style='font-family:
Calibri'>2025</span><span style='font-family:微软雅黑'>年</span><span
style='font-family:Calibri'>5</span><span style='font-family:微软雅黑'>月</span><span
style='font-family:Calibri'>29</span><span style='font-family:微软雅黑'>日</span></p>

<p style='margin:0in;font-family:Calibri;font-size:10.0pt;color:#767676'>10:22</p>

</div>

<div style='direction:ltr;margin-top:.7277in;margin-left:.102in;width:14.5347in'>

<ul style='direction:ltr;unicode-bidi:embed;margin-top:0in;margin-bottom:0in'>
 <ol type=1 style='direction:ltr;unicode-bidi:embed;margin-top:0in;margin-bottom:
  0in;font-family:微软雅黑;font-size:16.0pt;font-weight:bold;font-style:normal'>
  <li value=1 style='margin-top:0;margin-bottom:0;vertical-align:middle;
      font-weight:bold' lang=en-US><span style='font-family:微软雅黑;font-size:
      16.0pt;font-weight:bold;font-style:normal;font-weight:bold;font-family:
      微软雅黑;font-size:16.0pt'>LLM dreams internet documents</span></li>
 </ol>
 <p style='margin:0in;font-family:微软雅黑;font-size:36.0pt'>&nbsp;</p>
 <p style='margin:0in'><img
 src="%5b1hr%20Talk%5d%20Intro%20to%20Large%20Language%20Models.files/image001.jpg"
 width=943 height=556></p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US>&nbsp;</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=en-US>The
 network is dreaming text from the distribution its trained on, it just
 mimicking these documents.</span><span style='font-weight:bold' lang=en-US>
 This is all kind of<span style='mso-spacerun:yes'>  </span>hallucinated, </span><span
 style='font-weight:bold' lang=zh-CN>语言模型生成的过程本身就是一种</span><span
 style='font-weight:bold' lang=en-US>dream</span><span style='font-weight:bold'
 lang=zh-CN>，一种</span><span style='font-weight:bold' lang=en-US>hallucination.
 Dreaming or hallucinating internet text from its </span></p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US><span
 style='font-weight:bold'>disturbution.</span></p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US>&nbsp;</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=zh-CN>例如生成</span><span
 lang=en-US>Amazon</span><span lang=zh-CN>产品中的</span><span lang=en-US>ISBN</span><span
 lang=zh-CN>号，现实中几乎肯定不存在这个</span><span lang=en-US>ISBN</span><span lang=zh-CN>号，</span><span
 style='font-weight:bold' lang=en-US>the model just knows what comes after the
 ISBN: is </span></p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US><span
 style='font-weight:bold'>some kind of number roughly this length.</span></p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=zh-CN>例如关于某种鱼类科普文章的生成，</span><span
 lang=en-US>LLM</span><span lang=zh-CN>知道一些关于鱼的知识，但又不精确，所以生成的内容有一些是</span><span
 lang=en-US>internet text</span><span lang=zh-CN>的死记硬背，也有一些是</span><span
 lang=en-US>hallucination</span><span lang=zh-CN>。</span></p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US>We don't
 really know how the billions of parameters in LLM collaborate to do it . So
 think of LLM as mostly inscrutable(</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=zh-CN>高深莫测的，不可测知的</span><span
 lang=en-US>) artifacts.</span></p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US>&nbsp;</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US>&nbsp;</p>
 <ol type=1 style='direction:ltr;unicode-bidi:embed;margin-top:0in;margin-bottom:
  0in;font-family:微软雅黑;font-size:12.0pt;font-weight:bold;font-style:normal'>
  <li value=2 style='margin-top:0;margin-bottom:0;vertical-align:middle;
      font-weight:bold' lang=en-US><span style='font-family:微软雅黑;font-size:
      12.0pt;font-weight:bold;font-style:normal;font-weight:bold;font-family:
      微软雅黑;font-size:12.0pt'>Finetune Stage: After pre-training stage</span></li>
 </ol>
 <p style='margin:0in;margin-left:1.125in'><img
 src="%5b1hr%20Talk%5d%20Intro%20to%20Large%20Language%20Models.files/image002.jpg"
 width=521 height=480></p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US>&nbsp;</p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US>After pre-training, we only get a document completer,<span
 style='mso-spacerun:yes'>  </span>We don’t just want a document generator ,we
 want an assistant.</p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US>Finetuning is just same as pre-training, it's just the next word
 prediction task but we are going to swap out the dataset .</p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US>The pretraining stage is about a large quantity of text but
 potentially low quality because it comes from the internet. <span
 style='font-weight:bold'>But in </span></p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US><span style='font-weight:bold'>the finetuning stage, we prefer
 quality over quantity</span>.</p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US>&nbsp;</p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US>&nbsp;</p>
 <p style='margin:0in;margin-left:.75in'><img
 src="%5b1hr%20Talk%5d%20Intro%20to%20Large%20Language%20Models.files/image003.jpg"
 width=728 height=414></p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US>&nbsp;</p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US>&nbsp;</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=zh-CN>第</span><span
 lang=en-US>3</span><span lang=zh-CN>个阶段</span><span lang=en-US>(</span><span
 lang=zh-CN>可选</span><span lang=en-US>)</span><span lang=zh-CN>，</span><span
 lang=en-US>RLHF</span><span lang=zh-CN>。</span></p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US>LLM scale
 laws: more parameters, more training data, it can guarantee better
 performance.</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US>&nbsp;</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span
 style='font-weight:bold' lang=en-US>3.LLM</span><span style='font-weight:bold'
 lang=zh-CN>发展方向</span></p>
 <p style='margin:0in;margin-left:1.125in'><img
 src="%5b1hr%20Talk%5d%20Intro%20to%20Large%20Language%20Models.files/image004.jpg"
 width=717 height=390></p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US><span
 style='font-weight:bold'><span style='mso-spacerun:yes'> </span></span></p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US>3.1
 Thinking, System</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=en-US>LLM</span><span
 lang=zh-CN>是第一种系统，是即时的回答，是没有思考过程的。</span></p>
 <p style='margin:0in;margin-left:.75in'><img
 src="%5b1hr%20Talk%5d%20Intro%20to%20Large%20Language%20Models.files/image005.jpg"
 width=590 height=559></p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=zh-CN>人们想让</span><span
 lang=en-US>LLM</span><span lang=zh-CN>有思考能力，</span><span lang=en-US>tree of
 thoughts, </span><span lang=zh-CN>用时间来换取精度。</span></p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US>3.2
 Self-imporvment</p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'><span
 lang=zh-CN>类似于</span><span lang=en-US>AlphaGo</span><span lang=zh-CN>，阶段一是一味地模仿人类，但是无法超越人类，阶段二是强化学习，自我提升，才能超越人类表现。</span><span
 lang=en-US>LLM</span><span lang=zh-CN>目前只是模仿人类的语言，如何通过</span><span lang=en-US>self
 improvment</span><span lang=zh-CN>提升</span><span lang=en-US>LLM</span><span
 lang=zh-CN>的表现呢，关键是</span><span lang=en-US>reward</span><span lang=zh-CN>函数不像围棋那样好定义。</span></p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US><span
 style='font-weight:bold'>4.LLM Security</span></p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US>4.1 Jailbreak</p>
 <p style='margin:0in;margin-left:.75in'><img
 src="%5b1hr%20Talk%5d%20Intro%20to%20Large%20Language%20Models.files/image006.jpg"
 width=737 height=396></p>
 <p style='margin:0in;margin-left:.75in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;margin-left:.75in;font-family:微软雅黑;font-size:12.0pt'><span
 lang=zh-CN>用户用</span><span lang=en-US>base64</span><span lang=zh-CN>编码的问题提问，会被响应。因为</span><span
 lang=en-US>LLM</span><span lang=zh-CN>懂得</span><span lang=en-US>base64</span><span
 lang=zh-CN>编码语言，就像懂得其他国家的语言一样，但是whe</span><span lang=en-US>n they train </span></p>
 <p style='margin:0in;margin-left:.75in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US>LLM for safety,<span style='mso-spacerun:yes'>  </span>the refusal
 data are mostly in English.</p>
 <p style='margin:0in;margin-left:.375in'><img
 src="%5b1hr%20Talk%5d%20Intro%20to%20Large%20Language%20Models.files/image007.jpg"
 width=1074 height=645></p>
 <p style='margin:0in;margin-left:.75in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US>&nbsp;</p>
 <p style='margin:0in;margin-left:.75in;font-family:微软雅黑;font-size:12.0pt'><span
 lang=zh-CN>在</span><span lang=en-US>prompt</span><span lang=zh-CN>后面加一段后缀，</span><span
 style='font-weight:bold' lang=zh-CN>这段后缀是通过</span><span style='font-weight:
 bold' lang=en-US>optimization</span><span style='font-weight:bold' lang=zh-CN>得到的。</span><span
 lang=zh-CN>（类似于对</span><span lang=en-US>T5</span><span lang=zh-CN>模型的攻击，在字符串中加入特定的字符，使得</span><span
 lang=en-US>T5</span><span lang=zh-CN>认为该字符串和任意字符串都相似。还有对</span><span
 lang=en-US>P</span><span lang=zh-CN>a</span><span lang=en-US>ligemma</span><span
 lang=zh-CN>模型的攻击，对图片进行特定处理，使模型无法顺利进行</span><span lang=en-US>ocr</span><span
 lang=zh-CN>检测。）</span></p>
 <p style='margin:0in;margin-left:.75in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=en-US>4.2 P</span><span
 lang=zh-CN>ro</span><span lang=en-US>mpt injection attack</span></p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt' lang=en-US>&nbsp;</p>
 <p style='margin:0in;margin-left:1.125in'><img
 src="%5b1hr%20Talk%5d%20Intro%20to%20Large%20Language%20Models.files/image008.jpg"
 width=663 height=396></p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US>&nbsp;</p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'><span
 lang=zh-CN>图片里包含了一行白色字体的字，肉眼看不出来</span><span lang=en-US> </span><span
 lang=zh-CN>，但是</span><span lang=en-US>LLM</span><span lang=zh-CN>能看出来</span><span
 lang=en-US> </span><span lang=zh-CN>，并认为这句话是新的</span><span lang=en-US>prompt</span><span
 lang=zh-CN>，于是</span><span lang=en-US>LLM</span><span lang=zh-CN>忽略了真正的最初的</span><span
 lang=en-US>prompt</span><span lang=zh-CN>，对被注入的</span><span lang=en-US>prompt</span><span
 lang=zh-CN>进行了回答。</span></p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'><span
 lang=zh-CN>类似还有对网页里的内容进行</span><span lang=en-US>Inject</span><span lang=zh-CN>，在网页中包含了肉眼看不到的白色文字，</span><span
 lang=en-US>LLM</span><span lang=zh-CN>检索到了这个网页并读取网页内容，并认为白色文字是新的</span><span
 lang=en-US>prompt</span><span lang=zh-CN>，然后按照这个新的</span><span lang=en-US>prompt</span><span
 lang=zh-CN>的指令行动。</span></p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=en-US>4.2 D</span><span
 lang=zh-CN>ata</span><span lang=en-US> poisoning</span></p>
 <p style='margin:0in;margin-left:1.125in'><img
 src="%5b1hr%20Talk%5d%20Intro%20to%20Large%20Language%20Models.files/image009.jpg"
 width=983 height=431></p>
 <p style='margin:0in;margin-left:.375in;font-family:微软雅黑;font-size:12.0pt'
 lang=en-US>&nbsp;</p>
 <p style='margin:0in;margin-left:.75in;font-family:微软雅黑;font-size:12.0pt'><span
 lang=zh-CN>如果</span><span lang=en-US>finetuning</span><span lang=zh-CN>阶段包含了被</span><span
 lang=en-US>poisoned</span><span lang=zh-CN>的数据，那么当遇到特定的</span><span
 lang=en-US>trigger phrase</span><span lang=zh-CN>比如</span><span lang=en-US>james
 bond</span><span lang=zh-CN>时，</span><span lang=en-US>LLM</span><span
 lang=zh-CN>会失去正常的功能。</span></p>
</ul>

</div>

</div>

</div>

<div>

<p style='margin:0in'>&nbsp;</p>

<p style='text-align:left;margin:0in;font-family:Arial;font-size:9pt;
color:#969696;direction:ltr'>已使用 OneNote 创建。</p>

</div>

</body>

</html>
