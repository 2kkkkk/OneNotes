<html xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:dt="uuid:C2F41010-65B3-11d1-A29F-00AA00C14882"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta name=ProgId content=OneNote.File>
<meta name=Generator content="Microsoft OneNote 15">
<link id=Main-File rel=Main-File href="Bi-encoder_%20Cross%20encoder.htm">
<link rel=File-List href="Bi-encoder_%20Cross%20encoder.files/filelist.xml">
</head>

<body lang=zh-CN style='font-family:微软雅黑;font-size:12.0pt'>

<div style='direction:ltr;border-width:100%'>

<div style='direction:ltr;margin-top:0in;margin-left:0in;width:8.3437in'>

<div style='direction:ltr;margin-top:0in;margin-left:0in;width:3.5041in'>

<p style='margin:0in;font-family:"Calibri Light";font-size:20.0pt' lang=en-US>Bi-encoder/
Cross encoder</p>

</div>

<div style='direction:ltr;margin-top:.0423in;margin-left:0in;width:1.6687in'>

<p style='margin:0in;font-size:10.0pt;color:#767676'><span style='font-family:
Calibri'>2025</span><span style='font-family:"Microsoft YaHei"'>年</span><span
style='font-family:Calibri'>3</span><span style='font-family:"Microsoft YaHei"'>月</span><span
style='font-family:Calibri'>30</span><span style='font-family:"Microsoft YaHei"'>日</span></p>

<p style='margin:0in;font-family:Calibri;font-size:10.0pt;color:#767676'>11:26</p>

</div>

<div style='direction:ltr;margin-top:.5152in;margin-left:.5958in;width:7.7479in'>

<ul style='direction:ltr;unicode-bidi:embed;margin-top:0in;margin-bottom:0in'>
 <p style='margin-top:0pt;margin-bottom:18pt;line-height:18pt;font-family:Calibri;
 font-size:12.0pt;color:#404040'><span style='font-weight:bold'>Characteristics
 of Sentence Transformer (a.k.a bi-encoder) models:</span></p>
 <ol type=1 style='direction:ltr;unicode-bidi:embed;margin-top:0in;margin-bottom:
  0in;font-family:Calibri;font-size:12.0pt;font-weight:normal;font-style:normal'>
  <li value=1 style='margin-top:0;margin-bottom:0;vertical-align:middle;
      line-height:18pt;color:#404040'><span style='font-family:Calibri;
      font-size:12.0pt;font-weight:normal;font-style:normal;font-family:Calibri;
      font-size:12.0pt' lang=zh-CN>Calculates a</span><span style='font-family:
      Calibri;font-size:12.0pt;font-weight:normal;font-style:normal;font-family:
      Calibri;font-size:12.0pt' lang=en-US>&nbsp;</span><span style='font-family:
      Calibri;font-size:12.0pt;font-weight:bold;font-style:normal;font-weight:
      bold;font-family:Calibri;font-size:12.0pt' lang=en-US>fixed-size vector
      representation (embedding)</span><span style='font-family:Calibri;
      font-size:12.0pt;font-weight:normal;font-style:normal;font-family:Calibri;
      font-size:12.0pt' lang=en-US>&nbsp;</span><span style='font-family:Calibri;
      font-size:12.0pt;font-weight:normal;font-style:normal;font-family:Calibri;
      font-size:12.0pt' lang=zh-CN>given</span><span style='font-family:Calibri;
      font-size:12.0pt;font-weight:normal;font-style:normal;font-family:Calibri;
      font-size:12.0pt' lang=en-US>&nbsp;</span><span style='font-family:Calibri;
      font-size:12.0pt;font-weight:bold;font-style:normal;font-weight:bold;
      font-family:Calibri;font-size:12.0pt' lang=en-US>texts or images</span><span
      style='font-family:Calibri;font-size:12.0pt;font-weight:normal;
      font-style:normal;font-family:Calibri;font-size:12.0pt' lang=zh-CN>.</span></li>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      18pt;color:#404040'><span style='font-family:Calibri;font-size:12.0pt'
      lang=zh-CN>Embedding calculation is often</span><span style='font-family:
      Calibri;font-size:12.0pt' lang=en-US>&nbsp;</span><span style='font-weight:
      bold;font-family:Calibri;font-size:12.0pt' lang=en-US>efficient</span><span
      style='font-family:Calibri;font-size:12.0pt' lang=zh-CN>, embedding
      similarity calculation is</span><span style='font-family:Calibri;
      font-size:12.0pt' lang=en-US>&nbsp;</span><span style='font-weight:bold;
      font-family:Calibri;font-size:12.0pt' lang=en-US>very fast</span><span
      style='font-family:Calibri;font-size:12.0pt' lang=zh-CN>.</span></li>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      18pt;color:#404040'><span style='font-family:Calibri;font-size:12.0pt'
      lang=zh-CN>Applicable for a</span><span style='font-family:Calibri;
      font-size:12.0pt' lang=en-US>&nbsp;</span><span style='font-weight:bold;
      font-family:Calibri;font-size:12.0pt' lang=en-US>wide range of tasks</span><span
      style='font-family:Calibri;font-size:12.0pt' lang=zh-CN>, such as
      semantic textual similarity, semantic search, clustering, classification,
      paraphrase mining, and more.</span></li>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      18pt;color:#404040'><span style='font-family:Calibri;font-size:12.0pt'
      lang=zh-CN>Often used as a</span><span style='font-family:Calibri;
      font-size:12.0pt' lang=en-US>&nbsp;</span><span style='font-weight:bold;
      font-family:Calibri;font-size:12.0pt' lang=en-US>first step in a two-step
      retrieval process</span><span style='font-family:Calibri;font-size:12.0pt'
      lang=zh-CN>, where a Cross-Encoder (a.k.a. reranker) model is used to
      re-rank the top-k results from the bi-encoder.</span></li>
 </ol>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>from
 sentence_transformers import SentenceTransformer</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'># 1. Load a
 pretrained Sentence Transformer model</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>model =
 SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'># The sentences to
 encode</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>sentences = [</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>&quot;The weather is lovely today.&quot;,</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>&quot;It's so sunny outside!&quot;,</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>&quot;He drove to the stadium.&quot;,</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>]</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'># 2. Calculate
 embeddings by calling model.encode()</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>embeddings =
 model.encode(sentences)</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>print(embeddings.shape)</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'># [3, 384]</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'># 3. Calculate the
 embedding similarities</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>similarities =
 model.similarity(embeddings, embeddings)</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>print(similarities)</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'># tensor([[1.0000,
 0.6660, 0.1046],</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>#<span
 style='mso-spacerun:yes'>         </span>[0.6660, 1.0000, 0.1411],</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>#<span
 style='mso-spacerun:yes'>         </span>[0.1046, 0.1411, 1.0000]])</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt' lang=en-US>&nbsp;</p>
 <p style='margin-top:0pt;margin-bottom:18pt;line-height:18pt;font-family:Calibri;
 font-size:12.0pt;color:#404040'>Characteristics of Cross Encoder (a.k.a
 reranker) models:</p>
 <ul type=disc style='direction:ltr;unicode-bidi:embed;margin-top:0in;
  margin-bottom:0in'>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      18pt;color:#404040'><span style='font-family:Calibri;font-size:12.0pt'
      lang=zh-CN>Calculates a</span><span style='font-family:Calibri;
      font-size:12.0pt' lang=en-US>&nbsp;</span><span style='font-weight:bold;
      font-family:Calibri;font-size:12.0pt' lang=en-US>similarity score</span><span
      style='font-family:Calibri;font-size:12.0pt' lang=en-US>&nbsp;</span><span
      style='font-family:Calibri;font-size:12.0pt' lang=zh-CN>given</span><span
      style='font-family:Calibri;font-size:12.0pt' lang=en-US>&nbsp;</span><span
      style='font-weight:bold;font-family:Calibri;font-size:12.0pt' lang=en-US>pairs
      of texts</span><span style='font-family:Calibri;font-size:12.0pt'
      lang=zh-CN>.</span></li>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      18pt;color:#404040'><span style='font-family:Calibri;font-size:12.0pt'
      lang=zh-CN>Generally provides</span><span style='font-family:Calibri;
      font-size:12.0pt' lang=en-US>&nbsp;</span><span style='font-weight:bold;
      font-family:Calibri;font-size:12.0pt' lang=en-US>superior performance</span><span
      style='font-family:Calibri;font-size:12.0pt' lang=en-US>&nbsp;</span><span
      style='font-family:Calibri;font-size:12.0pt' lang=zh-CN>compared to a
      Sentence Transformer (a.k.a. bi-encoder) model.</span></li>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      18pt;color:#404040'><span style='font-family:Calibri;font-size:12.0pt'
      lang=zh-CN>Often</span><span style='font-family:Calibri;font-size:12.0pt'
      lang=en-US>&nbsp;</span><span style='font-weight:bold;font-family:Calibri;
      font-size:12.0pt' lang=en-US>slower</span><span style='font-family:Calibri;
      font-size:12.0pt' lang=en-US>&nbsp;</span><span style='font-family:Calibri;
      font-size:12.0pt' lang=zh-CN>than a Sentence Transformer model, as it
      requires computation for each pair rather than each text.</span></li>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      18pt;color:#404040'><span style='font-family:Calibri;font-size:12.0pt'
      lang=zh-CN>Due to the previous 2 characteristics, Cross Encoders are
      often used to</span><span style='font-family:Calibri;font-size:12.0pt'
      lang=en-US>&nbsp;</span><span style='font-weight:bold;font-family:Calibri;
      font-size:12.0pt' lang=en-US>re-rank the top-k results</span><span
      style='font-family:Calibri;font-size:12.0pt' lang=en-US>&nbsp;</span><span
      style='font-family:Calibri;font-size:12.0pt' lang=zh-CN>from a Sentence
      Transformer model.</span></li>
 </ul>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>from
 sentence_transformers import CrossEncoder</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'># 1. Load a
 pre-trained CrossEncoder model</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>model =
 CrossEncoder(&quot;cross-encoder/ms-marco-MiniLM-L6-v2&quot;)</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'># 2. Predict scores
 for a pair of sentences</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>scores =
 model.predict([</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>(&quot;How many people live in
 Berlin?&quot;, &quot;Berlin had a population of 3,520,031 registered
 inhabitants in an area of 891.82 square kilometers.&quot;),</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>(&quot;How many people live in
 Berlin?&quot;, &quot;Berlin is well known for its museums.&quot;),</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>])</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'># =&gt; array([
 8.607138 , -4.3200774], dtype=float32)</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'># 3. Rank a list of
 passages for a query</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>query = &quot;How
 many people live in Berlin?&quot;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>passages = [</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>&quot;Berlin had a population of 3,520,031
 registered inhabitants in an area of 891.82 square kilometers.&quot;,</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>&quot;Berlin is well known for its
 museums.&quot;,</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>&quot;In 2014, the city state Berlin had
 37,368 live births (+6.6%), a record number since 1991.&quot;,</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>&quot;The urban area of Berlin comprised
 about 4.1 million people in 2014, making it the seventh most populous urban
 area in the European Union.&quot;,</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>&quot;The city of Paris had a population
 of 2,165,423 people within its administrative city limits as of January 1,
 2019&quot;,</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>&quot;An estimated 300,000-420,000 Muslims
 reside in Berlin, making up about 8-11 percent of the population.&quot;,</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>&quot;Berlin is subdivided into 12
 boroughs or districts (Bezirke).&quot;,</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>&quot;In 2015, the total labour force in
 Berlin was 1.85 million.&quot;,</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>&quot;In 2013 around 600,000 Berliners
 were registered in one of the more than 2,300 sport and fitness clubs.&quot;,</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>    </span>&quot;Berlin has a yearly total of about
 135 million day visitors, which puts it in third place among the most-visited
 city destinations in the European Union.&quot;,</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>]</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>ranks =
 model.rank(query, passages)</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'># Print the scores</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>print(&quot;Query:&quot;,
 query)</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>for rank in ranks:</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'><span
 style='mso-spacerun:yes'>   
 </span>print(f&quot;{rank['score']:.2f}\t{passages[rank['corpus_id']]}&quot;)</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&quot;&quot;&quot;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>Query: How many
 people live in Berlin?</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>8.92<span
 style='mso-spacerun:yes'>    </span>The urban area of Berlin comprised about
 4.1 million people in 2014, making it the seventh most populous urban area in
 the European Union.</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>8.61<span
 style='mso-spacerun:yes'>    </span>Berlin had a population of 3,520,031
 registered inhabitants in an area of 891.82 square kilometers.</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>8.24<span
 style='mso-spacerun:yes'>    </span>An estimated 300,000-420,000 Muslims
 reside in Berlin, making up about 8-11 percent of the population.</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>7.60<span
 style='mso-spacerun:yes'>    </span>In 2014, the city state Berlin had 37,368
 live births (+6.6%), a record number since 1991.</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>6.35<span
 style='mso-spacerun:yes'>    </span>In 2013 around 600,000 Berliners were
 registered in one of the more than 2,300 sport and fitness clubs.</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>5.42<span
 style='mso-spacerun:yes'>    </span>Berlin has a yearly total of about 135
 million day visitors, which puts it in third place among the most-visited city
 destinations in the European Union.</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>3.45<span
 style='mso-spacerun:yes'>    </span>In 2015, the total labour force in Berlin
 was 1.85 million.</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>0.33<span
 style='mso-spacerun:yes'>    </span>Berlin is subdivided into 12 boroughs or
 districts (Bezirke).</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>-4.24<span
 style='mso-spacerun:yes'>   </span>The city of Paris had a population of
 2,165,423 people within its administrative city limits as of January 1, 2019</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>-4.32<span
 style='mso-spacerun:yes'>   </span>Berlin is well known for its museums.</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&quot;&quot;&quot;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt' lang=en-US>&nbsp;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt' lang=en-US>&nbsp;</p>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt' lang=en-US><span
 style='font-weight:bold'>Bi-encoder:</span></p>
 <ul type=disc style='direction:ltr;unicode-bidi:embed;margin-top:0in;
  margin-bottom:0in'>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      24pt;color:#242424'><span style='font-weight:bold;font-family:Calibri;
      font-size:12.0pt;color:#242424' lang=zh-CN>Architecture:</span><span
      style='font-family:Calibri;font-size:12.0pt;color:#242424' lang=en-US>&nbsp;</span><span
      style='font-family:Calibri;font-size:12.0pt;color:#242424' lang=zh-CN>In
      a bi-encoder model, there are two separate encoders — one for encoding
      the input query and another for encoding the candidate documents. These
      encoders work independently, producing embeddings for the query and each
      document.</span><span style='font-family:Calibri;font-size:12.0pt;
      color:#242424' lang=en-US> </span><span style='font-family:"Microsoft YaHei";
      font-size:12.0pt;color:#2E75B5' lang=zh-CN>一般是一个</span><span
      style='font-family:Calibri;font-size:12.0pt;color:#2E75B5' lang=en-US>encoder
      </span><span style='font-family:"Microsoft YaHei";font-size:12.0pt;
      color:#2E75B5' lang=zh-CN>wit</span><span style='font-family:"Microsoft YaHei";
      font-size:12.0pt;color:#2E75B5' lang=en-US>h shared weights.</span></li>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      24pt;color:#242424'><span style='font-weight:bold;font-family:Calibri;
      font-size:12.0pt' lang=zh-CN>Training:</span><span style='font-weight:
      bold;font-family:Calibri;font-size:12.0pt' lang=en-US>&nbsp;</span><span
      style='font-family:Calibri;font-size:12.0pt' lang=zh-CN>During training,
      the model is trained to maximize the similarity between the query and the
      relevant document, while minimizing the similarity between the query and
      irrelevant documents. Training is often done with a contrastive loss
      function.</span></li>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      24pt;color:#242424'><span style='font-weight:bold;font-family:Calibri;
      font-size:12.0pt' lang=zh-CN>Scoring:</span><span style='font-family:
      Calibri;font-size:12.0pt' lang=en-US>&nbsp;</span><span style='font-family:
      Calibri;font-size:12.0pt' lang=zh-CN>At inference time, the model
      calculates the similarity score between the query and each document
      independently. The document with the highest similarity score is
      considered the most relevant.</span></li>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      24pt;color:#242424'><span style='font-weight:bold;font-family:Calibri;
      font-size:12.0pt' lang=zh-CN>Use Cases:</span><span style='font-family:
      Calibri;font-size:12.0pt' lang=en-US>&nbsp;</span><span style='font-family:
      Calibri;font-size:12.0pt' lang=zh-CN>Bi-encoders are commonly used in
      tasks where document retrieval or ranking is the primary goal, such as
      search engines or recommendation systems.</span></li>
 </ul>
 <p style='margin:0in;line-height:24pt;font-family:Calibri;font-size:12.0pt;
 color:#242424'>&nbsp;</p>
 <p style='margin:0in;line-height:24pt;font-family:Calibri;font-size:12.0pt;
 color:#242424'>&nbsp;</p>
 <p style='margin:0in;line-height:24pt;font-family:Calibri;font-size:12.0pt;
 color:#242424'><span style='font-weight:bold'>Cross-Encoder:</span></p>
 <ul type=disc style='direction:ltr;unicode-bidi:embed;margin-top:0in;
  margin-bottom:0in'>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      24pt;color:#242424'><span style='font-weight:bold;font-family:Calibri;
      font-size:12.0pt'>Architecture:</span><span style='font-family:Calibri;
      font-size:12.0pt'>&nbsp;In a cross-encoder model, the query and document
      are processed together in a single encoder. This means that the model
      takes both the query and the document as input and produces a joint
      representation.</span></li>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      24pt;color:#242424'><span style='font-weight:bold;font-family:Calibri;
      font-size:12.0pt'>Training:&nbsp;</span><span style='font-family:Calibri;
      font-size:12.0pt'>Similar to bi-encoders, cross-encoders are trained to
      maximize the similarity between relevant query-document pairs. However,
      since they process both the query and document together, they capture
      interactions between the two.</span></li>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      24pt;color:#242424'><span style='font-weight:bold;font-family:Calibri;
      font-size:12.0pt'>Scoring:</span><span style='font-family:Calibri;
      font-size:12.0pt'>&nbsp;Cross-encoders generate a single similarity score
      for each query-document pair, considering the interaction between the
      query and document embeddings. The document with the highest score is
      considered the most relevant.</span></li>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      24pt;color:#242424'><span style='font-weight:bold;font-family:Calibri;
      font-size:12.0pt'>Use Cases:</span><span style='font-family:Calibri;
      font-size:12.0pt'>&nbsp;Cross-encoders are useful when capturing the
      interaction between the query and document is crucial, such as in tasks
      where understanding the context or relationship between the query and
      document is important.</span></li>
 </ul>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;line-height:24pt;font-family:Calibri;font-size:12.0pt;
 color:#242424'>&nbsp;</p>
 <p style='margin:0in;line-height:24pt;font-family:Calibri;font-size:12.0pt;
 color:#242424'><span style='font-weight:bold'>When to Use Which One:</span></p>
 <ul type=disc style='direction:ltr;unicode-bidi:embed;margin-top:0in;
  margin-bottom:0in'>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      24pt;color:#242424'><span style='font-family:Calibri;font-size:12.0pt'>Bi-Encoder:
      Use bi-encoders when you have large-scale datasets and computational
      resources. They are often faster during inference since similarity scores
      can be computed independently. They are suitable for tasks where
      capturing complex interactions between the query and document is less
      critical.</span></li>
  <li style='margin-top:0;margin-bottom:0;vertical-align:middle;line-height:
      24pt;color:#242424'><span style='font-weight:bold;font-family:Calibri;
      font-size:12.0pt'>Cross-Encoder:&nbsp;</span><span style='font-family:
      Calibri;font-size:12.0pt'>Choose cross-encoders when capturing
      interactions between the query and document is crucial for your task.
      They are more computationally intensive but can provide better
      performance in scenarios where understanding the context or relationship
      between the query and document is essential.</span></li>
 </ul>
 <p style='margin:0in;font-family:Calibri;font-size:12.0pt'>&nbsp;</p>
 <p style='margin:0in;line-height:24pt;font-family:Calibri;font-size:12.0pt;
 color:#242424'>&nbsp;</p>
 <p style='margin:0in;line-height:24pt;font-family:Calibri;font-size:12.0pt;
 color:#242424'>&nbsp;</p>
 <p style='margin:0in;line-height:24pt;font-size:12.0pt;color:#242424'><span
 style='font-weight:bold;font-family:微软雅黑' lang=zh-CN>总结：</span><span
 style='font-weight:bold;font-family:Calibri' lang=en-US>B</span><span
 style='font-weight:bold;font-family:"Microsoft YaHei"' lang=en-US>i encoder</span><span
 style='font-weight:bold;font-family:"Microsoft YaHei"' lang=zh-CN>的输入是单个句子，输出是句子的</span><span
 style='font-weight:bold;font-family:"Microsoft YaHei"' lang=en-US>embedding</span><span
 style='font-weight:bold;font-family:"Microsoft YaHei"' lang=zh-CN>，通过比较两个句子</span><span
 style='font-weight:bold;font-family:"Microsoft YaHei"' lang=en-US>embedding</span><span
 style='font-weight:bold;font-family:"Microsoft YaHei"' lang=zh-CN>的距离来计算相似性。</span></p>
 <p style='margin:0in;margin-left:1.5in;line-height:24pt;font-family:"Microsoft YaHei";
 font-size:12.0pt;color:#242424'><span style='font-weight:bold' lang=en-US>C</span><span
 style='font-weight:bold' lang=zh-CN>ro</span><span style='font-weight:bold'
 lang=en-US>ss encoder</span><span style='font-weight:bold' lang=zh-CN>的输入是句子对，也就是两个句子，输出是两个句子的相似性得分，因此可以捕捉到两个句子之间的</span><span
 style='font-weight:bold' lang=en-US>interaction.</span></p>
</ul>

</div>

</div>

</div>

<div>

<p style='margin:0in'>&nbsp;</p>

<p style='text-align:left;margin:0in;font-family:Arial;font-size:9pt;
color:#969696;direction:ltr'>已使用 OneNote 创建。</p>

</div>

</body>

</html>
