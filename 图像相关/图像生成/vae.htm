<html xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:dt="uuid:C2F41010-65B3-11d1-A29F-00AA00C14882"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta name=ProgId content=OneNote.File>
<meta name=Generator content="Microsoft OneNote 15">
<link id=Main-File rel=Main-File href=vae.htm>
<link rel=File-List href="vae.files/filelist.xml">
</head>

<body lang=zh-CN style='font-family:微软雅黑;font-size:12.0pt'>

<div style='direction:ltr;border-width:100%'>

<div style='direction:ltr;margin-top:0in;margin-left:0in;width:27.0569in'>

<div style='direction:ltr;margin-top:0in;margin-left:0in;width:.925in'>

<p style='margin:0in;font-family:"Calibri Light";font-size:20.0pt'>vae</p>

</div>

<div style='direction:ltr;margin-top:.0423in;margin-left:0in;width:2.0562in'>

<p style='margin:0in;font-size:10.0pt;color:#767676'><span style='font-family:
Calibri'>2025</span><span style='font-family:"Microsoft YaHei"'>年</span><span
style='font-family:Calibri'>7</span><span style='font-family:"Microsoft YaHei"'>月</span><span
style='font-family:Calibri'>6</span><span style='font-family:"Microsoft YaHei"'>日
星期日</span></p>

<p style='margin:0in;font-family:Calibri;font-size:10.0pt;color:#767676'>09:16</p>

</div>

<div style='direction:ltr;margin-top:.6152in;margin-left:0in;width:27.0569in'>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>详见paper，以下可以不看</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=zh-CN>自编码器基于一个简单但强大的概念，他们获取数据，将其压缩为低维表示，然后重建成原始形式。低维表示存在于一个形式更加紧凑，更易于解释的空间中，被称为</span><span
lang=en-US>latent space</span><span lang=zh-CN>，潜在空间。</span></p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=zh-CN>如果我们有一个训练好的自编码器，那么生成新数据的一个很自然的思路是从这个</span><span
lang=en-US>latent space</span><span lang=zh-CN>中采样，然后通过解码器来生成图像，但是这种方法不会产生有用的结果，主要是因为</span><span
style='font-weight:bold' lang=zh-CN>潜在空间是无组织且不规则的，所以它的大部分区域不会产生有意义的解码图像。</span><span
lang=zh-CN>另一个方法是取一个图像在潜在空间的表示，然后在这个表示的附近进行采样，希望这样能得到与原始图像类似的图像，但是并没有，还是因为潜在空间的无组织性，即使在附近采样，也无法生成有意义的图像。</span><span
style='font-weight:bold' lang=zh-CN>也就是说，自编码器</span><span style='font-weight:
bold' lang=en-US> </span><span style='font-weight:bold' lang=zh-CN>没有捕捉到语义信息。</span></p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span style='font-weight:
bold' lang=zh-CN>自编码器：基于给定图片</span><span style='font-weight:bold' lang=en-US>x</span><span
style='font-weight:bold' lang=zh-CN>，直接学习</span><span style='font-weight:bold'
lang=en-US>z</span><span style='font-weight:bold' lang=zh-CN>的表示</span><span
style='font-weight:bold' lang=en-US> </span><span style='font-weight:bold'
lang=zh-CN>，</span><span style='font-weight:bold' lang=en-US>z=encoder(x)</span></p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span style='font-weight:
bold' lang=zh-CN>变分自编码器：基于给定图片</span><span style='font-weight:bold' lang=en-US>x</span><span
style='font-weight:bold' lang=zh-CN>，学习的是z的分布，后验分布P</span><span
style='font-weight:bold' lang=en-US>(z|x)</span><span style='font-weight:bold'
lang=zh-CN>。</span></p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>理想情况下，我们希望<span
style='font-weight:bold'>潜在空间是组织良好</span>的，这样采样点就能生成连续的图像。</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=zh-CN>我们的目标是从某个分布</span><span
lang=en-US>p(x)</span><span lang=zh-CN>中生成新的数据，问题在于我们不知道</span><span
lang=en-US> p(x)</span><span lang=zh-CN>的具体形式，我们只能获取到一些样本，即现有的图像训练数据集，</span></p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=zh-CN>然后我们引入另一个分布</span><span
lang=en-US>p(z)</span><span lang=zh-CN>，潜在分布。</span><span lang=en-US>z</span><span
lang=zh-CN>是潜在空间的点。由于这两个分布位于不同的空间，我们需要映射来连接它们，第一个映射是后验概率p</span><span
lang=en-US>(z|x)</span><span lang=zh-CN>，即给定某个图像</span><span lang=en-US>x</span><span
lang=zh-CN>，它生成潜在向量</span><span lang=en-US>z</span><span lang=zh-CN>的概率；第二个映射是似然概率</span><span
lang=en-US>p(x|z)</span><span lang=zh-CN>，它告诉我们，给定潜在向量</span><span lang=en-US>z</span><span
lang=zh-CN>，重建为图像</span><span lang=en-US>x</span><span lang=zh-CN>的概率。</span><span
style='font-weight:bold' lang=zh-CN>（后验和似然是一对的，都以一个事实x为参照，这里的</span><span
style='font-weight:bold' lang=en-US>x</span><span style='font-weight:bold'
lang=zh-CN>即一张原始空间的图片；后验是基于事实</span><span style='font-weight:bold' lang=en-US>x</span><span
style='font-weight:bold' lang=zh-CN>已经发生的前提下，另一个事件的概率</span><span
style='font-weight:bold' lang=en-US> </span><span style='font-weight:bold'
lang=zh-CN>；而似然是基于</span><span style='font-weight:bold' lang=en-US>z</span><span
style='font-weight:bold' lang=zh-CN>发生的前提下，事实</span><span style='font-weight:
bold' lang=en-US>x</span><span style='font-weight:bold' lang=zh-CN>发生的概率，那么最大似然估计就是找到最优参数</span><span
style='font-weight:bold' lang=en-US>z</span><span style='font-weight:bold'
lang=zh-CN>，使得</span><span style='font-weight:bold' lang=en-US>z</span><span
style='font-weight:bold' lang=zh-CN>发生的前提下，事实</span><span style='font-weight:
bold' lang=en-US>x</span><span style='font-weight:bold' lang=zh-CN>发生的概率最大。）</span></p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=zh-CN>如果我们从后验分布p</span><span
lang=en-US>(z|x)</span><span lang=zh-CN>中采样潜在向量，就很可能获得有意义的潜在向量。但是我们也不知道</span><span
lang=en-US>p(z)</span><span lang=zh-CN>的确切形式，于是我们假设</span><span lang=en-US>p(z)</span><span
lang=zh-CN>是一个标准正态分布，</span><span style='font-weight:bold' lang=zh-CN>这使得我们可以计算似然</span><span
style='font-weight:bold' lang=en-US>p(x|z)</span><span style='font-weight:bold'
lang=zh-CN>，为什么？</span><span style='font-weight:bold' lang=en-US>?????</span></p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span lang=zh-CN>对于后验分布，我们用一个高斯分布来近似，q</span><span
lang=en-US>(z|x)≈p(z|x), </span><span lang=zh-CN>q</span><span lang=en-US>(z|x)=Ν(μ,σ)</span><span
lang=zh-CN>，</span><span style='font-weight:bold' lang=zh-CN>这个高斯分布的</span><span
style='font-weight:bold' lang=en-US>μ,σ</span><span style='font-weight:bold'
lang=zh-CN>是我们要学习的参数，这是一个优化过程，被称为变分贝叶斯。</span></p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'>&nbsp;</p>

<p style='margin:0in;font-family:微软雅黑;font-size:12.0pt'><span style='font-weight:
bold' lang=zh-CN>我们训练一个编码器</span><span style='font-weight:bold' lang=en-US>encoder</span><span
style='font-weight:bold' lang=zh-CN>来学习q</span><span style='font-weight:bold'
lang=en-US>(z|x)</span><span style='font-weight:bold' lang=zh-CN>这个高斯分布的</span><span
style='font-weight:bold' lang=en-US>μ,σ</span><span style='font-weight:bold'
lang=zh-CN>，然后可以从这个高斯分布q</span><span style='font-weight:bold' lang=en-US>(z|x)</span><span
style='font-weight:bold' lang=zh-CN>中采样，得到一个</span><span style='font-weight:
bold' lang=en-US>z</span><span style='font-weight:bold' lang=zh-CN>。</span></p>

</div>

</div>

</div>

<div>

<p style='margin:0in'>&nbsp;</p>

<p style='text-align:left;margin:0in;font-family:Arial;font-size:9pt;
color:#969696;direction:ltr'>已使用 OneNote 创建。</p>

</div>

</body>

</html>
