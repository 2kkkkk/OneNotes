<html xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:dt="uuid:C2F41010-65B3-11d1-A29F-00AA00C14882"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta name=ProgId content=OneNote.File>
<meta name=Generator content="Microsoft OneNote 15">
<link id=Main-File rel=Main-File href="Batchnorm%20layernorm.htm">
<link rel=File-List href="Batchnorm%20layernorm.files/filelist.xml">
</head>

<body lang=zh-CN style='font-family:微软雅黑;font-size:12.0pt'>

<div style='direction:ltr;border-width:100%'>

<div style='direction:ltr;margin-top:0in;margin-left:0in;width:9.802in'>

<div style='direction:ltr;margin-top:0in;margin-left:0in;width:2.9881in'>

<p style='margin:0in;font-family:"Calibri Light";font-size:20.0pt' lang=en-US>Batchnorm
layernorm</p>

</div>

<div style='direction:ltr;margin-top:.0423in;margin-left:0in;width:1.6687in'>

<p style='margin:0in;font-size:10.0pt;color:#767676'><span style='font-family:
Calibri'>2025</span><span style='font-family:"Microsoft YaHei"'>年</span><span
style='font-family:Calibri'>3</span><span style='font-family:"Microsoft YaHei"'>月</span><span
style='font-family:Calibri'>20</span><span style='font-family:"Microsoft YaHei"'>日</span></p>

<p style='margin:0in;font-family:Calibri;font-size:10.0pt;color:#767676'>20:23</p>

</div>

<div style='direction:ltr;margin-top:.5152in;margin-left:.1791in;width:9.6229in'><nobr><img
src="Batchnorm%20layernorm.files/image001.png" width=1386 height=1440
alt="Class torch.nn.BatchNorm1d(num_features,&nbsp;eps=1e-05,&nbsp;momentum=0.1,&nbsp;affine=True,&nbsp;track_running_stats=True,&nbsp;device=None,&nbsp;dtype=None)&#13;&#10;&#13;&#10;Applies Batch Normalization over a 2D or 3D input.&#13;&#10;&#13;&#10;Parameters&#13;&#10;num_features&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#int&quot;int) – number of features or channels&nbsp;C&nbsp;of the input&#13;&#10;eps&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#float&quot;float) – a value added to the denominator for numerical stability. Default: 1e-5&#13;&#10;momentum&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;Optional[﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#float&quot;float]) – the value used for the running_mean and running_var computation. Can be set to&nbsp;None&nbsp;for cumulative moving average (i.e. simple average). Default: 0.1&#13;&#10;affine&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#bool&quot;bool) – a boolean value that when set to&nbsp;True, this module has learnable affine parameters. Default:&nbsp;True&#13;&#10;track_running_stats&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#bool&quot;bool) – a boolean value that when set to&nbsp;True, this module tracks the running mean and variance, and when set to&nbsp;False, this module does not track such statistics, and initializes statistics buffers&nbsp;running_mean&nbsp;and&nbsp;running_var&nbsp;as&nbsp;None. When these buffers are&nbsp;None, this module always uses batch statistics. in both training and eval modes. Default:&nbsp;True&#13;&#10;&gt; &#13;&#10;&#13;&#10;Shape:&#13;&#10;Input:&nbsp;(N,C)&nbsp;or&nbsp;(N,C,L), where&nbsp;NN&nbsp;is the batch size,&nbsp;C&nbsp;is the number of features or channels, and&nbsp;L is the sequence length&#13;&#10;Output:&nbsp;(N,C)or&nbsp;(N,C,L)(same shape as input)&#13;&#10;Examples:&#13;&#10;&gt;&gt;&gt; # With Learnable Parameters&#13;&gt;&gt;&gt; m = nn.BatchNorm1d(100)&#13;&gt;&gt;&gt; # Without Learnable Parameters&#13;&gt;&gt;&gt; m = nn.BatchNorm1d(100, affine=False)&#13;&gt;&gt;&gt; input = torch.randn(20, 100)&#13;&gt;&gt;&gt; output = m(input)&#13;&#10;&#13;&#10;&#13;&#10;&#13;&#10;Class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)&#13;&#10;&#13;&#10;Applies Batch Normalization over a 4D input.&#13;&#10;4D is a mini-batch of 2D inputs with additional channel dimension. &#13;&#10;Because the Batch Normalization is done over the&nbsp;C&nbsp;dimension, computing statistics on&nbsp;(N, H, W)&nbsp;slices, it’s common terminology to call this Spatial Batch Normalization.&#13;&#10;&#13;&#10;Parameters&#13;&#10;num_features&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#int&quot;int) –&nbsp;C&nbsp;from an expected input of size&nbsp;(N,C,H,W)&#13;&#10;eps&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#float&quot;float) – a value added to the denominator for numerical stability. Default: 1e-5&#13;&#10;momentum&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;Optional[﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#float&quot;float]) – the value used for the running_mean and running_var computation. Can be set to&nbsp;None&nbsp;for cumulative moving average (i.e. simple average). Default: 0.1&#13;&#10;affine&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#bool&quot;bool) – a boolean value that when set to&nbsp;True, this module has learnable affine parameters. Default:&nbsp;True&#13;&#10;track_running_stats&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#bool&quot;bool) – a boolean value that when set to&nbsp;True, this module tracks the running mean and variance, and when set to&nbsp;False, this module does not track such statistics, and initializes statistics buffers&nbsp;running_mean&nbsp;and&nbsp;running_var&nbsp;as&nbsp;None. When these buffers are&nbsp;None, this module always uses batch statistics. in both training and eval modes. Default:&nbsp;True&#13;&#10;Shape:&#13;&#10;Input:&nbsp;(N,C,H,W&#13;&#10;Output:&nbsp;(N,C,H,W)&nbsp;(same shape as input)&#13;&#10;&#13;&#10;"><br>
<img src="Batchnorm%20layernorm.files/image002.png" width=1386 height=1440
alt="&#13;&#10;# With Learnable Parameters&#13;&#10;m = nn.BatchNorm2d(100)&#13;&#10;# Without Learnable Parameters&#13;&#10;m = nn.BatchNorm2d(100, affine=False)&#13;&#10;input = torch.randn(20, 100, 35, 45)&#13;&#10;output = m(input)&#13;&#10;&#13;&#10;Class torch.nn.InstanceNorm1d(num_features,&nbsp;eps=1e-05,&nbsp;momentum=0.1,&nbsp;affine=False,&nbsp;track_running_stats=False,&nbsp;device=None,&nbsp;dtype=None)&#13;&#10;&#13;&#10;Applies Instance Normalization.&#13;&#10;This operation applies Instance Normalization over a 2D (unbatched) or 3D (batched) input as described in the paper&nbsp;&#13;&#10;Parameters&#13;&#10;num_features&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#int&quot;int) – number of features or channels&nbsp;CC&nbsp;of the input&#13;&#10;eps&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#float&quot;float) – a value added to the denominator for numerical stability. Default: 1e-5&#13;&#10;momentum&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;Optional[﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#float&quot;float]) – the value used for the running_mean and running_var computation. Default: 0.1&#13;&#10;affine&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#bool&quot;bool) – a boolean value that when set to&nbsp;True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default:&nbsp;False.&#13;&#10;track_running_stats&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#bool&quot;bool) – a boolean value that when set to&nbsp;True, this module tracks the running mean and variance, and when set to&nbsp;False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default:&nbsp;False&#13;&#10;Shape:&#13;&#10;Input:&nbsp;(N,C,L)(N,C,L)&nbsp;or&nbsp;(C,L)(C,L)&#13;&#10;Output:&nbsp;(N,C,L)(N,C,L)&nbsp;or&nbsp;(C,L)(C,L)&nbsp;(same shape as input)&#13;&#10;Examples:&#13;&#10;&gt;&gt;&gt; # Without Learnable Parameters&#13;&gt;&gt;&gt; m = nn.InstanceNorm1d(100)&#13;&gt;&gt;&gt; # With Learnable Parameters&#13;&gt;&gt;&gt; m = nn.InstanceNorm1d(100, affine=True)&#13;&gt;&gt;&gt; input = torch.randn(20, 100, 40)&#13;&gt;&gt;&gt; output = m(input)&#13;&#10;&#13;&#10;&#13;&#10;&#13;&#10;classtorch.nn.InstanceNorm2d(num_features,&nbsp;eps=1e-05,&nbsp;momentum=0.1,&nbsp;affine=False,&nbsp;track_running_stats=False,&nbsp;device=None,&nbsp;dtype=None)﷟HYPERLINK &quot;https://pytorch.org/docs/stable/_modules/torch/nn/modules/instancenorm.html#InstanceNorm2d&quot;[source]﷟HYPERLINK &quot;https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/instancenorm.py#L241&quot;[source]&#13;&#10;&#13;&#10;Applies Instance Normalization.&#13;&#10;This operation applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper&nbsp;﷟HYPERLINK &quot;https://arxiv.org/abs/1607.08022&quot;Instance Normalization: The Missing Ingredient for Fast Stylization.&#13;&#10;&#13;&#10;Parameters&#13;&#10;num_features&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#int&quot;int) –&nbsp;CC&nbsp;from an expected input of size&nbsp;(N,C,H,W)(N,C,H,W)&nbsp;or&nbsp;(C,H,W)(C,H,W)&#13;&#10;eps&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#float&quot;float) – a value added to the denominator for numerical stability. Default: 1e-5&#13;&#10;momentum&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;Optional[﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#float&quot;float]) – the value used for the running_mean and running_var computation. Default: 0.1&#13;&#10;"><br>
<img src="Batchnorm%20layernorm.files/image003.png" width=1386 height=1440
alt="momentum&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;Optional[﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#float&quot;float]) – the value used for the running_mean and running_var computation. Default: 0.1&#13;&#10;affine&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#bool&quot;bool) – a boolean value that when set to&nbsp;True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default:&nbsp;False.&#13;&#10;track_running_stats&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#bool&quot;bool) – a boolean value that when set to&nbsp;True, this module tracks the running mean and variance, and when set to&nbsp;False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default:&nbsp;False&#13;&#10;Shape:&#13;&#10;Input:&nbsp;(N,C,H,W)(N,C,H,W)&nbsp;or&nbsp;(C,H,W)(C,H,W)&#13;&#10;Output:&nbsp;(N,C,H,W)(N,C,H,W)&nbsp;or&nbsp;(C,H,W)(C,H,W)&nbsp;(same shape as input)&#13;&#10;Examples:&#13;&#10;&gt;&gt;&gt; # Without Learnable Parameters&#13;&gt;&gt;&gt; m = nn.InstanceNorm2d(100)&#13;&gt;&gt;&gt; # With Learnable Parameters&#13;&gt;&gt;&gt; m = nn.InstanceNorm2d(100, affine=True)&#13;&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45)&#13;&gt;&gt;&gt; output = m(input)&#13;&#10;&#13;&#10;&#13;&#10;BatchNorm和InstanceNorm的参数都是num_features，即channel的个数或者特征的个数（对于2Dtensor来说），也就是说都是对每个channel独立地计算均值和方差，区别在于，对于channel A来说，BatchNorm统计均值和方差时用到的是batch中所有样本的值，而InstanceNorm统计均值和方差时用到的是单个样本的值。BatchNorm和InstanceNorm都有两个可学习参数，这两个参数的shape都是num_features。&#13;&#10;&#13;&#10;★对于Instancenorm来说，如果特征是标量，不是vector，那么就没办法做InstanceNorm，因为只有一个数，没法算平均值，只有vector才能算平均值。所以对InstanceNorm1d来说，当输入是2D的时候 ，是unbatched的，也就是这个2D输入是单个样本。而BatchNorm1d 就没有这个问题，当BatchNorm1d的输入是2D的时候，这个输入也是batched。&#13;&#10;&#13;&#10;&#13;&#10;&#13;&#10;Instancenorm没法用于NLP中。&#13;&#10;BatchNorm可以。对于NLP来说，input的shape是[batch_size, seq_len, embedding_len（hidden_size）]，用的时候，将前两维flatten，即reshape成 [batch_size* seq_len, embedding_len]的2D tensor。用Batchnorm1d。有两种方式，效果是一样的。&#13;&#10;第一种：&#13;&#10;feature = torch.randn(4, 2, 5)  # [ batch, seq_len, hidden_size ]&#13;feature = feature.transpose(1, 2)  # [ batch, hidden_size, seq_len ] 输入是3D的，&#13;&#10;"><br>
<img src="Batchnorm%20layernorm.files/image004.png" width=1386 height=1440
alt="feature = torch.randn(4, 2, 5)  # [ batch, seq_len, hidden_size ]&#13;feature = feature.transpose(1, 2)  # [ batch, hidden_size, seq_len ] 输入是3D的，&#13;&#10;&#13;&#10;bn = nn.BatchNorm1d(5, eps=1e-5)  # hidden_dim&#13;output = bn(feature1)&#13;&#10;Output = output.reshape(4,2,5)&#13;&#10;&#13;&#10;第二种：&#13;&#10;feature = torch.randn(4, 2, 5)  # [ batch, seq_len, hidden_size ]&#13;feature = feature.reshape(4*2, 5)  # [ batch * seq_len , hidden_size ]  输入是2D的&#13;&#10;&#13;&#10;bn = nn.BatchNorm1d(5, eps=1e-5)  # hidden_dim&#13;output = bn(feature1)&#13;&#10;Output = output.reshape(4,2,5)&#13;&#10;&#13;&#10;总结：Batchnorm是按照特征进行操作的，对于每个特征，计算batch中所有样本的均值和标准差，然后进行norm以及缩放。Batchnorm有四个参数，gamma ,beta,running_mean,running_variance，对于每一个特征来说，都有这样4个参数，即gamma.shape == beta.shape == running_mean.shape == running_var.shape == num_features。&#13;&#10;对于image来说，特征的数目=channel的数目，一张图片是一个样本，样本的特征数目=图片的通道数，单个样本的单个特征是2D tensor.&#13;&#10;对于NLP来说，特征的数目= token 的 embedding size,或者 token 的hidden_size，一个token是一个样本，样本的特征数目= hidden_size，单个样本的单个特征是一个标量。&#13;&#10;&#13;&#10;★为什么Batchnorm不用于Transformer中？&#13;&#10;一个batch中text_length是不同的，需要统一padding到max_length，正是因为一个token是一个样本，有些token是padding token，这些padding会影响均值和方差的计算。&#13;&#10;The fundamental issue is that padding tokens, although necessary for alignment, are not part of the original data. They introduce a lot of zeros into the dataset, which can mislead the normalization process. This is why batch normalization is not applied in the context of self-attention in transformers.&#13;&#10;&#13;&#10;&#13;&#10;Batchnorm进行计算时，计算每个batch的均值和方差，计算方差时需要注意用bessel correction。pytorch里unbiased = True。&#13;&#10;&#13;&#10;Class torch.nn.LayerNorm(normalized_shape,&nbsp;eps=1e-05,&nbsp;elementwise_affine=True,&nbsp;bias=True,&nbsp;device=None,&nbsp;dtype=None)&#13;&#10;&#13;&#10;The mean and standard-deviation are calculated over the last&nbsp;D&nbsp;dimensions, where&nbsp;D&nbsp;is the dimension of&nbsp;normalized_shape. For example, if&nbsp;normalized_shape&nbsp;is&nbsp;(3,&nbsp;5)&nbsp;(a 2-dimensional shape), the mean and standard-deviation are computed over the last 2 dimensions of the input (i.e.&nbsp;input.mean((-2,&nbsp;-1))).&nbsp;γγ&nbsp;and&nbsp;ββ&nbsp;are learnable affine transform parameters of&nbsp;normalized_shape&nbsp;if&nbsp;elementwise_affine&nbsp;is&nbsp;True. The variance is calculated via the biased estimator, equivalent to&nbsp;torch.var(input, unbiased=False).&#13;&#10;"><br>
<img src="Batchnorm%20layernorm.files/image005.png" width=1386 height=1440
alt="The mean and standard-deviation are calculated over the last&nbsp;D&nbsp;dimensions, where&nbsp;D&nbsp;is the dimension of&nbsp;normalized_shape. For example, if&nbsp;normalized_shape&nbsp;is&nbsp;(3,&nbsp;5)&nbsp;(a 2-dimensional shape), the mean and standard-deviation are computed over the last 2 dimensions of the input (i.e.&nbsp;input.mean((-2,&nbsp;-1))).&nbsp;γγ&nbsp;and&nbsp;ββ&nbsp;are learnable affine transform parameters of&nbsp;normalized_shape&nbsp;if&nbsp;elementwise_affine&nbsp;is&nbsp;True. The variance is calculated via the biased estimator, equivalent to&nbsp;torch.var(input, unbiased=False).&#13;&#10;&#13;&#10;This layer uses statistics computed from input data in both training and evaluation modes.&#13;&#10;Parameters&#13;&#10;normalized_shape&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#int&quot;int&nbsp;or&nbsp;﷟HYPERLINK &quot;https://docs.python.org/3/library/stdtypes.html#list&quot;list&nbsp;or&nbsp;﷟HYPERLINK &quot;https://pytorch.org/docs/stable/size.html#torch.Size&quot;torch.Size) –&#13;input shape from an expected input of size&#13;[∗×normalized_shape[0]×normalized_shape[1]×…×normalized_shape[−1]][∗×normalized_shape[0]×normalized_shape[1]×…×normalized_shape[−1]]&#13;If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size.&#13;&#10;eps&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#float&quot;float) – a value added to the denominator for numerical stability. Default: 1e-5&#13;&#10;elementwise_affine&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#bool&quot;bool) – a boolean value that when set to&nbsp;True, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default:&nbsp;True.&#13;&#10;bias&nbsp;(﷟HYPERLINK &quot;https://docs.python.org/3/library/functions.html#bool&quot;bool) – If set to&nbsp;False, the layer will not learn an additive bias (only relevant if&nbsp;elementwise_affine&nbsp;is&nbsp;True). Default:&nbsp;True.&#13;&#10;Variables&#13;&#10;weight&nbsp;– the learnable weights of the module of shape&nbsp;normalized_shapenormalized_shape&nbsp;when&nbsp;elementwise_affine&nbsp;is set to&nbsp;True. The values are initialized to 1.&#13;&#10;bias&nbsp;– the learnable bias of the module of shape&nbsp;normalized_shapenormalized_shape&nbsp;when&nbsp;elementwise_affine&nbsp;is set to&nbsp;True. The values are initialized to 0.&#13;&#10;&#13;&#10;Variables&#13;&#10;weight&nbsp;– the learnable weights of the module of shape&nbsp;normalized_shapenormalized_shape&nbsp;when&nbsp;elementwise_affine&nbsp;is set to&nbsp;True. The values are initialized to 1.&#13;&#10;bias&nbsp;– the learnable bias of the module of shape&nbsp;normalized_shapenormalized_shape&nbsp;when&nbsp;elementwise_affine&nbsp;is set to&nbsp;True. The values are initialized to 0.&#13;&#10;&#13;&#10;Shape:&#13;&#10;Input:&nbsp;(N,∗)(N,∗)&#13;&#10;Output:&nbsp;(N,∗)(N,∗)&nbsp;(same shape as input)&#13;&#10;Examples:&#13;&#10;&gt;&gt;&gt; # NLP Example&#13;&gt;&gt;&gt; batch, sentence_length, embedding_dim = 20, 5, 10&#13;&gt;&gt;&gt; embedding = torch.randn(batch, sentence_length, embedding_dim)&#13;&gt;&gt;&gt; layer_norm = nn.LayerNorm(embedding_dim)&#13;&gt;&gt;&gt; # Activate module&#13;&gt;&gt;&gt; layer_norm(embedding)&#13;&gt;&gt;&gt;&#13;&gt;&gt;&gt; # Image Example&#13;&gt;&gt;&gt; N, C, H, W = 20, 5, 10, 10&#13;&gt;&gt;&gt; input = torch.randn(N, C, H, W)&#13;&gt;&gt;&gt; # Normalize over the last three dimensions (i.e. the channel and spatial dimensions)&#13;&gt;&gt;&gt; # as shown in the image below&#13;&gt;&gt;&gt; layer_norm = nn.LayerNorm([C, H, W])&#13;&gt;&gt;&gt; output = layer_norm(input)&#13;&#10;&#13;&#10;LayerNorm中不会像BatchNorm那样跟踪统计全局的均值方差，因此train()和eval()对LayerNorm没有影响。&#13;&#10;"><br>
<img src="Batchnorm%20layernorm.files/image006.png" width=1386 height=1223
alt="LayerNorm中不会像BatchNorm那样跟踪统计全局的均值方差，因此train()和eval()对LayerNorm没有影响。&#13;&#10;&#13;&#10;normalized_shape：&#13;&#10;如果传入整数，比如4，则被看做只有一个整数的list，此时LayerNorm会对输入的最后一维进行归一化，这个int值需要和输入的最后一维一样大。&#13;&#10;&#13;&#10;假设此时输入的数据维度是[3, 4]，则对3个长度为4的向量求均值方差，得到3个均值和3个方差，分别对这3行进行归一化（每一行的4个数字都是均值为0，方差为1）；LayerNorm中的weight和bias也分别包含4个数字，重复使用3次，对每一行进行仿射变换（仿射变换即乘以weight中对应的数字后，然后加bias中对应的数字），并会在反向传播时得到学习。&#13;&#10;&#13;&#10;如果输入的是个list或者torch.Size，比如[3, 4]或torch.Size([3, 4])，则会对网络最后的两维进行归一化，且要求输入数据的最后两维尺寸也是[3, 4]。&#13;&#10;&#13;&#10;假设此时输入的数据维度也是[3, 4]，首先对这12个数字求均值和方差，然后归一化这个12个数字；weight和bias也分别包含12个数字，分别对12个归一化后的数字进行仿射变换（仿射变换即乘以weight中对应的数字后，然后加bias中对应的数字），并会在反向传播时得到学习。&#13;&#10;假设此时输入的数据维度是[N, 3, 4]，则对着N个[3,4]做和上述一样的操作，只是此时做仿射变换时，weight和bias被重复用了N次。&#13;&#10;假设此时输入的数据维度是[N, T, 3, 4]，也是一样的，维度可以更多。&#13;&#10;注意：显然LayerNorm中weight和bias的shape就是传入的normalized_shape。对于NLP来说，weight和bias的shape是embedding_dim，对于Image来说，weight和bias的shape是[C, H, W]。&#13;&#10;Batchnorm make sure that across batch dimension, any individual neuron has unit gaussian distribution.&#13;&#10;★以最简单的Batchnorm1D和LayerNorm1D来说，也就是输入是2维的，shape  = batch_size, d，其中d是特征的个数，Batchnorm1D是对每一列（每个特征）进行归一化，LayerNorm1D是对每一行（每个样本）进行归一化。&#13;&#10;对于NLP来说，特征的个数==token的embedding的长度，一个样本==一个token&#13;&#10;&#13;&#10;&#13;&#10;&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;墨迹绘图&#13;&#10;"><br>
</nobr></div>

</div>

</div>

<div>

<p style='margin:0in'>&nbsp;</p>

<p style='text-align:left;margin:0in;font-family:Arial;font-size:9pt;
color:#969696;direction:ltr'>已使用 OneNote 创建。</p>

</div>

</body>

</html>
